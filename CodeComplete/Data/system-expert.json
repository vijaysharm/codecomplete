{
   "glossary":[
	  {
		 "name":"ACID Transaction",
		 "definition":"\u003cp\u003e\n  A type of database transaction that has four important properties:\n\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003e\n    \u003cb\u003eAtomicity\u003c/b\u003e: The operations that constitute the transaction will either\n    all succeed or all fail. There is no in-between state.\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cb\u003eConsistency\u003c/b\u003e: The transaction cannot bring the database to an invalid\n    state. After the transaction is committed or rolled back, the rules for each\n    record will still apply, and all future transactions will see the effect of\n    the transaction. Also named \u003cb\u003eStrong Consistency\u003c/b\u003e.\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cb\u003eIsolation\u003c/b\u003e: The execution of multiple transactions concurrently will\n    have the same effect as if they had been executed sequentially.\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cb\u003eDurability\u003c/b\u003e: Any committed transaction is written to non-volatile\n    storage. It will not be undone by a crash, power loss, or network partition.\n  \u003c/li\u003e\n\u003c/ul\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"ACL",
		 "definition":"\u003cp\u003e\n  Short for \u003cb\u003eAccess-Control List\u003c/b\u003e. This term is often used to refer to a\n  permissioning model: which users in a system can perform which operations. For\n  instance, APIs often come with ACLs defining which users can delete, edit, or\n  view certain entities.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"AES",
		 "definition":"\u003cp\u003e\n  Stands for \u003cb\u003eAdvanced Encryption Standard\u003c/b\u003e. AES is a widely used\n  encryption standard that has three symmetric-key algorithms (AES-128, AES-192,\n  and AES-256).\n\u003c/p\u003e\n\u003cp\u003e\n  Of note, AES is considered to be the \"gold standard\" in encryption and is even\n  used by the U.S. National Security Agency to encrypt top secret information.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"Alerting",
		 "definition":"\u003cp\u003e\n  The process through which system administrators get notified when critical\n  system issues occur. Alerting can be be set up by defining specific thresholds\n  on monitoring charts, past which alerts are sent to a communication channel\n  like Slack.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"Apache Kafka",
		 "definition":"\u003cp\u003e\n  A distributed messaging system created by LinkedIn. Very useful\n  when using the \u003cb\u003estreaming\u003c/b\u003e paradigm as opposed to \u003cb\u003epolling\u003c/b\u003e.\n\u003c/p\u003e\n",
		 "is_tech":true,
		 "url":"https://kafka.apache.org/"
	  },
	  {
		 "name":"Asymmetric Encryption",
		 "definition":"\u003cp\u003e\n  Also known as public-key encryption, asymmetric encryption relies on two\n  keys—a public key and a private key—to encrypt and decrypt data. The keys are\n  generated using cryptographic algorithms and are mathematically connected such\n  that data encrypted with the public key can only be decrypted with the private\n  key.\n\u003c/p\u003e\n\u003cp\u003e\n  While the private key must be kept secure to maintain the fidelity of this\n  encryption paradigm, the public key can be openly shared.\n\u003c/p\u003e\n\u003cp\u003e\n  Asymmetric-key algorithms tend to be slower than their symmetric counterparts.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"Availability",
		 "definition":"\u003cp\u003e\n  The odds of a particular server or service being up and running at any point\n  in time, usually measured in percentages. A server that has 99% availability\n  will be operational 99% of the time (this would be described as having two\n  \u003cb\u003enines\u003c/b\u003e of availability).\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"Availability Zone",
		 "definition":"\u003cp\u003e\n  Sometimes referred to as an \u003cb\u003eAZ\u003c/b\u003e, an availability zone designates a group\n  of machines that share one or more central system components (e.g., power\n  source, network connectivity, machine-cooling system). Availability zones are\n  typically located far away from each other such that no natural disaster can\n  realistically bring down two of them at once. This ensures that if you have\n  redundant storage, for instance, with data stored in two availability zones,\n  losing one \u003cb\u003eAZ\u003c/b\u003e still leaves you with an operational system that abides\n  by any \u003cb\u003eSLA\u003c/b\u003e that it might have.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"Blob Storage",
		 "definition":"\u003cp\u003e\n  Widely used kind of storage, in small and large scale systems. They don’t\n  really count as databases per se, partially because they only allow the user\n  to store and retrieve data based on the name of the blob. This is sort of like\n  a key-value store but usually blob stores have different guarantees. They\n  might be slower than KV stores but values can be megabytes large (or sometimes\n  gigabytes large). Usually people use this to store things like\n  \u003cb\u003elarge binaries, database snapshots, or images\u003c/b\u003e and other static assets\n  that a website might have.\n\u003c/p\u003e\n\u003cp\u003e\n  Blob storage is rather complicated to have on premise, and only giant\n  companies like Google and Amazon have infrastructure that supports it. So\n  usually in the context of System Design interviews you can assume that you\n  will be able to use \u003cb\u003eGCS\u003c/b\u003e or \u003cb\u003eS3\u003c/b\u003e. These are blob storage services\n  hosted by Google and Amazon respectively, that cost money depending on how\n  much storage you use and how often you store and retrieve blobs from that\n  storage.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"Cache",
		 "definition":"\u003cp\u003e\n  A piece of hardware or software that stores data, typically meant to retrieve\n  that data faster than otherwise.\n\u003c/p\u003e\n\u003cp\u003e\n  Caches are often used to store responses to network requests as well as\n  results of computationally-long operations.\n\u003c/p\u003e\n\u003cp\u003e\n  Note that data in a cache can become \u003cb\u003estale\u003c/b\u003e if the main source of truth\n  for that data (i.e., the main database behind the cache) gets updated and the\n  cache doesn't.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"Cache Eviction Policy",
		 "definition":"\u003cp\u003e\n  The policy by which values get evicted or removed from a cache. Popular cache\n  eviction policies include \u003cb\u003eLRU\u003c/b\u003e (least-recently used), \u003cb\u003eFIFO\u003c/b\u003e (first\n  in first out), and \u003cb\u003eLFU\u003c/b\u003e (least-frequently used).\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"Cache Hit",
		 "definition":"\u003cp\u003eWhen requested data is found in a cache.\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"Cache Miss",
		 "definition":"\u003cp\u003e\n  When requested data could have been found in a cache but isn't. This is\n  typically used to refer to a negative consequence of a system failure or of a\n  poor design choice. For example:\n\u003c/p\u003e\n\u003cp\u003e\n  \u003ci\u003e\n    If a server goes down, our load balancer will have to forward requests to a\n    new server, which will result in cache misses.\n  \u003c/i\u003e\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"CAP Theorem",
		 "definition":"\u003cp\u003e\n  Stands for \u003cb\u003eConsistency\u003c/b\u003e, \u003cb\u003eAvailability\u003c/b\u003e,\n  \u003cb\u003ePartition tolerance\u003c/b\u003e. In a nutshell, this theorem states that any\n  distributed system can only achieve 2 of these 3 properties. Furthermore,\n  since almost all useful systems do have network-partition tolerance, it's\n  generally boiled down to: \u003ci\u003eConsistency vs. Availability; pick one\u003c/i\u003e.\n\u003c/p\u003e\n\u003cp\u003e\n  One thing to keep in mind is that some levels of consistency are still\n  achievable with high availability, but \u003ci\u003estrong\u003c/i\u003e consistency is much\n  harder.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"Certificate Authority",
		 "definition":"\u003cp\u003e\n  A trusted entity that signs digital certificates—namely, SSL certificates that\n  are relied on in \u003cb\u003eHTTPS\u003c/b\u003e connections.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"Client",
		 "definition":"\u003cp\u003e\n  A machine or process that requests data or service from a server.\n\u003c/p\u003e\n\u003cp\u003e\n  Note that a single machine or piece of software can be both a client and a\n  server at the same time. For instance, a single machine could act as a server\n  for end users and as a client for a database.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"Client—Server Model",
		 "definition":"\u003cp\u003e\n  The paradigm by which modern systems are designed, which consists of clients\n  requesting data or service from servers and servers providing data or service\n  to clients.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"Cloud Pub/Sub",
		 "definition":"\u003cp\u003e\n  A highly-scalable Pub/Sub messaging service created by Google. Guarantees\n  \u003cb\u003eat-least-once delivery\u003c/b\u003e of messages and supports \"rewinding\" in order to\n  reprocess messages.\n\u003c/p\u003e\n",
		 "is_tech":true,
		 "url":"https://cloud.google.com/pubsub/"
	  },
	  {
		 "name":"Configuration",
		 "definition":"\u003cp\u003e\n  A set of parameters or constants that are critical to a system. Configuration\n  is typically written in \u003cb\u003eJSON\u003c/b\u003e or \u003cb\u003eYAML\u003c/b\u003e and can be either \u003cb\u003estatic\u003c/b\u003e, meaning\n  that it's hard-coded in and shipped with your system's application code (like\n  frontend code, for instance), or \u003cb\u003edynamic\u003c/b\u003e, meaning that it lives outside\n  of your system's application code.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"Consensus Algorithm",
		 "definition":"\u003cp\u003e\n  A type of complex algorithms used to have multiple entities agree on a single\n  data value, like who the \"leader\" is amongst a group of machines. Two popular\n  consensus algorithms are \u003cb\u003ePaxos\u003c/b\u003e and \u003cb\u003eRaft\u003c/b\u003e.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"Consistent Hashing",
		 "definition":"\u003cp\u003e\n  A type of hashing that minimizes the number of keys that need to be remapped\n  when a hash table gets resized. It's often used by load balancers to\n  distribute traffic to servers; it minimizes the number of requests that get\n  forwarded to different servers when new servers are added or when existing\n  servers are brought down.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"Content Delivery Network",
		 "definition":"\u003cp\u003e\n  A \u003cb\u003eCDN\u003c/b\u003e is a third-party service that acts like a cache for your servers.\n  Sometimes, web applications can be slow for users in a particular region if\n  your servers are located only in another region. A CDN has servers all around\n  the world, meaning that the latency to a CDN's servers will almost always be\n  far better than the latency to your servers. A CDN's servers are often referred\n  to as \u003cb\u003ePoPs\u003c/b\u003e (Points of Presence). Two of the most popular CDNs are\n  \u003cb\u003eCloudflare\u003c/b\u003e and \u003cb\u003eGoogle Cloud CDN\u003c/b\u003e.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"CRUD Operations",
		 "definition":"\u003cp\u003e\n  Stands for \u003cb\u003eCreate\u003c/b\u003e, \u003cb\u003eRead\u003c/b\u003e, \u003cb\u003eUpdate\u003c/b\u003e,\n  \u003cb\u003eDelete\u003c/b\u003e Operations. These four operations often serve as the bedrock of a\n  functioning system and therefore find themselves at the core of many APIs.\n  The term \u003cb\u003eCRUD\u003c/b\u003e is very likely to come up during an API-design interview.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"Cypher",
		 "definition":"\u003cp\u003e\n  A \u003cb\u003egraph query language\u003c/b\u003e that was originally developed for the Neo4j\n  graph database, but that has since been standardized to be used with other\n  graph databases in an effort to make it the \"SQL for graphs.\"\n\u003c/p\u003e\n\u003cp\u003e\n  Cypher queries are often much simpler than their SQL counterparts. Example\n  Cypher query to find data in \u003cb\u003eNeo4j\u003c/b\u003e, a popular graph database:\n\u003c/p\u003e\n\u003cpre\u003e\nMATCH (some_node:SomeLabel)-[:SOME_RELATIONSHIP]-\u003e(some_other_node:SomeLabel {some_property:'value'})\n\u003c/pre\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"Database Index",
		 "definition":"\u003cp\u003e\n  A special auxiliary data structure that allows your database to perform\n  certain queries much faster. Indexes can typically only exist to reference\n  structured data, like data stored in relational databases. In practice, you\n  create an index on one or multiple columns in your database to greatly speed\n  up \u003cb\u003eread\u003c/b\u003e queries that you run very often, with the downside of slightly\n  longer \u003cb\u003ewrites\u003c/b\u003e to your database, since writes have to also take place in\n  the relevant index.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"Database Lock",
		 "definition":"\u003cp\u003e\n  In a relational database that provides ACID transactions, updating rows inside\n  a table will cause a \u003cb\u003elock\u003c/b\u003e to be held on that table or on the rows you\n  are updating. If a second transaction tries to update the same rows, it will\n  block before the update until the first transaction releases that lock. This\n  is one of the core mechanisms behind the \u003cb\u003eAtomicity\u003c/b\u003e of ACID\n  transactions.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"Databases",
		 "definition":"\u003cp\u003e\n  Databases are programs that either use disk or memory to do 2 core things:\n  \u003cb\u003erecord\u003c/b\u003e data and \u003cb\u003equery\u003c/b\u003e data. In general, they are themselves\n  servers that are long lived and interact with the rest of your application\n  through network calls, with protocols on top of TCP or even HTTP.\n\u003c/p\u003e\n\u003cp\u003e\n  Some databases only keep records in memory, and the users of such databases\n  are aware of the fact that those records may be lost forever if the machine or\n  process dies.\n\u003c/p\u003e\n\u003cp\u003e\n  For the most part though, databases need persistence of those records, and\n  thus cannot use memory. This means that you have to write your data to disk.\n  Anything written to disk will remain through power loss or network partitions,\n  so that’s what is used to keep permanent records.\n\u003c/p\u003e\n\u003cp\u003e\n  Since machines die often in a large scale system, special disk partitions or\n  volumes are used by the database processes, and those volumes can get\n  recovered even if the machine were to go down permanently.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"DDoS Attack",
		 "definition":"\u003cp\u003e\n  Short for \"distributed denial-of-service attack\", a DDoS attack is a DoS\n  attack in which the traffic flooding the target system comes from many\n  different sources (like thousands of machines), making it much harder to\n  defend against.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"Disk",
		 "definition":"\u003cp\u003e\n  Usually refers to either \u003cb\u003eHDD (hard-disk drive)\u003c/b\u003e or\n  \u003cb\u003eSSD (solid-state drive)\u003c/b\u003e. Data written to disk will persist through\n  power failures and general machine crashes. Disk is also referred to as\n  \u003cb\u003enon-volatile storage\u003c/b\u003e.\n\u003c/p\u003e\n\u003cp\u003e\n  SSD is far faster than HDD (see latencies of accessing data from SSD and HDD)\n  but also far more expensive from a financial point of view. Because of that,\n  HDD will typically be used for data that's rarely accessed or updated, but\n  that's stored for a long time, and SSD will be used for data that's frequently\n  accessed and updated.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"Distributed File System",
		 "definition":"\u003cp\u003e\n  A Distributed File System is an abstraction over a (usually large) cluster of\n  machines that allows them to act like one large file system. The two most\n  popular implementations of a DFS are the \u003cb\u003eGoogle File System\u003c/b\u003e (GFS) and\n  the \u003cb\u003eHadoop Distributed File System\u003c/b\u003e (HDFS).\n\u003c/p\u003e\n\u003cp\u003e\n  Typically, DFSs take care of the classic \u003cb\u003eavailability\u003c/b\u003e and\n  \u003cb\u003ereplication\u003c/b\u003e guarantees that can be tricky to obtain in a\n  distributed-system setting. The overarching idea is that files are split into\n  chunks of a certain size (4MB or 64MB, for instance), and those chunks are\n  sharded across a large cluster of machines. A central control plane is in\n  charge of deciding where each chunk resides, routing reads to the right nodes,\n  and handling communication between machines.\n\u003c/p\u003e\n\u003cp\u003e\n  Different DFS implementations have slightly different APIs and semantics, but\n  they achieve the same common goal: extremely large-scale persistent storage.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"DNS",
		 "definition":"\u003cp\u003e\nShort for Domain Name System, it describes the entities and protocols involved in the\ntranslation from domain names to IP Addresses. Typically, machines make a DNS query to\na well known entity which is responsible for returning the IP address (or multiple ones)\nof the requested domain name in the response.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"DoS Attack",
		 "definition":"\u003cp\u003e\n  Short for \"denial-of-service attack\", a DoS attack is an attack in which a\n  malicious user tries to bring down or damage a system in order to render it\n  unavailable to users. Much of the time, it consists of flooding it with\n  traffic. Some DoS attacks are easily preventable with rate limiting, while\n  others can be far trickier to defend against.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"Etcd",
		 "definition":"\u003cp\u003e\n  Etcd is a strongly consistent and highly available key-value store that's\n  often used to implement leader election in a system.\n\u003c/p\u003e\n",
		 "is_tech":true,
		 "url":"https://etcd.io/"
	  },
	  {
		 "name":"Eventual Consistency",
		 "definition":"\u003cp\u003e\n  A consistency model which is unlike \u003cb\u003eStrong Consistency\u003c/b\u003e. In this model,\n  reads might return a view of the system that is stale. An eventually\n  consistent datastore will give guarantees that the state of the database will\n  eventually reflect writes within a time period (could be 10 seconds, or\n  minutes).\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"File System",
		 "definition":"\u003cp\u003e\n  An abstraction over a storage medium that defines how to manage data. While\n  there exist many different types of file systems, most follow a hierarchical\n  structure that consists of directories and files, like the\n  \u003cb\u003eUnix file system\u003c/b\u003e's structure.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"Forward Proxy",
		 "definition":"\u003cp\u003e\n  A server that sits between a client and servers and acts on behalf of the\n  client, typically used to mask the client's identity (IP address). Note that\n  forward proxies are often referred to as just proxies.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"Google Cloud Storage",
		 "definition":"\u003cp\u003eGCS is a blob storage service provided by Google.\u003c/p\u003e\n",
		 "is_tech":true,
		 "url":"https://cloud.google.com/storage"
	  },
	  {
		 "name":"Gossip Protocol",
		 "definition":"\u003cp\u003e\n  When a set of machines talk to each other in a uncoordinated manner in a\n  cluster to spread information through a system without requiring a central\n  source of data.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"Graph Database",
		 "definition":"\u003cp\u003e\n  A type of database that stores data following the graph data model. Data\n  entries in a graph database can have explicitly defined relationships, much\n  like nodes in a graph can have edges.\n\u003c/p\u003e\n\u003cp\u003e\n  Graph databases take advantage of their underlying graph structure to perform\n  complex queries on deeply connected data very fast.\n\u003c/p\u003e\n\u003cp\u003e\n  Graph databases are thus often preferred to relational databases when dealing\n  with systems where data points naturally form a graph and have multiple levels\n  of relationships—for example, social networks.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"Hadoop",
		 "definition":"\u003cp\u003e\n  A popular, open-source framework that supports MapReduce jobs and many\n  other kinds of data-processing pipelines. Its central component is \u003cb\u003eHDFS\u003c/b\u003e\n  (Hadoop Distributed File System), on top of which other technologies have\n  been developed.\n\u003c/p\u003e\n",
		 "is_tech":true,
		 "url":"https://hadoop.apache.org/"
	  },
	  {
		 "name":"Hashing Function",
		 "definition":"\u003cp\u003e\n  A function that takes in a specific data type (such as a string\n  or an identifier) and outputs a number. Different inputs \u003ci\u003emay\u003c/i\u003e\n  have the same output, but a good hashing function attempts to\n  minimize those \u003cb\u003ehashing collisions\u003c/b\u003e (which is equivalent to\n  maximizing \u003cb\u003euniformity\u003c/b\u003e).\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"High Availability",
		 "definition":"\u003cp\u003e\n  Used to describe systems that have particularly high levels of availability,\n  typically 5 nines or more; sometimes abbreviated \"HA\".\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"Horizontal Scaling",
		 "definition":"\u003cp\u003e\n  Scaling a system horizontally means adding more machines to perform the same\n  task, resulting in increased throughput for the system. Typically, horizontal\n  scaling increases a system's throughput roughly linearly with the number of\n  machines performing a given task.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"Hot Spot",
		 "definition":"\u003cp\u003e\n  When distributing a workload across a set of servers, that workload might be\n  spread unevenly. This can happen if your \u003cb\u003esharding key\u003c/b\u003e or your \u003cb\u003ehashing function\u003c/b\u003e\n  are suboptimal, or if your workload is naturally skewed: some servers will\n  receive a lot more traffic than others, thus creating a \"hot spot\".\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"HTTP",
		 "definition":"\u003cp\u003e\n  The \u003cb\u003eH\u003c/b\u003eyper\u003cb\u003eT\u003c/b\u003eext \u003cb\u003eT\u003c/b\u003eransfer \u003cb\u003eP\u003c/b\u003erotocol is a very common network protocol implemented on top\n  of TCP. Clients make HTTP requests, and servers respond with a response.\n\u003c/p\u003e\n\u003cp\u003e\n  Requests typically have the following schema: \u003cbr /\u003e\n  \u003cpre\u003ehost: string (example: algoexpert.io)\nport: integer (example: 80 or 443)\nmethod: string (example: GET, PUT, POST, DELETE, OPTIONS or PATCH)\nheaders: \u003ckey, value\u003e pair list (example: \"Content-Type\" =\u003e \"application/json\")\nbody: opaque sequence of bytes\u003c/pre\u003e\n\u003c/p\u003e\n\u003cp\u003e\n  Responses typically have the following schema: \u003cbr /\u003e\n  \u003cpre\u003e\nstatus code: integer (example: 200, 401)\nheaders: \u003ckey, value\u003e pair list (example: \"Content-Length\" =\u003e 1238)\nbody: opaque sequence of bytes\u003c/pre\u003e\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"HTTPS",
		 "definition":"\u003cp\u003e\n  The \u003cb\u003eH\u003c/b\u003eyper\u003cb\u003eT\u003c/b\u003eext \u003cb\u003eT\u003c/b\u003eransfer \u003cb\u003eP\u003c/b\u003erotocol \u003cb\u003eS\u003c/b\u003eecure is\n  an extension of \u003cb\u003eHTTP\u003c/b\u003e that's used for secure communication online. It\n  requires servers to have trusted certificates (usually\n  \u003cb\u003eSSL certificates\u003c/b\u003e) and uses the Transport Layer Security (\u003cb\u003eTLS\u003c/b\u003e), a\n  security protocol built on top of \u003cb\u003eTCP\u003c/b\u003e, to encrypt data communicated\n  between a client and a server.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"Idempotent Operation",
		 "definition":"\u003cp\u003e\n  An operation that has the same ultimate outcome regardless of how many times\n  it's performed. If an operation can be performed multiple times without\n  changing its overall effect, it's idempotent. Operations performed through a\n  \u003cb\u003ePub/Sub\u003c/b\u003e messaging system typically have to be idempotent, since Pub/Sub\n  systems tend to allow the same messages to be consumed multiple times.\n\u003c/p\u003e\n\u003cp\u003e\n  For example, increasing an integer value in a database is \u003ci\u003enot\u003c/i\u003e an\n  idempotent operation, since repeating this operation will not have the same\n  effect as if it had been performed only once. Conversly, setting a value to\n  \"COMPLETE\" \u003ci\u003eis\u003c/i\u003e an idempotent operation, since repeating this operation\n  will always yield the same result: the value will be \"COMPLETE\".\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"InfluxDB",
		 "definition":"\u003cp\u003eA popular open-source time series database.\u003c/p\u003e\n",
		 "is_tech":true,
		 "url":"https://www.influxdata.com/"
	  },
	  {
		 "name":"IP",
		 "definition":"\u003cp\u003e\n  Stands for \u003cb\u003eInternet Protocol\u003c/b\u003e. This network protocol outlines how almost\n  all machine-to-machine communications should happen in the world. Other\n  protocols like \u003cb\u003eTCP\u003c/b\u003e, \u003cb\u003eUDP\u003c/b\u003e and \u003cb\u003eHTTP\u003c/b\u003e are built on top of IP.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"IP Address",
		 "definition":"\u003cp\u003e\n  An address given to each machine connected to the public internet. IPv4\n  addresses consist of four numbers separated by dots: \u003cb\u003ea.b.c.d\u003c/b\u003e where all\n  four numbers are between 0 and 255. Special values include:\n\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003e\n    \u003cb\u003e127.0.0.1\u003c/b\u003e: Your own local machine. Also referred to as\n    \u003cb\u003elocalhost\u003c/b\u003e.\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cb\u003e192.168.x.y\u003c/b\u003e: Your private network. For instance, your machine and all\n    machines on your private wifi network will usually have the\n    \u003cb\u003e192.168\u003c/b\u003e prefix.\n  \u003c/li\u003e\n\u003c/ul\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"IP Packet",
		 "definition":"\u003cp\u003e\n  Sometimes more broadly referred to as just a (network) \u003cb\u003epacket\u003c/b\u003e, an IP\n  packet is effectively the smallest unit used to describe data being sent over\n  \u003cb\u003eIP\u003c/b\u003e, aside from bytes. An IP packet consists of:\n\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003e\n    an \u003cb\u003eIP header\u003c/b\u003e, which contains the source and destination\n    \u003cb\u003eIP addresses\u003c/b\u003e as well as other information related to the network\n  \u003c/li\u003e\n  \u003cli\u003ea \u003cb\u003epayload\u003c/b\u003e, which is just the data being sent over the network\u003c/li\u003e\n\u003c/ul\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"JSON",
		 "definition":"\u003cp\u003eA file format heavily used in APIs and configuration. Stands for \u003cb\u003eJ\u003c/b\u003eava\u003cb\u003eS\u003c/b\u003ecript \u003cb\u003eO\u003c/b\u003ebject \u003cb\u003eN\u003c/b\u003eotation\u003c/b\u003e. Example:\u003c/p\u003e\n\u003cpre\u003e{\n   \"version\": 1.0,\n   \"name\": \"AlgoExpert Configuration\"\n}\u003c/pre\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"Key-Value Store",
		 "definition":"\u003cp\u003e\n  A Key-Value Store is a flexible NoSQL database that's often used for caching\n  and dynamic configuration. Popular options include DynamoDB, Etcd, Redis, and\n  ZooKeeper.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"Latency",
		 "definition":"\u003cp\u003e\n  The time it takes for a certain operation to complete in a system. Most often\n  this measure is a time duration, like milliseconds or seconds. You should know\n  these orders of magnitude:\n\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003e\u003cb\u003eReading 1 MB from RAM\u003c/b\u003e: 250 μs (0.25 ms)\u003c/li\u003e\n  \u003cli\u003e\u003cb\u003eReading 1 MB from SSD\u003c/b\u003e: 1,000 μs (1 ms)\u003c/li\u003e\n  \u003cli\u003e\u003cb\u003eTransfer 1 MB over Network\u003c/b\u003e: 10,000 μs (10 ms)\u003c/li\u003e\n  \u003cli\u003e\u003cb\u003eReading 1MB from HDD\u003c/b\u003e: 20,000 μs (20 ms)\u003c/li\u003e\n  \u003cli\u003e\u003cb\u003eInter-Continental Round Trip\u003c/b\u003e: 150,000 μs (150 ms)\u003c/li\u003e\n\u003c/ul\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"Leader Election",
		 "definition":"\u003cp\u003e\n  The process by which nodes in a cluster (for instance, servers in a set of\n  servers) elect a so-called \"leader\" amongst them, responsible for the primary\n  operations of the service that these nodes support. When correctly\n  implemented, leader election guarantees that all nodes in the cluster know\n  which one is the leader at any given time and can elect a new leader if the\n  leader dies for whatever reason.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"Load Balancer",
		 "definition":"\u003cp\u003e\n  A type of \u003cb\u003ereverse proxy\u003c/b\u003e that distributes traffic across servers. Load\n  balancers can be found in many parts of a system, from the DNS layer all the\n  way to the database layer.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"Logging",
		 "definition":"\u003cp\u003e\n  The act of collecting and storing logs--useful information about events in\n  your system. Typically your programs will output log messages to its STDOUT\n  or STDERR pipes, which will automatically get aggregated into a \u003cb\u003ecentralized\n  logging solution\u003c/b\u003e.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"Man-In-The-Middle Attack",
		 "definition":"\u003cp\u003e\n  An attack in which the attacker intercepts a line of communication that is\n  thought to be private by its two communicating parties.\n\u003c/p\u003e\n\u003cp\u003e\n  If a malicious actor intercepted and mutated an IP packet on its way from a\n  client to a server, that would be a man-in-the-middle attack.\n\u003c/p\u003e\n\u003cp\u003e\n  MITM attacks are the primary threat that encryption and \u003cb\u003eHTTPS\u003c/b\u003e aim to\n  defend against.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"MapReduce",
		 "definition":"\u003cp\u003e\n  A popular framework for processing very large datasets in a distributed\n  setting efficiently, quickly, and in a fault-tolerant manner. A MapReduce job\n  is comprised of 3 main steps:\n\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003e\n    the \u003cb\u003eMap\u003c/b\u003e step, which runs a \u003cb\u003emap function\u003c/b\u003e on the various chunks\n    of the dataset and transforms these chunks into intermediate\n    \u003cb\u003ekey-value pairs\u003c/b\u003e.\n  \u003c/li\u003e\n  \u003cli\u003e\n    the \u003cb\u003eShuffle\u003c/b\u003e step, which reorganizes the intermediate\n    \u003cb\u003ekey-value pairs\u003c/b\u003e such that pairs of the same key are routed\n    to the same machine in the final step.\n  \u003c/li\u003e\n  \u003cli\u003e\n    the \u003cb\u003eReduce\u003c/b\u003e step, which runs a \u003cb\u003ereduce function\u003c/b\u003e on the newly\n    shuffled \u003cb\u003ekey-value pairs\u003c/b\u003e and transforms them into more meaningful\n    data.\n  \u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\n  The canonical example of a MapReduce use case is counting the number of\n  occurrences of words in a large text file.\n\u003c/p\u003e\n\u003cp\u003e\n  When dealing with a MapReduce library, engineers and/or systems administrators\n  only need to worry about the map and reduce functions, as well as their inputs\n  and outputs. All other concerns, including the parallelization of tasks and\n  the fault-tolerance of the MapReduce job, are abstracted away and taken care\n  of by the MapReduce implementation.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"Memory",
		 "definition":"\u003cp\u003e\n  Short for \u003cb\u003eRandom Access Memory (RAM)\u003c/b\u003e. Data stored in memory will be\n  \u003cu\u003elost\u003c/u\u003e when the process that has written that data dies.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"Microservice Architecture",
		 "definition":"\u003cp\u003e\n  When a system is made up of many small web services that can be compiled and\n  deployed independently. This is usually thought of as a counterpart of\n  \u003cb\u003emonoliths\u003c/b\u003e.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"MongoDB",
		 "definition":"\u003cp\u003e\n  A NoSQL database with powerful querying through a JavaScript-like language.\n  Consistency guarantees depend on the settings that the database is setup with.\n\u003c/p\u003e\n",
		 "is_tech":true,
		 "url":"https://www.mongodb.com/"
	  },
	  {
		 "name":"Monitoring",
		 "definition":"\u003cp\u003e\n  The process of having visibility into a system's key metrics, monitoring is\n  typically implemented by collecting important events in a system and\n  aggregating them in human-readable charts.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"Monolith Architecture",
		 "definition":"\u003cp\u003e\n  When a system is primarily made up of a single large web application that is\n  compiled and rolled out as a unit. Typically a counterpart of\n  \u003cb\u003emicroservices\u003c/b\u003e. Companies sometimes try to split up this monolith into\n  microservices once it reaches a very large size in an attempt to increase\n  \u003cb\u003edeveloper productivity\u003c/b\u003e.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"MySQL",
		 "definition":"\u003cp\u003e\n  A relational database that provides ACID transactions and supports a SQL\n  dialect.\n\u003c/p\u003e\n",
		 "is_tech":true,
		 "url":"https://www.mysql.com/"
	  },
	  {
		 "name":"Neo4j",
		 "definition":"\u003cp\u003e\n  A popular graph database that consists of \u003cb\u003enodes\u003c/b\u003e, \u003cb\u003erelationships\u003c/b\u003e,\n  \u003cb\u003eproperties\u003c/b\u003e, and \u003cb\u003elabels\u003c/b\u003e.\n\u003c/p\u003e\n",
		 "is_tech":true,
		 "url":"https://neo4j.com/"
	  },
	  {
		 "name":"Nginx",
		 "definition":"\u003cp\u003e\n  Pronounced \"engine X\"—not \"N jinx\", Nginx is a very popular webserver that's\n  often used as a \u003cb\u003ereverse proxy\u003c/b\u003e and \u003cb\u003eload balancer\u003c/b\u003e.\n\u003c/p\u003e\n",
		 "is_tech":true,
		 "url":"https://www.nginx.com/"
	  },
	  {
		 "name":"Nines",
		 "definition":"\u003cp\u003e\n  Typically refers to percentages of uptime. For example, 5 nines of\n  availability means an uptime of 99.999% of the time. Below are the downtimes\n  expected per year depending on those 9s:\n\u003c/p\u003e\n\u003cpre\u003e\n- 99% (two 9s): 87.7 hours\n- 99.9% (three 9s): 8.8 hours\n- 99.99%: 52.6 minutes\n- 99.999%: 5.3 minutes\n\u003c/pre\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"Node/Instance/Host",
		 "definition":"\u003cp\u003e\n  These three terms refer to the same thing most of the time: a virtual or\n  physical machine on which the developer runs processes. Sometimes the word\n  \u003cb\u003eserver\u003c/b\u003e also refers to this same concept.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"Non-Relational Database",
		 "definition":"\u003cp\u003e\n  In contrast with relational database (SQL databases), a type of database that\n  is free of imposed, tabular-like structure. Non-relational databases are often\n  referred to as NoSQL databases.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"NoSQL Database",
		 "definition":"\u003cp\u003eAny database that is not SQL-compatible is called NoSQL.\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"Pagination",
		 "definition":"\u003cp\u003e\n  When a network request potentially warrants a really large response, the\n  relevant API might be designed to return only a single \u003cb\u003epage\u003c/b\u003e\n  of that response (i.e., a limited portion of the response), accompanied by an\n  identifier or token for the client to request the next page if desired.\n\u003c/p\u003e\n\u003cp\u003e\n  Pagination is often used when designing \u003cb\u003eList\u003c/b\u003e endpoints. For instance,\n  an endpoint to list videos on the YouTube Trending page could return a huge\n  list of videos. This wouldn't perform very well on mobile devices due to the\n  lower network speeds and simply wouldn't be optimal, since most users will\n  only ever scroll through the first ten or twenty videos. So, the API could be\n  designed to respond with only the first few videos of that list; in this case,\n  we would say that the API response is \u003cb\u003epaginated\u003c/b\u003e.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"Paxos \u0026 Raft",
		 "definition":"\u003cp\u003e\n  Two consensus algorithms that, when implemented correctly, allow for the\n  synchronization of certain operations, even in a distributed setting.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"Peer-To-Peer Network",
		 "definition":"\u003cp\u003e\n  A collection of machines referred to as peers that divide a workload between\n  themselves to presumably complete the workload faster than would otherwise be\n  possible. Peer-to-peer networks are often used in file-distribution systems.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"Percentiles",
		 "definition":"\u003cp\u003e\n  Most often used when describing a \u003cb\u003elatency distribution\u003c/b\u003e. If\n  your \u003cb\u003eX\u003c/b\u003eth percentile is 100 milliseconds, it means that\n  \u003cb\u003eX\u003c/b\u003e% of the requests have latencies of 100ms or less. Sometimes,\n  SLAs describe their guarantees using these percentiles.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"Persistent Storage",
		 "definition":"\u003cp\u003e\n  Usually refers to disk, but in general it is any form of storage that persists\n  if the process in charge of managing it dies.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"Polling",
		 "definition":"\u003cp\u003e\n  The act of fetching a resource or piece of data regularly at an interval to\n  make sure your data is not too stale.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"Port",
		 "definition":"\u003cp\u003e\n  In order for multiple programs to listen for new network connections on the\n  same machine without colliding, they pick a \u003cb\u003eport\u003c/b\u003e to listen on. A port\n  is an integer between 0 and 65,535 (2\u003csup\u003e16\u003c/sup\u003e ports total).\n\u003c/p\u003e\n\u003cp\u003e\n  Typically, ports 0-1023 are reserved for \u003ci\u003esystem ports\u003c/i\u003e (also called\n  \u003ci\u003ewell-known\u003c/i\u003e ports) and shouldn't be used by user-level processes.\n  Certain ports have pre-defined uses, and although you usually won't be\n  required to have them memorized, they can sometimes come in handy. Below are\n  some examples:\n\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003e22: Secure Shell\u003c/li\u003e\n  \u003cli\u003e53: DNS lookup\u003c/li\u003e\n  \u003cli\u003e80: HTTP\u003c/li\u003e\n  \u003cli\u003e443: HTTPS\u003c/li\u003e\n\u003c/ul\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"Postgres",
		 "definition":"\u003cp\u003e\n  A relational database that uses a dialect of SQL called PostgreSQL. Provides\n  ACID transactions.\n\u003c/p\u003e\n",
		 "is_tech":true,
		 "url":"https://www.postgresql.org/"
	  },
	  {
		 "name":"Process",
		 "definition":"\u003cp\u003e\n  A program that is currently running on a machine. You should always assume\n  that any process may get terminated at any time in a sufficiently large\n  system.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"Prometheus",
		 "definition":"\u003cp\u003e\n  A popular open-source time series database, typically used for monitoring\n  purposes.\n\u003c/p\u003e\n",
		 "is_tech":true,
		 "url":"https://prometheus.io/"
	  },
	  {
		 "name":"Publish/Subscribe Pattern",
		 "definition":"\u003cp\u003e\n  Often shortened as \u003cb\u003ePub/Sub\u003c/b\u003e, the Publish/Subscribe pattern is a popular\n  messaging model that consists of \u003cb\u003epublishers\u003c/b\u003e and \u003cb\u003esubscribers\u003c/b\u003e.\n  Publishers publish messages to special \u003cb\u003etopics\u003c/b\u003e (sometimes called\n  \u003cb\u003echannels\u003c/b\u003e) without caring about or even knowing who will read those\n  messages, and subscribers subscribe to topics and read messages coming through\n  those topics.\n\u003c/p\u003e\n\u003cp\u003e\n  Pub/Sub systems often come with very powerful guarantees like\n  \u003cb\u003eat-least-once delivery\u003c/b\u003e, \u003cb\u003epersistent storage\u003c/b\u003e, \n  \u003cb\u003eordering\u003c/b\u003e of messages, and \u003cb\u003ereplayability\u003c/b\u003e of messages.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"Quadtree",
		 "definition":"\u003cp\u003e\n  A tree data structure most commonly used to index two-dimensional spatial\n  data. Each node in a quadtree has either zero children nodes (and is therefore\n  a leaf node) or exactly four children nodes.\n\u003c/p\u003e\n\u003cp\u003e\n  Typically, quadtree nodes contain some form of spatial data—for example,\n  locations on a map—with a maximum capacity of some specified number \u003cb\u003en\u003c/b\u003e.\n  So long as nodes aren't at capacity, they remain leaf nodes; once they reach\n  capacity, they're given four children nodes, and their data entries are split\n  across the four children nodes.\n\u003c/p\u003e\n\u003cp\u003e\n  A quadtree lends itself well to storing spatial data because it can be\n  represented as a grid filled with rectangles that are recursively subdivided\n  into four sub-rectangles, where each quadtree node is represented by a\n  rectangle and each rectangle represents a spatial region. Assuming we're\n  storing locations in the world, we can imagine a quadtree with a maximum\n  node-capacity \u003cb\u003en\u003c/b\u003e as follows:\n\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003e\n    The root node, which represents the entire world, is the outermost\n    rectangle.\n  \u003c/li\u003e\n  \u003cli\u003e\n    If the entire world has more than \u003cb\u003en\u003c/b\u003e locations, the outermost\n    rectangle is divided into four quadrants, each representing a region of the\n    world.\n  \u003c/li\u003e\n  \u003cli\u003e\n    So long as a region has more than \u003cb\u003en\u003c/b\u003e locations, its corresponding\n    rectangle is subdivided into four quadrants (the corresponding node in the\n    quadtree is given four children nodes).\n  \u003c/li\u003e\n  \u003cli\u003e\n    Regions that have fewer than \u003cb\u003en\u003c/b\u003e locations are undivided rectangles\n    (leaf nodes).\n  \u003c/li\u003e\n  \u003cli\u003e\n    The parts of the grid that have many subdivided rectangles represent densely\n    populated areas (like cities), while the parts of the grid that have few\n    subdivided rectangles represent sparsely populated areas (like rural areas).\n  \u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\n  Finding a given location in a perfect quadtree is an extremely fast operation\n  that runs in \u003cb\u003elog\u003csub\u003e4\u003c/sub\u003e(x)\u003c/b\u003e time (where \u003cb\u003ex\u003c/b\u003e is the total\n  number of locations), since quadtree nodes have four children nodes.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"Rate Limiting",
		 "definition":"\u003cp\u003e\n  The act of limiting the number of requests sent to or from a system. Rate\n  limiting is most often used to limit the number of incoming requests in order\n  to prevent \u003cb\u003eDoS attacks\u003c/b\u003e and can be enforced at the IP-address level, at the\n  user-account level, or at the region level, for example. Rate limiting can\n  also be implemented in tiers; for instance, a type of network request could be\n  limited to 1 per second, 5 per 10 seconds, and 10 per minute.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"Redis",
		 "definition":"\u003cp\u003e\n  An in-memory key-value store. Does offer some persistent storage options but is\n  typically used as a really fast, best-effort caching solution. Redis is also often\n  used to implement \u003cb\u003erate limiting\u003c/b\u003e.\n\u003c/p\u003e\n",
		 "is_tech":true,
		 "url":"https://redis.io/"
	  },
	  {
		 "name":"Redundancy",
		 "definition":"\u003cp\u003e\n  The process of replicating parts of a system in an effort to make it more\n  reliable.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"Relational Database",
		 "definition":"\u003cp\u003e\n  A type of structured database in which data is stored following a tabular\n  format; often supports powerful querying using SQL.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"Rendezvous Hashing",
		 "definition":"\u003cp\u003e\n  A type of hashing also coined \u003cb\u003ehighest random weight\u003c/b\u003e hashing. Allows for\n  minimal re-distribution of mappings when a server goes down.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"Replication",
		 "definition":"\u003cp\u003e\n  The act of duplicating the data from one database server to others. This\n  is sometimes used to increase the redundancy of your system and\n  tolerate regional failures for instance. Other times you can use\n  replication to move data closer to your clients, thus decreasing\n  the latency of accessing specific data.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"Reverse Proxy",
		 "definition":"\u003cp\u003e\n  A server that sits between clients and servers and acts on behalf of the\n  servers, typically used for logging, load balancing, or caching.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"S3",
		 "definition":"\u003cp\u003e\n  S3 is a blob storage service provided by Amazon through\n  \u003cb\u003eAmazon Web Services (AWS)\u003c/b\u003e.\n\u003c/p\u003e\n",
		 "is_tech":true,
		 "url":"https://aws.amazon.com/s3/"
	  },
	  {
		 "name":"Server",
		 "definition":"\u003cp\u003e\n  A machine or process that provides data or service for a client, usually by\n  listening for incoming network calls.\n\u003c/p\u003e\n\u003cp\u003e\n  Note that a single machine or piece of software can be both a client and a\n  server at the same time. For instance, a single machine could act as a server\n  for end users and as a client for a database.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"Server-Selection Strategy",
		 "definition":"\u003cp\u003e\n  How a \u003cb\u003eload balancer\u003c/b\u003e chooses servers when distributing traffic amongst\n  multiple servers. Commonly used strategies include round-robin, random\n  selection, performance-based selection (choosing the server with the best\n  performance metrics, like the fastest response time or the least amount of\n  traffic), and IP-based routing.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"SHA",
		 "definition":"\u003cp\u003e\n  Short for \"Secure Hash Algorithms\", the SHA is a collection of cryptographic\n  hash functions used in the industry. These days, SHA-3 is a popular choice to\n  use in a system.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"Sharding",
		 "definition":"\u003cp\u003e\n  Sometimes called \u003cb\u003edata partitioning\u003c/b\u003e, sharding is the\n  act of splitting a database into two or more pieces called\n  \u003cb\u003eshards\u003c/b\u003e and is typically done to increase the throughput\n  of your database. Popular sharding strategies include:\n\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003eSharding based on a client's region\u003c/li\u003e\n  \u003cli\u003eSharding based on the type of data being stored (e.g: user data gets\n      stored in one shard, payments data gets stored in another\n      shard)\u003c/li\u003e\n  \u003cli\u003eSharding based on the hash of a column (only for structured\n      data)\u003c/li\u003e\n\u003c/ul\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"SLA",
		 "definition":"\u003cp\u003e\n  Short for \"service-level agreement\", an SLA is a collection of guarantees\n  given to a customer by a service provider. SLAs typically make guarantees on a\n  system's availability, amongst other things. SLAs are made up of one or\n  multiple SLOs.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"SLO",
		 "definition":"\u003cp\u003e\n  Short for \"service-level objective\", an SLO is a guarantee given to a customer\n  by a service provider. SLOs typically make guarantees on a system's\n  availability, amongst other things. SLOs constitute an SLA.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"Socket",
		 "definition":"\u003cp\u003e\n  A kind of file that acts like a stream. Processes can read and write to\n  sockets and communicate in this manner. Most of the time the sockets are\n  fronts for TCP connection.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"Spatial Database",
		 "definition":"\u003cp\u003e\n  A type of database optimized for storing and querying spatial data like\n  locations on a map. Spatial databases rely on spatial indexes like\n  \u003cb\u003equadtrees\u003c/b\u003e to quickly perform spatial queries like finding all\n  locations in the vicinity of a region.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"SQL",
		 "definition":"\u003cp\u003e\n  Structured Query Language. Relational databases can be used using a derivative\n  of SQL such as PostgreSQL in the case of Postgres.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"SQL Database",
		 "definition":"\u003cp\u003e\n  Any database that supports SQL. This term is often used synonymously with\n  \"Relational Database\", though in practice, not \u003ci\u003eevery\u003c/i\u003e relational\n  database supports SQL.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"SSL Certificate",
		 "definition":"\u003cp\u003e\n  A digital certificate granted to a server by a \u003cb\u003ecertificate authority\u003c/b\u003e.\n  Contains the server's public key, to be used as part of the\n  \u003cb\u003eTLS handshake\u003c/b\u003e process in an \u003cb\u003eHTTPS\u003c/b\u003e connection.\n\u003c/p\u003e\n\u003cp\u003e\n  An SSL certificate effectively confirms that a public key belongs to the\n  server claiming it belongs to them. SSL certificates are a crucial defense\n  against \u003cb\u003eman-in-the-middle attacks\u003c/b\u003e.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"Stateful",
		 "definition":"\u003cp\u003e\nA server or process is called \"stateful\" when it derives its\nfunctionality from storing and retrieving things from disk.\nDatabases are primary case studies for stateful servers. Because of this\npersistence requirement, it's much more difficult to run and manage\nstateful servers compared to \u003cb\u003eStateless\u003c/b\u003e servers because they\ncan't be stopped and restarted on any physical machine.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"Stateless",
		 "definition":"\u003cp\u003e\nA server is usually called \"stateless\" if it does not \u003ci\u003erequire\u003c/i\u003e\nstate to be persisted to disk in order to run successfully. Although\nmany server process typically hold some state in memory including\ncaching layers for instance, this typically means that we can\nrun the server process the same way on any machine, and move\nit around whenever we want. This contrasts with \u003cb\u003eStateful\u003c/b\u003e\nprocesses.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"Streaming",
		 "definition":"\u003cp\u003e\n  In networking, it usually refers to the act of continuously getting a feed of\n  information from a server by keeping an open connection between the two\n  machines or processes.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"Strong Consistency",
		 "definition":"\u003cp\u003e\nStrong Consistency usually refers to the consistency of ACID transactions, as opposed to \u003cb\u003eEventual Consistency\u003c/b\u003e.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"Symmetric Encryption",
		 "definition":"\u003cp\u003e\n  A type of encryption that relies on only a single key to both encrypt and\n  decrypt data. The key must be known to all parties involved in communication\n  and must therefore typically be shared between the parties at one point or\n  another.\n\u003c/p\u003e\n\u003cp\u003e\n  Symmetric-key algorithms tend to be faster than their asymmetric counterparts.\n\u003c/p\u003e\n\u003cp\u003e\n  The most widely used symmetric-key algorithms are part of the Advanced\n  Encryption Standard (\u003cb\u003eAES\u003c/b\u003e).\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"TCP",
		 "definition":"\u003cp\u003e\n  Network protocol built on top of the Internet Protocol (IP). Allows for\n  ordered, reliable data delivery between machines over the public internet by\n  creating a \u003cb\u003econnection\u003c/b\u003e.\n\u003c/p\u003e\n\u003cp\u003e\n  TCP is usually implemented in the kernel, which exposes \u003cb\u003esockets\u003c/b\u003e to\n  applications that they can use to stream data through an open connection.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"Throughput",
		 "definition":"\u003cp\u003e\n  The number of operations that a system can handle properly per time unit. For\n  instance the throughput of a server can often be measured in requests per\n  second (RPS or QPS).\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"Time Series Database",
		 "definition":"\u003cp\u003e\n  A \u003cb\u003eTSDB\u003c/b\u003e is a special kind of database optimized for storing and\n  analyzing time-indexed data: data points that specifically occur at a given\n  moment in time. Examples of TSDBs are InfluxDB, Prometheus, and Graphite.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"TLS",
		 "definition":"\u003cp\u003e\n  The \u003cb\u003eT\u003c/b\u003eransport \u003cb\u003eL\u003c/b\u003eayer \u003cb\u003eS\u003c/b\u003eecurity is a security protocol over\n  which \u003cb\u003eHTTP\u003c/b\u003e runs in order to achieve secure communication online. \"HTTP\n  over TLS\" is also known as \u003cb\u003eHTTPS\u003c/b\u003e.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"TLS Handshake",
		 "definition":"\u003cp\u003e\n  The process through which a client and a server communicating over\n  \u003cb\u003eHTTPS\u003c/b\u003e exchange encryption-related information and establish a secure\n  communication. The typical steps in a TLS handshake are roughly as follows:\n\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003e\n    The client sends a \u003cb\u003eclient hello\u003c/b\u003e—a string of random bytes—to the\n    server.\n  \u003c/li\u003e\n  \u003cli\u003e\n    The server responds with a \u003cb\u003eserver hello\u003c/b\u003e—another string of random\n    bytes—as well as its \u003cb\u003eSSL certificate\u003c/b\u003e, which contains its\n    \u003cb\u003epublic key\u003c/b\u003e.\n  \u003c/li\u003e\n  \u003cli\u003e\n    The client verifies that the certificate was issued by a\n    \u003cb\u003ecertificate authority\u003c/b\u003e and sends a \u003cb\u003epremaster secret\u003c/b\u003e—yet another\n    string of random bytes, this time encrypted with the server's public key—to\n    the server.\n  \u003c/li\u003e\n  \u003cli\u003e\n    The client and the server use the client hello, the server hello, and the\n    premaster secret to the generate the same \u003cb\u003esymmetric-encryption\u003c/b\u003e session keys,\n    to be used to encrypt and decrypt all data communicated during the remainder\n    of the connection.\n  \u003c/li\u003e\n\u003c/ul\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"Vertical Scaling",
		 "definition":"\u003cp\u003e\n  Scaling a system vertically means increasing the resources (CPU / Memory)\n  available to a certain task on a single machine, so that your throughput may\n  increase.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"Virtual Machine",
		 "definition":"\u003cp\u003e\n  A \u003cb\u003eVM\u003c/b\u003e is a form of computer inside of a computer. It\n  is a program that you run on a machine that completely\n  emulates a new kernel and operating system. Very useful when\n  isolating programs from one another while having them\n  share the same physical machine.\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"Worker Pool Pattern",
		 "definition":"\u003cp\u003e\n  Similar to the \u003cb\u003eTask Queue Pattern\u003c/b\u003e. In this design, a pool of workers,\n  usually themselves servers, take tasks off of a single shared queue and\n  process those tasks independently. In order to ensure that every task gets\n  done at least once despite potential partitions between queue and workers, the\n  workers \u003ci\u003emust\u003c/i\u003e confirm the status of the task after it is done (usually\n  \u003cb\u003esuccess\u003c/b\u003e or \u003cb\u003efailure\u003c/b\u003e).\n\u003c/p\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"YAML",
		 "definition":"\u003cp\u003eA file format mostly used in configuration. Example:\u003c/p\u003e\n\u003cpre\u003e\nversion: 1.0\nname: AlgoExpert Configuration\n\u003c/pre\u003e\n",
		 "is_tech":false,
		 "url":null
	  },
	  {
		 "name":"ZooKeeper",
		 "definition":"\u003cp\u003e\n  ZooKeeper is a strongly consistent, highly available key-value store. It's\n  often used to store important configuration or to perform leader election.\n\u003c/p\u003e\n",
		 "is_tech":true,
		 "url":"https://zookeeper.apache.org/"
	  }
   ],
   "fundamentals":[
	  {
		 "name":"Introduction",
		 "available":true,
		 "date_added":"",
		 "ready":true,
		 "video_url":"https://player.vimeo.com/video/385860724",
		 "description":"Where the coding interview serves primarily as an assessment of your problem-solving ability, the systems design interview is a test of your engineering knowledge veiled behind the facade of an open-ended design question.\n\nWelcome to the crucible of modern software.\n",
		 "thumbnail_url":"https://i.vimeocdn.com/video/855241617.jpg?mw=400\u0026mh=225\u0026q=70",
		 "video_duration":4,
		 "free_video_url":null,
		 "release_date":"",
		 "has_code_example":false,
		 "num_pre_reqs":0,
		 "pre_reqs":null,
		 "num_key_terms":0,
		 "key_terms":null
	  },
	  {
		 "name":"What Are Design Fundamentals?",
		 "available":true,
		 "date_added":"",
		 "ready":true,
		 "video_url":"https://player.vimeo.com/video/388787322",
		 "description":"Building scalable, production-ready applications is both art and science. Science, in that it requires knowledge of many topics in computer engineering; art, in that it demands an eye for making smart design choices and piecing together the right technologies.\n\nMaster both disciplines and you, too, can become a Systems Expert.",
		 "thumbnail_url":"https://i.vimeocdn.com/video/852198998.jpg?mw=400\u0026mh=225\u0026q=70",
		 "video_duration":8,
		 "free_video_url":null,
		 "release_date":"",
		 "has_code_example":false,
		 "num_pre_reqs":0,
		 "pre_reqs":null,
		 "num_key_terms":0,
		 "key_terms":null
	  },
	  {
		 "name":"Client—Server Model",
		 "available":true,
		 "date_added":"",
		 "ready":true,
		 "video_url":"https://player.vimeo.com/video/385524504",
		 "description":"A client is a thing that talks to servers. A server is a thing that talks to clients. The client—server model is a thing made up of a bunch of clients and servers talking to one another.\n\nAnd that, kids, is how the Internet works!\n",
		 "thumbnail_url":"https://i.vimeocdn.com/video/847689305.jpg?mw=400\u0026mh=225\u0026q=70",
		 "video_duration":15,
		 "free_video_url":null,
		 "release_date":"",
		 "has_code_example":true,
		 "num_pre_reqs":0,
		 "pre_reqs":null,
		 "num_key_terms":6,
		 "key_terms":[
			{
			   "name":"Client",
			   "definition":"\u003cp\u003e\n  A machine or process that requests data or service from a server.\n\u003c/p\u003e\n\u003cp\u003e\n  Note that a single machine or piece of software can be both a client and a\n  server at the same time. For instance, a single machine could act as a server\n  for end users and as a client for a database.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Server",
			   "definition":"\u003cp\u003e\n  A machine or process that provides data or service for a client, usually by\n  listening for incoming network calls.\n\u003c/p\u003e\n\u003cp\u003e\n  Note that a single machine or piece of software can be both a client and a\n  server at the same time. For instance, a single machine could act as a server\n  for end users and as a client for a database.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Client—Server Model",
			   "definition":"\u003cp\u003e\n  The paradigm by which modern systems are designed, which consists of clients\n  requesting data or service from servers and servers providing data or service\n  to clients.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"IP Address",
			   "definition":"\u003cp\u003e\n  An address given to each machine connected to the public internet. IPv4\n  addresses consist of four numbers separated by dots: \u003cb\u003ea.b.c.d\u003c/b\u003e where all\n  four numbers are between 0 and 255. Special values include:\n\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003e\n    \u003cb\u003e127.0.0.1\u003c/b\u003e: Your own local machine. Also referred to as\n    \u003cb\u003elocalhost\u003c/b\u003e.\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cb\u003e192.168.x.y\u003c/b\u003e: Your private network. For instance, your machine and all\n    machines on your private wifi network will usually have the\n    \u003cb\u003e192.168\u003c/b\u003e prefix.\n  \u003c/li\u003e\n\u003c/ul\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Port",
			   "definition":"\u003cp\u003e\n  In order for multiple programs to listen for new network connections on the\n  same machine without colliding, they pick a \u003cb\u003eport\u003c/b\u003e to listen on. A port\n  is an integer between 0 and 65,535 (2\u003csup\u003e16\u003c/sup\u003e ports total).\n\u003c/p\u003e\n\u003cp\u003e\n  Typically, ports 0-1023 are reserved for \u003ci\u003esystem ports\u003c/i\u003e (also called\n  \u003ci\u003ewell-known\u003c/i\u003e ports) and shouldn't be used by user-level processes.\n  Certain ports have pre-defined uses, and although you usually won't be\n  required to have them memorized, they can sometimes come in handy. Below are\n  some examples:\n\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003e22: Secure Shell\u003c/li\u003e\n  \u003cli\u003e53: DNS lookup\u003c/li\u003e\n  \u003cli\u003e80: HTTP\u003c/li\u003e\n  \u003cli\u003e443: HTTPS\u003c/li\u003e\n\u003c/ul\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"DNS",
			   "definition":"\u003cp\u003e\nShort for Domain Name System, it describes the entities and protocols involved in the\ntranslation from domain names to IP Addresses. Typically, machines make a DNS query to\na well known entity which is responsible for returning the IP address (or multiple ones)\nof the requested domain name in the response.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			}
		 ]
	  },
	  {
		 "name":"Network Protocols",
		 "available":true,
		 "date_added":"",
		 "ready":true,
		 "video_url":"https://player.vimeo.com/video/390863139",
		 "description":"IP packets. TCP headers. HTTP requests.\n\nAs daunting as they may seem, these low-level networking concepts are essential to understanding how machines in a system communicate with one another. And as we all know, proper communication is key for thriving relationships!",
		 "thumbnail_url":"https://i.vimeocdn.com/video/855192584.jpg?mw=400\u0026mh=225\u0026q=70",
		 "video_duration":29,
		 "free_video_url":null,
		 "release_date":"",
		 "has_code_example":true,
		 "num_pre_reqs":3,
		 "pre_reqs":[
			{
			   "name":"Client",
			   "definition":"\u003cp\u003e\n  A machine or process that requests data or service from a server.\n\u003c/p\u003e\n\u003cp\u003e\n  Note that a single machine or piece of software can be both a client and a\n  server at the same time. For instance, a single machine could act as a server\n  for end users and as a client for a database.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Server",
			   "definition":"\u003cp\u003e\n  A machine or process that provides data or service for a client, usually by\n  listening for incoming network calls.\n\u003c/p\u003e\n\u003cp\u003e\n  Note that a single machine or piece of software can be both a client and a\n  server at the same time. For instance, a single machine could act as a server\n  for end users and as a client for a database.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"IP Address",
			   "definition":"\u003cp\u003e\n  An address given to each machine connected to the public internet. IPv4\n  addresses consist of four numbers separated by dots: \u003cb\u003ea.b.c.d\u003c/b\u003e where all\n  four numbers are between 0 and 255. Special values include:\n\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003e\n    \u003cb\u003e127.0.0.1\u003c/b\u003e: Your own local machine. Also referred to as\n    \u003cb\u003elocalhost\u003c/b\u003e.\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cb\u003e192.168.x.y\u003c/b\u003e: Your private network. For instance, your machine and all\n    machines on your private wifi network will usually have the\n    \u003cb\u003e192.168\u003c/b\u003e prefix.\n  \u003c/li\u003e\n\u003c/ul\u003e\n",
			   "is_tech":false,
			   "url":null
			}
		 ],
		 "num_key_terms":4,
		 "key_terms":[
			{
			   "name":"IP",
			   "definition":"\u003cp\u003e\n  Stands for \u003cb\u003eInternet Protocol\u003c/b\u003e. This network protocol outlines how almost\n  all machine-to-machine communications should happen in the world. Other\n  protocols like \u003cb\u003eTCP\u003c/b\u003e, \u003cb\u003eUDP\u003c/b\u003e and \u003cb\u003eHTTP\u003c/b\u003e are built on top of IP.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"TCP",
			   "definition":"\u003cp\u003e\n  Network protocol built on top of the Internet Protocol (IP). Allows for\n  ordered, reliable data delivery between machines over the public internet by\n  creating a \u003cb\u003econnection\u003c/b\u003e.\n\u003c/p\u003e\n\u003cp\u003e\n  TCP is usually implemented in the kernel, which exposes \u003cb\u003esockets\u003c/b\u003e to\n  applications that they can use to stream data through an open connection.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"HTTP",
			   "definition":"\u003cp\u003e\n  The \u003cb\u003eH\u003c/b\u003eyper\u003cb\u003eT\u003c/b\u003eext \u003cb\u003eT\u003c/b\u003eransfer \u003cb\u003eP\u003c/b\u003erotocol is a very common network protocol implemented on top\n  of TCP. Clients make HTTP requests, and servers respond with a response.\n\u003c/p\u003e\n\u003cp\u003e\n  Requests typically have the following schema: \u003cbr /\u003e\n  \u003cpre\u003ehost: string (example: algoexpert.io)\nport: integer (example: 80 or 443)\nmethod: string (example: GET, PUT, POST, DELETE, OPTIONS or PATCH)\nheaders: \u003ckey, value\u003e pair list (example: \"Content-Type\" =\u003e \"application/json\")\nbody: opaque sequence of bytes\u003c/pre\u003e\n\u003c/p\u003e\n\u003cp\u003e\n  Responses typically have the following schema: \u003cbr /\u003e\n  \u003cpre\u003e\nstatus code: integer (example: 200, 401)\nheaders: \u003ckey, value\u003e pair list (example: \"Content-Length\" =\u003e 1238)\nbody: opaque sequence of bytes\u003c/pre\u003e\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"IP Packet",
			   "definition":"\u003cp\u003e\n  Sometimes more broadly referred to as just a (network) \u003cb\u003epacket\u003c/b\u003e, an IP\n  packet is effectively the smallest unit used to describe data being sent over\n  \u003cb\u003eIP\u003c/b\u003e, aside from bytes. An IP packet consists of:\n\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003e\n    an \u003cb\u003eIP header\u003c/b\u003e, which contains the source and destination\n    \u003cb\u003eIP addresses\u003c/b\u003e as well as other information related to the network\n  \u003c/li\u003e\n  \u003cli\u003ea \u003cb\u003epayload\u003c/b\u003e, which is just the data being sent over the network\u003c/li\u003e\n\u003c/ul\u003e\n",
			   "is_tech":false,
			   "url":null
			}
		 ]
	  },
	  {
		 "name":"Storage",
		 "available":true,
		 "date_added":"",
		 "ready":true,
		 "video_url":"https://player.vimeo.com/video/389200103",
		 "description":"An entire video dedicated just to the storage of data?\n\nYes! Multiple videos, in fact, as you'll see later on when we discuss databases. As it turns out, information storage is an incredibly complex topic that is of vital importance to systems design. \nDon't even think of skipping this lesson!",
		 "thumbnail_url":"https://i.vimeocdn.com/video/852779342.jpg?mw=400\u0026mh=225\u0026q=70",
		 "video_duration":18,
		 "free_video_url":null,
		 "release_date":"",
		 "has_code_example":true,
		 "num_pre_reqs":0,
		 "pre_reqs":null,
		 "num_key_terms":4,
		 "key_terms":[
			{
			   "name":"Databases",
			   "definition":"\u003cp\u003e\n  Databases are programs that either use disk or memory to do 2 core things:\n  \u003cb\u003erecord\u003c/b\u003e data and \u003cb\u003equery\u003c/b\u003e data. In general, they are themselves\n  servers that are long lived and interact with the rest of your application\n  through network calls, with protocols on top of TCP or even HTTP.\n\u003c/p\u003e\n\u003cp\u003e\n  Some databases only keep records in memory, and the users of such databases\n  are aware of the fact that those records may be lost forever if the machine or\n  process dies.\n\u003c/p\u003e\n\u003cp\u003e\n  For the most part though, databases need persistence of those records, and\n  thus cannot use memory. This means that you have to write your data to disk.\n  Anything written to disk will remain through power loss or network partitions,\n  so that’s what is used to keep permanent records.\n\u003c/p\u003e\n\u003cp\u003e\n  Since machines die often in a large scale system, special disk partitions or\n  volumes are used by the database processes, and those volumes can get\n  recovered even if the machine were to go down permanently.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Disk",
			   "definition":"\u003cp\u003e\n  Usually refers to either \u003cb\u003eHDD (hard-disk drive)\u003c/b\u003e or\n  \u003cb\u003eSSD (solid-state drive)\u003c/b\u003e. Data written to disk will persist through\n  power failures and general machine crashes. Disk is also referred to as\n  \u003cb\u003enon-volatile storage\u003c/b\u003e.\n\u003c/p\u003e\n\u003cp\u003e\n  SSD is far faster than HDD (see latencies of accessing data from SSD and HDD)\n  but also far more expensive from a financial point of view. Because of that,\n  HDD will typically be used for data that's rarely accessed or updated, but\n  that's stored for a long time, and SSD will be used for data that's frequently\n  accessed and updated.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Memory",
			   "definition":"\u003cp\u003e\n  Short for \u003cb\u003eRandom Access Memory (RAM)\u003c/b\u003e. Data stored in memory will be\n  \u003cu\u003elost\u003c/u\u003e when the process that has written that data dies.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Persistent Storage",
			   "definition":"\u003cp\u003e\n  Usually refers to disk, but in general it is any form of storage that persists\n  if the process in charge of managing it dies.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			}
		 ]
	  },
	  {
		 "name":"Latency And Throughput",
		 "available":true,
		 "date_added":"",
		 "ready":true,
		 "video_url":"https://player.vimeo.com/video/385893803",
		 "description":"If you've ever experienced lag in a video game, it was most likely due to a combination of high latency and low throughput. And lag sucks.\n\nIt is therefore your Call of Duty to master these two concepts and to join the crusade against high ping.\n",
		 "thumbnail_url":"https://i.vimeocdn.com/video/848212566.jpg?mw=400\u0026mh=225\u0026q=70",
		 "video_duration":17,
		 "free_video_url":null,
		 "release_date":"",
		 "has_code_example":false,
		 "num_pre_reqs":2,
		 "pre_reqs":[
			{
			   "name":"Disk",
			   "definition":"\u003cp\u003e\n  Usually refers to either \u003cb\u003eHDD (hard-disk drive)\u003c/b\u003e or\n  \u003cb\u003eSSD (solid-state drive)\u003c/b\u003e. Data written to disk will persist through\n  power failures and general machine crashes. Disk is also referred to as\n  \u003cb\u003enon-volatile storage\u003c/b\u003e.\n\u003c/p\u003e\n\u003cp\u003e\n  SSD is far faster than HDD (see latencies of accessing data from SSD and HDD)\n  but also far more expensive from a financial point of view. Because of that,\n  HDD will typically be used for data that's rarely accessed or updated, but\n  that's stored for a long time, and SSD will be used for data that's frequently\n  accessed and updated.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Memory",
			   "definition":"\u003cp\u003e\n  Short for \u003cb\u003eRandom Access Memory (RAM)\u003c/b\u003e. Data stored in memory will be\n  \u003cu\u003elost\u003c/u\u003e when the process that has written that data dies.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			}
		 ],
		 "num_key_terms":2,
		 "key_terms":[
			{
			   "name":"Latency",
			   "definition":"\u003cp\u003e\n  The time it takes for a certain operation to complete in a system. Most often\n  this measure is a time duration, like milliseconds or seconds. You should know\n  these orders of magnitude:\n\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003e\u003cb\u003eReading 1 MB from RAM\u003c/b\u003e: 250 μs (0.25 ms)\u003c/li\u003e\n  \u003cli\u003e\u003cb\u003eReading 1 MB from SSD\u003c/b\u003e: 1,000 μs (1 ms)\u003c/li\u003e\n  \u003cli\u003e\u003cb\u003eTransfer 1 MB over Network\u003c/b\u003e: 10,000 μs (10 ms)\u003c/li\u003e\n  \u003cli\u003e\u003cb\u003eReading 1MB from HDD\u003c/b\u003e: 20,000 μs (20 ms)\u003c/li\u003e\n  \u003cli\u003e\u003cb\u003eInter-Continental Round Trip\u003c/b\u003e: 150,000 μs (150 ms)\u003c/li\u003e\n\u003c/ul\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Throughput",
			   "definition":"\u003cp\u003e\n  The number of operations that a system can handle properly per time unit. For\n  instance the throughput of a server can often be measured in requests per\n  second (RPS or QPS).\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			}
		 ]
	  },
	  {
		 "name":"Availability",
		 "available":true,
		 "date_added":"",
		 "ready":true,
		 "video_url":"https://player.vimeo.com/video/388102223",
		 "description":"Oops! This content is unavailable right now. Please try again later.\n\n\n\n\nJust kidding! SystemsExpert is a highly available system.",
		 "thumbnail_url":"https://i.vimeocdn.com/video/851269900.jpg?mw=400\u0026mh=225\u0026q=70",
		 "video_duration":26,
		 "free_video_url":null,
		 "release_date":"",
		 "has_code_example":false,
		 "num_pre_reqs":3,
		 "pre_reqs":[
			{
			   "name":"Process",
			   "definition":"\u003cp\u003e\n  A program that is currently running on a machine. You should always assume\n  that any process may get terminated at any time in a sufficiently large\n  system.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Server",
			   "definition":"\u003cp\u003e\n  A machine or process that provides data or service for a client, usually by\n  listening for incoming network calls.\n\u003c/p\u003e\n\u003cp\u003e\n  Note that a single machine or piece of software can be both a client and a\n  server at the same time. For instance, a single machine could act as a server\n  for end users and as a client for a database.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Node/Instance/Host",
			   "definition":"\u003cp\u003e\n  These three terms refer to the same thing most of the time: a virtual or\n  physical machine on which the developer runs processes. Sometimes the word\n  \u003cb\u003eserver\u003c/b\u003e also refers to this same concept.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			}
		 ],
		 "num_key_terms":6,
		 "key_terms":[
			{
			   "name":"Availability",
			   "definition":"\u003cp\u003e\n  The odds of a particular server or service being up and running at any point\n  in time, usually measured in percentages. A server that has 99% availability\n  will be operational 99% of the time (this would be described as having two\n  \u003cb\u003enines\u003c/b\u003e of availability).\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"High Availability",
			   "definition":"\u003cp\u003e\n  Used to describe systems that have particularly high levels of availability,\n  typically 5 nines or more; sometimes abbreviated \"HA\".\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Nines",
			   "definition":"\u003cp\u003e\n  Typically refers to percentages of uptime. For example, 5 nines of\n  availability means an uptime of 99.999% of the time. Below are the downtimes\n  expected per year depending on those 9s:\n\u003c/p\u003e\n\u003cpre\u003e\n- 99% (two 9s): 87.7 hours\n- 99.9% (three 9s): 8.8 hours\n- 99.99%: 52.6 minutes\n- 99.999%: 5.3 minutes\n\u003c/pre\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Redundancy",
			   "definition":"\u003cp\u003e\n  The process of replicating parts of a system in an effort to make it more\n  reliable.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"SLA",
			   "definition":"\u003cp\u003e\n  Short for \"service-level agreement\", an SLA is a collection of guarantees\n  given to a customer by a service provider. SLAs typically make guarantees on a\n  system's availability, amongst other things. SLAs are made up of one or\n  multiple SLOs.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"SLO",
			   "definition":"\u003cp\u003e\n  Short for \"service-level objective\", an SLO is a guarantee given to a customer\n  by a service provider. SLOs typically make guarantees on a system's\n  availability, amongst other things. SLOs constitute an SLA.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			}
		 ]
	  },
	  {
		 "name":"Caching",
		 "available":true,
		 "date_added":"",
		 "ready":true,
		 "video_url":"https://player.vimeo.com/video/387513953",
		 "description":"What do a punching bag and a cache have in common?\n.\n.    \n.\n.    \nThey can both take a hit!  ( ͡~ ͜ʖ ͡°)\n",
		 "thumbnail_url":"https://i.vimeocdn.com/video/850501401.jpg?mw=400\u0026mh=225\u0026q=70",
		 "video_duration":28,
		 "free_video_url":null,
		 "release_date":"",
		 "has_code_example":true,
		 "num_pre_reqs":3,
		 "pre_reqs":[
			{
			   "name":"Latency",
			   "definition":"\u003cp\u003e\n  The time it takes for a certain operation to complete in a system. Most often\n  this measure is a time duration, like milliseconds or seconds. You should know\n  these orders of magnitude:\n\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003e\u003cb\u003eReading 1 MB from RAM\u003c/b\u003e: 250 μs (0.25 ms)\u003c/li\u003e\n  \u003cli\u003e\u003cb\u003eReading 1 MB from SSD\u003c/b\u003e: 1,000 μs (1 ms)\u003c/li\u003e\n  \u003cli\u003e\u003cb\u003eTransfer 1 MB over Network\u003c/b\u003e: 10,000 μs (10 ms)\u003c/li\u003e\n  \u003cli\u003e\u003cb\u003eReading 1MB from HDD\u003c/b\u003e: 20,000 μs (20 ms)\u003c/li\u003e\n  \u003cli\u003e\u003cb\u003eInter-Continental Round Trip\u003c/b\u003e: 150,000 μs (150 ms)\u003c/li\u003e\n\u003c/ul\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Throughput",
			   "definition":"\u003cp\u003e\n  The number of operations that a system can handle properly per time unit. For\n  instance the throughput of a server can often be measured in requests per\n  second (RPS or QPS).\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Memory",
			   "definition":"\u003cp\u003e\n  Short for \u003cb\u003eRandom Access Memory (RAM)\u003c/b\u003e. Data stored in memory will be\n  \u003cu\u003elost\u003c/u\u003e when the process that has written that data dies.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			}
		 ],
		 "num_key_terms":5,
		 "key_terms":[
			{
			   "name":"Cache",
			   "definition":"\u003cp\u003e\n  A piece of hardware or software that stores data, typically meant to retrieve\n  that data faster than otherwise.\n\u003c/p\u003e\n\u003cp\u003e\n  Caches are often used to store responses to network requests as well as\n  results of computationally-long operations.\n\u003c/p\u003e\n\u003cp\u003e\n  Note that data in a cache can become \u003cb\u003estale\u003c/b\u003e if the main source of truth\n  for that data (i.e., the main database behind the cache) gets updated and the\n  cache doesn't.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Cache Hit",
			   "definition":"\u003cp\u003eWhen requested data is found in a cache.\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Cache Miss",
			   "definition":"\u003cp\u003e\n  When requested data could have been found in a cache but isn't. This is\n  typically used to refer to a negative consequence of a system failure or of a\n  poor design choice. For example:\n\u003c/p\u003e\n\u003cp\u003e\n  \u003ci\u003e\n    If a server goes down, our load balancer will have to forward requests to a\n    new server, which will result in cache misses.\n  \u003c/i\u003e\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Cache Eviction Policy",
			   "definition":"\u003cp\u003e\n  The policy by which values get evicted or removed from a cache. Popular cache\n  eviction policies include \u003cb\u003eLRU\u003c/b\u003e (least-recently used), \u003cb\u003eFIFO\u003c/b\u003e (first\n  in first out), and \u003cb\u003eLFU\u003c/b\u003e (least-frequently used).\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Content Delivery Network",
			   "definition":"\u003cp\u003e\n  A \u003cb\u003eCDN\u003c/b\u003e is a third-party service that acts like a cache for your servers.\n  Sometimes, web applications can be slow for users in a particular region if\n  your servers are located only in another region. A CDN has servers all around\n  the world, meaning that the latency to a CDN's servers will almost always be\n  far better than the latency to your servers. A CDN's servers are often referred\n  to as \u003cb\u003ePoPs\u003c/b\u003e (Points of Presence). Two of the most popular CDNs are\n  \u003cb\u003eCloudflare\u003c/b\u003e and \u003cb\u003eGoogle Cloud CDN\u003c/b\u003e.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			}
		 ]
	  },
	  {
		 "name":"Proxies",
		 "available":true,
		 "date_added":"",
		 "ready":true,
		 "video_url":"https://player.vimeo.com/video/387002346",
		 "description":"Often used by nefarious hackers to conceal their identity and obfuscate their location, these special intermediary servers boast many important real-life applications within the context of caching, access control, and censorship bypassing, amongst other things.",
		 "thumbnail_url":"https://i.vimeocdn.com/video/849787496.jpg?mw=400\u0026mh=225\u0026q=70",
		 "video_duration":17,
		 "free_video_url":null,
		 "release_date":"",
		 "has_code_example":true,
		 "num_pre_reqs":2,
		 "pre_reqs":[
			{
			   "name":"Client",
			   "definition":"\u003cp\u003e\n  A machine or process that requests data or service from a server.\n\u003c/p\u003e\n\u003cp\u003e\n  Note that a single machine or piece of software can be both a client and a\n  server at the same time. For instance, a single machine could act as a server\n  for end users and as a client for a database.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Server",
			   "definition":"\u003cp\u003e\n  A machine or process that provides data or service for a client, usually by\n  listening for incoming network calls.\n\u003c/p\u003e\n\u003cp\u003e\n  Note that a single machine or piece of software can be both a client and a\n  server at the same time. For instance, a single machine could act as a server\n  for end users and as a client for a database.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			}
		 ],
		 "num_key_terms":3,
		 "key_terms":[
			{
			   "name":"Forward Proxy",
			   "definition":"\u003cp\u003e\n  A server that sits between a client and servers and acts on behalf of the\n  client, typically used to mask the client's identity (IP address). Note that\n  forward proxies are often referred to as just proxies.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Reverse Proxy",
			   "definition":"\u003cp\u003e\n  A server that sits between clients and servers and acts on behalf of the\n  servers, typically used for logging, load balancing, or caching.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Nginx",
			   "definition":"\u003cp\u003e\n  Pronounced \"engine X\"—not \"N jinx\", Nginx is a very popular webserver that's\n  often used as a \u003cb\u003ereverse proxy\u003c/b\u003e and \u003cb\u003eload balancer\u003c/b\u003e.\n\u003c/p\u003e\n",
			   "is_tech":true,
			   "url":"https://www.nginx.com/"
			}
		 ]
	  },
	  {
		 "name":"Load Balancers",
		 "available":true,
		 "date_added":"",
		 "ready":true,
		 "video_url":"https://player.vimeo.com/video/387194182",
		 "description":"Relentlessly distributing network requests across multiple servers, these digital traffic cops act as watchful guardians for your system, ensuring that it operates at peak performance day and night.",
		 "thumbnail_url":"https://i.vimeocdn.com/video/850063820.jpg?mw=400\u0026mh=225\u0026q=70",
		 "video_duration":25,
		 "free_video_url":null,
		 "release_date":"",
		 "has_code_example":true,
		 "num_pre_reqs":1,
		 "pre_reqs":[
			{
			   "name":"Reverse Proxy",
			   "definition":"\u003cp\u003e\n  A server that sits between clients and servers and acts on behalf of the\n  servers, typically used for logging, load balancing, or caching.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			}
		 ],
		 "num_key_terms":4,
		 "key_terms":[
			{
			   "name":"Load Balancer",
			   "definition":"\u003cp\u003e\n  A type of \u003cb\u003ereverse proxy\u003c/b\u003e that distributes traffic across servers. Load\n  balancers can be found in many parts of a system, from the DNS layer all the\n  way to the database layer.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Server-Selection Strategy",
			   "definition":"\u003cp\u003e\n  How a \u003cb\u003eload balancer\u003c/b\u003e chooses servers when distributing traffic amongst\n  multiple servers. Commonly used strategies include round-robin, random\n  selection, performance-based selection (choosing the server with the best\n  performance metrics, like the fastest response time or the least amount of\n  traffic), and IP-based routing.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Hot Spot",
			   "definition":"\u003cp\u003e\n  When distributing a workload across a set of servers, that workload might be\n  spread unevenly. This can happen if your \u003cb\u003esharding key\u003c/b\u003e or your \u003cb\u003ehashing function\u003c/b\u003e\n  are suboptimal, or if your workload is naturally skewed: some servers will\n  receive a lot more traffic than others, thus creating a \"hot spot\".\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Nginx",
			   "definition":"\u003cp\u003e\n  Pronounced \"engine X\"—not \"N jinx\", Nginx is a very popular webserver that's\n  often used as a \u003cb\u003ereverse proxy\u003c/b\u003e and \u003cb\u003eload balancer\u003c/b\u003e.\n\u003c/p\u003e\n",
			   "is_tech":true,
			   "url":"https://www.nginx.com/"
			}
		 ]
	  },
	  {
		 "name":"Hashing",
		 "available":true,
		 "date_added":"",
		 "ready":true,
		 "video_url":"https://player.vimeo.com/video/388541476",
		 "description":"Hashing? Like from hash tables? Should be simple enough, right?\n\nThe good news is that, yes, hashing like from hash tables.\n\nThe bad news is that, no, not simple enough. The video duration and thumbnail should be ominously indicative.\n",
		 "thumbnail_url":"https://i.vimeocdn.com/video/851835312.jpg?mw=400\u0026mh=225\u0026q=70",
		 "video_duration":38,
		 "free_video_url":null,
		 "release_date":"",
		 "has_code_example":true,
		 "num_pre_reqs":2,
		 "pre_reqs":[
			{
			   "name":"Hashing Function",
			   "definition":"\u003cp\u003e\n  A function that takes in a specific data type (such as a string\n  or an identifier) and outputs a number. Different inputs \u003ci\u003emay\u003c/i\u003e\n  have the same output, but a good hashing function attempts to\n  minimize those \u003cb\u003ehashing collisions\u003c/b\u003e (which is equivalent to\n  maximizing \u003cb\u003euniformity\u003c/b\u003e).\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Load Balancer",
			   "definition":"\u003cp\u003e\n  A type of \u003cb\u003ereverse proxy\u003c/b\u003e that distributes traffic across servers. Load\n  balancers can be found in many parts of a system, from the DNS layer all the\n  way to the database layer.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			}
		 ],
		 "num_key_terms":3,
		 "key_terms":[
			{
			   "name":"Consistent Hashing",
			   "definition":"\u003cp\u003e\n  A type of hashing that minimizes the number of keys that need to be remapped\n  when a hash table gets resized. It's often used by load balancers to\n  distribute traffic to servers; it minimizes the number of requests that get\n  forwarded to different servers when new servers are added or when existing\n  servers are brought down.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Rendezvous Hashing",
			   "definition":"\u003cp\u003e\n  A type of hashing also coined \u003cb\u003ehighest random weight\u003c/b\u003e hashing. Allows for\n  minimal re-distribution of mappings when a server goes down.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"SHA",
			   "definition":"\u003cp\u003e\n  Short for \"Secure Hash Algorithms\", the SHA is a collection of cryptographic\n  hash functions used in the industry. These days, SHA-3 is a popular choice to\n  use in a system.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			}
		 ]
	  },
	  {
		 "name":"Relational Databases",
		 "available":true,
		 "date_added":"",
		 "ready":true,
		 "video_url":"https://player.vimeo.com/video/390079322",
		 "description":"Tables and ACID.\n\nNo, we're not describing a drug lord's desk, but rather referring to key properties of relational databases. There's a lot of material to cover here, so hit the play button, kick back, and get ready to store tons of knowledge in the biggest database of them all: your brain.",
		 "thumbnail_url":"https://i.vimeocdn.com/video/854052859.jpg?mw=400\u0026mh=225\u0026q=70",
		 "video_duration":38,
		 "free_video_url":null,
		 "release_date":"",
		 "has_code_example":true,
		 "num_pre_reqs":3,
		 "pre_reqs":[
			{
			   "name":"Databases",
			   "definition":"\u003cp\u003e\n  Databases are programs that either use disk or memory to do 2 core things:\n  \u003cb\u003erecord\u003c/b\u003e data and \u003cb\u003equery\u003c/b\u003e data. In general, they are themselves\n  servers that are long lived and interact with the rest of your application\n  through network calls, with protocols on top of TCP or even HTTP.\n\u003c/p\u003e\n\u003cp\u003e\n  Some databases only keep records in memory, and the users of such databases\n  are aware of the fact that those records may be lost forever if the machine or\n  process dies.\n\u003c/p\u003e\n\u003cp\u003e\n  For the most part though, databases need persistence of those records, and\n  thus cannot use memory. This means that you have to write your data to disk.\n  Anything written to disk will remain through power loss or network partitions,\n  so that’s what is used to keep permanent records.\n\u003c/p\u003e\n\u003cp\u003e\n  Since machines die often in a large scale system, special disk partitions or\n  volumes are used by the database processes, and those volumes can get\n  recovered even if the machine were to go down permanently.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Disk",
			   "definition":"\u003cp\u003e\n  Usually refers to either \u003cb\u003eHDD (hard-disk drive)\u003c/b\u003e or\n  \u003cb\u003eSSD (solid-state drive)\u003c/b\u003e. Data written to disk will persist through\n  power failures and general machine crashes. Disk is also referred to as\n  \u003cb\u003enon-volatile storage\u003c/b\u003e.\n\u003c/p\u003e\n\u003cp\u003e\n  SSD is far faster than HDD (see latencies of accessing data from SSD and HDD)\n  but also far more expensive from a financial point of view. Because of that,\n  HDD will typically be used for data that's rarely accessed or updated, but\n  that's stored for a long time, and SSD will be used for data that's frequently\n  accessed and updated.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Memory",
			   "definition":"\u003cp\u003e\n  Short for \u003cb\u003eRandom Access Memory (RAM)\u003c/b\u003e. Data stored in memory will be\n  \u003cu\u003elost\u003c/u\u003e when the process that has written that data dies.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			}
		 ],
		 "num_key_terms":10,
		 "key_terms":[
			{
			   "name":"Relational Database",
			   "definition":"\u003cp\u003e\n  A type of structured database in which data is stored following a tabular\n  format; often supports powerful querying using SQL.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Non-Relational Database",
			   "definition":"\u003cp\u003e\n  In contrast with relational database (SQL databases), a type of database that\n  is free of imposed, tabular-like structure. Non-relational databases are often\n  referred to as NoSQL databases.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"SQL",
			   "definition":"\u003cp\u003e\n  Structured Query Language. Relational databases can be used using a derivative\n  of SQL such as PostgreSQL in the case of Postgres.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"SQL Database",
			   "definition":"\u003cp\u003e\n  Any database that supports SQL. This term is often used synonymously with\n  \"Relational Database\", though in practice, not \u003ci\u003eevery\u003c/i\u003e relational\n  database supports SQL.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"NoSQL Database",
			   "definition":"\u003cp\u003eAny database that is not SQL-compatible is called NoSQL.\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"ACID Transaction",
			   "definition":"\u003cp\u003e\n  A type of database transaction that has four important properties:\n\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003e\n    \u003cb\u003eAtomicity\u003c/b\u003e: The operations that constitute the transaction will either\n    all succeed or all fail. There is no in-between state.\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cb\u003eConsistency\u003c/b\u003e: The transaction cannot bring the database to an invalid\n    state. After the transaction is committed or rolled back, the rules for each\n    record will still apply, and all future transactions will see the effect of\n    the transaction. Also named \u003cb\u003eStrong Consistency\u003c/b\u003e.\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cb\u003eIsolation\u003c/b\u003e: The execution of multiple transactions concurrently will\n    have the same effect as if they had been executed sequentially.\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cb\u003eDurability\u003c/b\u003e: Any committed transaction is written to non-volatile\n    storage. It will not be undone by a crash, power loss, or network partition.\n  \u003c/li\u003e\n\u003c/ul\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Database Index",
			   "definition":"\u003cp\u003e\n  A special auxiliary data structure that allows your database to perform\n  certain queries much faster. Indexes can typically only exist to reference\n  structured data, like data stored in relational databases. In practice, you\n  create an index on one or multiple columns in your database to greatly speed\n  up \u003cb\u003eread\u003c/b\u003e queries that you run very often, with the downside of slightly\n  longer \u003cb\u003ewrites\u003c/b\u003e to your database, since writes have to also take place in\n  the relevant index.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Strong Consistency",
			   "definition":"\u003cp\u003e\nStrong Consistency usually refers to the consistency of ACID transactions, as opposed to \u003cb\u003eEventual Consistency\u003c/b\u003e.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Eventual Consistency",
			   "definition":"\u003cp\u003e\n  A consistency model which is unlike \u003cb\u003eStrong Consistency\u003c/b\u003e. In this model,\n  reads might return a view of the system that is stale. An eventually\n  consistent datastore will give guarantees that the state of the database will\n  eventually reflect writes within a time period (could be 10 seconds, or\n  minutes).\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Postgres",
			   "definition":"\u003cp\u003e\n  A relational database that uses a dialect of SQL called PostgreSQL. Provides\n  ACID transactions.\n\u003c/p\u003e\n",
			   "is_tech":true,
			   "url":"https://www.postgresql.org/"
			}
		 ]
	  },
	  {
		 "name":"Key-Value Stores",
		 "available":true,
		 "date_added":"",
		 "ready":true,
		 "video_url":"https://player.vimeo.com/video/388933857",
		 "description":"One of the most commonly used NoSQL paradigms today, the key-value store bases its data model on the associative array data type.\n\nThe result? A fast, flexible storage machine that resembles a hash table. That's right folks, our favorite friendly neighborhood data structure strikes again!",
		 "thumbnail_url":"https://i.vimeocdn.com/video/852412937.jpg?mw=400\u0026mh=225\u0026q=70",
		 "video_duration":11,
		 "free_video_url":null,
		 "release_date":"",
		 "has_code_example":true,
		 "num_pre_reqs":2,
		 "pre_reqs":[
			{
			   "name":"Relational Database",
			   "definition":"\u003cp\u003e\n  A type of structured database in which data is stored following a tabular\n  format; often supports powerful querying using SQL.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Non-Relational Database",
			   "definition":"\u003cp\u003e\n  In contrast with relational database (SQL databases), a type of database that\n  is free of imposed, tabular-like structure. Non-relational databases are often\n  referred to as NoSQL databases.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			}
		 ],
		 "num_key_terms":4,
		 "key_terms":[
			{
			   "name":"Key-Value Store",
			   "definition":"\u003cp\u003e\n  A Key-Value Store is a flexible NoSQL database that's often used for caching\n  and dynamic configuration. Popular options include DynamoDB, Etcd, Redis, and\n  ZooKeeper.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Etcd",
			   "definition":"\u003cp\u003e\n  Etcd is a strongly consistent and highly available key-value store that's\n  often used to implement leader election in a system.\n\u003c/p\u003e\n",
			   "is_tech":true,
			   "url":"https://etcd.io/"
			},
			{
			   "name":"Redis",
			   "definition":"\u003cp\u003e\n  An in-memory key-value store. Does offer some persistent storage options but is\n  typically used as a really fast, best-effort caching solution. Redis is also often\n  used to implement \u003cb\u003erate limiting\u003c/b\u003e.\n\u003c/p\u003e\n",
			   "is_tech":true,
			   "url":"https://redis.io/"
			},
			{
			   "name":"ZooKeeper",
			   "definition":"\u003cp\u003e\n  ZooKeeper is a strongly consistent, highly available key-value store. It's\n  often used to store important configuration or to perform leader election.\n\u003c/p\u003e\n",
			   "is_tech":true,
			   "url":"https://zookeeper.apache.org/"
			}
		 ]
	  },
	  {
		 "name":"Specialized Storage Paradigms",
		 "available":true,
		 "date_added":"",
		 "ready":true,
		 "video_url":"https://player.vimeo.com/video/456768635",
		 "description":"Yup. This is another video about storage.\n\nNope. This isn't the last video about storage.",
		 "thumbnail_url":"https://i.vimeocdn.com/video/955434705.jpg?mw=400\u0026mh=225\u0026q=70",
		 "video_duration":33,
		 "free_video_url":null,
		 "release_date":"2020-09-16T12:00:00-04:00",
		 "has_code_example":true,
		 "num_pre_reqs":8,
		 "pre_reqs":[
			{
			   "name":"Relational Database",
			   "definition":"\u003cp\u003e\n  A type of structured database in which data is stored following a tabular\n  format; often supports powerful querying using SQL.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Non-Relational Database",
			   "definition":"\u003cp\u003e\n  In contrast with relational database (SQL databases), a type of database that\n  is free of imposed, tabular-like structure. Non-relational databases are often\n  referred to as NoSQL databases.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"SQL",
			   "definition":"\u003cp\u003e\n  Structured Query Language. Relational databases can be used using a derivative\n  of SQL such as PostgreSQL in the case of Postgres.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"SQL Database",
			   "definition":"\u003cp\u003e\n  Any database that supports SQL. This term is often used synonymously with\n  \"Relational Database\", though in practice, not \u003ci\u003eevery\u003c/i\u003e relational\n  database supports SQL.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"NoSQL Database",
			   "definition":"\u003cp\u003eAny database that is not SQL-compatible is called NoSQL.\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Key-Value Store",
			   "definition":"\u003cp\u003e\n  A Key-Value Store is a flexible NoSQL database that's often used for caching\n  and dynamic configuration. Popular options include DynamoDB, Etcd, Redis, and\n  ZooKeeper.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Database Index",
			   "definition":"\u003cp\u003e\n  A special auxiliary data structure that allows your database to perform\n  certain queries much faster. Indexes can typically only exist to reference\n  structured data, like data stored in relational databases. In practice, you\n  create an index on one or multiple columns in your database to greatly speed\n  up \u003cb\u003eread\u003c/b\u003e queries that you run very often, with the downside of slightly\n  longer \u003cb\u003ewrites\u003c/b\u003e to your database, since writes have to also take place in\n  the relevant index.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Postgres",
			   "definition":"\u003cp\u003e\n  A relational database that uses a dialect of SQL called PostgreSQL. Provides\n  ACID transactions.\n\u003c/p\u003e\n",
			   "is_tech":true,
			   "url":"https://www.postgresql.org/"
			}
		 ],
		 "num_key_terms":11,
		 "key_terms":[
			{
			   "name":"Blob Storage",
			   "definition":"\u003cp\u003e\n  Widely used kind of storage, in small and large scale systems. They don’t\n  really count as databases per se, partially because they only allow the user\n  to store and retrieve data based on the name of the blob. This is sort of like\n  a key-value store but usually blob stores have different guarantees. They\n  might be slower than KV stores but values can be megabytes large (or sometimes\n  gigabytes large). Usually people use this to store things like\n  \u003cb\u003elarge binaries, database snapshots, or images\u003c/b\u003e and other static assets\n  that a website might have.\n\u003c/p\u003e\n\u003cp\u003e\n  Blob storage is rather complicated to have on premise, and only giant\n  companies like Google and Amazon have infrastructure that supports it. So\n  usually in the context of System Design interviews you can assume that you\n  will be able to use \u003cb\u003eGCS\u003c/b\u003e or \u003cb\u003eS3\u003c/b\u003e. These are blob storage services\n  hosted by Google and Amazon respectively, that cost money depending on how\n  much storage you use and how often you store and retrieve blobs from that\n  storage.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Time Series Database",
			   "definition":"\u003cp\u003e\n  A \u003cb\u003eTSDB\u003c/b\u003e is a special kind of database optimized for storing and\n  analyzing time-indexed data: data points that specifically occur at a given\n  moment in time. Examples of TSDBs are InfluxDB, Prometheus, and Graphite.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Graph Database",
			   "definition":"\u003cp\u003e\n  A type of database that stores data following the graph data model. Data\n  entries in a graph database can have explicitly defined relationships, much\n  like nodes in a graph can have edges.\n\u003c/p\u003e\n\u003cp\u003e\n  Graph databases take advantage of their underlying graph structure to perform\n  complex queries on deeply connected data very fast.\n\u003c/p\u003e\n\u003cp\u003e\n  Graph databases are thus often preferred to relational databases when dealing\n  with systems where data points naturally form a graph and have multiple levels\n  of relationships—for example, social networks.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Cypher",
			   "definition":"\u003cp\u003e\n  A \u003cb\u003egraph query language\u003c/b\u003e that was originally developed for the Neo4j\n  graph database, but that has since been standardized to be used with other\n  graph databases in an effort to make it the \"SQL for graphs.\"\n\u003c/p\u003e\n\u003cp\u003e\n  Cypher queries are often much simpler than their SQL counterparts. Example\n  Cypher query to find data in \u003cb\u003eNeo4j\u003c/b\u003e, a popular graph database:\n\u003c/p\u003e\n\u003cpre\u003e\nMATCH (some_node:SomeLabel)-[:SOME_RELATIONSHIP]-\u003e(some_other_node:SomeLabel {some_property:'value'})\n\u003c/pre\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Spatial Database",
			   "definition":"\u003cp\u003e\n  A type of database optimized for storing and querying spatial data like\n  locations on a map. Spatial databases rely on spatial indexes like\n  \u003cb\u003equadtrees\u003c/b\u003e to quickly perform spatial queries like finding all\n  locations in the vicinity of a region.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Quadtree",
			   "definition":"\u003cp\u003e\n  A tree data structure most commonly used to index two-dimensional spatial\n  data. Each node in a quadtree has either zero children nodes (and is therefore\n  a leaf node) or exactly four children nodes.\n\u003c/p\u003e\n\u003cp\u003e\n  Typically, quadtree nodes contain some form of spatial data—for example,\n  locations on a map—with a maximum capacity of some specified number \u003cb\u003en\u003c/b\u003e.\n  So long as nodes aren't at capacity, they remain leaf nodes; once they reach\n  capacity, they're given four children nodes, and their data entries are split\n  across the four children nodes.\n\u003c/p\u003e\n\u003cp\u003e\n  A quadtree lends itself well to storing spatial data because it can be\n  represented as a grid filled with rectangles that are recursively subdivided\n  into four sub-rectangles, where each quadtree node is represented by a\n  rectangle and each rectangle represents a spatial region. Assuming we're\n  storing locations in the world, we can imagine a quadtree with a maximum\n  node-capacity \u003cb\u003en\u003c/b\u003e as follows:\n\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003e\n    The root node, which represents the entire world, is the outermost\n    rectangle.\n  \u003c/li\u003e\n  \u003cli\u003e\n    If the entire world has more than \u003cb\u003en\u003c/b\u003e locations, the outermost\n    rectangle is divided into four quadrants, each representing a region of the\n    world.\n  \u003c/li\u003e\n  \u003cli\u003e\n    So long as a region has more than \u003cb\u003en\u003c/b\u003e locations, its corresponding\n    rectangle is subdivided into four quadrants (the corresponding node in the\n    quadtree is given four children nodes).\n  \u003c/li\u003e\n  \u003cli\u003e\n    Regions that have fewer than \u003cb\u003en\u003c/b\u003e locations are undivided rectangles\n    (leaf nodes).\n  \u003c/li\u003e\n  \u003cli\u003e\n    The parts of the grid that have many subdivided rectangles represent densely\n    populated areas (like cities), while the parts of the grid that have few\n    subdivided rectangles represent sparsely populated areas (like rural areas).\n  \u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\n  Finding a given location in a perfect quadtree is an extremely fast operation\n  that runs in \u003cb\u003elog\u003csub\u003e4\u003c/sub\u003e(x)\u003c/b\u003e time (where \u003cb\u003ex\u003c/b\u003e is the total\n  number of locations), since quadtree nodes have four children nodes.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Google Cloud Storage",
			   "definition":"\u003cp\u003eGCS is a blob storage service provided by Google.\u003c/p\u003e\n",
			   "is_tech":true,
			   "url":"https://cloud.google.com/storage"
			},
			{
			   "name":"S3",
			   "definition":"\u003cp\u003e\n  S3 is a blob storage service provided by Amazon through\n  \u003cb\u003eAmazon Web Services (AWS)\u003c/b\u003e.\n\u003c/p\u003e\n",
			   "is_tech":true,
			   "url":"https://aws.amazon.com/s3/"
			},
			{
			   "name":"InfluxDB",
			   "definition":"\u003cp\u003eA popular open-source time series database.\u003c/p\u003e\n",
			   "is_tech":true,
			   "url":"https://www.influxdata.com/"
			},
			{
			   "name":"Prometheus",
			   "definition":"\u003cp\u003e\n  A popular open-source time series database, typically used for monitoring\n  purposes.\n\u003c/p\u003e\n",
			   "is_tech":true,
			   "url":"https://prometheus.io/"
			},
			{
			   "name":"Neo4j",
			   "definition":"\u003cp\u003e\n  A popular graph database that consists of \u003cb\u003enodes\u003c/b\u003e, \u003cb\u003erelationships\u003c/b\u003e,\n  \u003cb\u003eproperties\u003c/b\u003e, and \u003cb\u003elabels\u003c/b\u003e.\n\u003c/p\u003e\n",
			   "is_tech":true,
			   "url":"https://neo4j.com/"
			}
		 ]
	  },
	  {
		 "name":"Replication And Sharding",
		 "available":true,
		 "date_added":"",
		 "ready":true,
		 "video_url":"https://player.vimeo.com/video/390248030",
		 "description":"A system's performance is often only as good as its database's; optimize the latter, and watch as the former improves in tandem!\n\nOn that note, in this video we'll examine how data redundancy and data partitioning techniques can be used to enhance a system's fault tolerance, throughput, and overall reliability.",
		 "thumbnail_url":"https://i.vimeocdn.com/video/854306335.jpg?mw=400\u0026mh=225\u0026q=70",
		 "video_duration":29,
		 "free_video_url":null,
		 "release_date":"",
		 "has_code_example":true,
		 "num_pre_reqs":6,
		 "pre_reqs":[
			{
			   "name":"Availability",
			   "definition":"\u003cp\u003e\n  The odds of a particular server or service being up and running at any point\n  in time, usually measured in percentages. A server that has 99% availability\n  will be operational 99% of the time (this would be described as having two\n  \u003cb\u003enines\u003c/b\u003e of availability).\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Latency",
			   "definition":"\u003cp\u003e\n  The time it takes for a certain operation to complete in a system. Most often\n  this measure is a time duration, like milliseconds or seconds. You should know\n  these orders of magnitude:\n\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003e\u003cb\u003eReading 1 MB from RAM\u003c/b\u003e: 250 μs (0.25 ms)\u003c/li\u003e\n  \u003cli\u003e\u003cb\u003eReading 1 MB from SSD\u003c/b\u003e: 1,000 μs (1 ms)\u003c/li\u003e\n  \u003cli\u003e\u003cb\u003eTransfer 1 MB over Network\u003c/b\u003e: 10,000 μs (10 ms)\u003c/li\u003e\n  \u003cli\u003e\u003cb\u003eReading 1MB from HDD\u003c/b\u003e: 20,000 μs (20 ms)\u003c/li\u003e\n  \u003cli\u003e\u003cb\u003eInter-Continental Round Trip\u003c/b\u003e: 150,000 μs (150 ms)\u003c/li\u003e\n\u003c/ul\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Throughput",
			   "definition":"\u003cp\u003e\n  The number of operations that a system can handle properly per time unit. For\n  instance the throughput of a server can often be measured in requests per\n  second (RPS or QPS).\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Redundancy",
			   "definition":"\u003cp\u003e\n  The process of replicating parts of a system in an effort to make it more\n  reliable.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Databases",
			   "definition":"\u003cp\u003e\n  Databases are programs that either use disk or memory to do 2 core things:\n  \u003cb\u003erecord\u003c/b\u003e data and \u003cb\u003equery\u003c/b\u003e data. In general, they are themselves\n  servers that are long lived and interact with the rest of your application\n  through network calls, with protocols on top of TCP or even HTTP.\n\u003c/p\u003e\n\u003cp\u003e\n  Some databases only keep records in memory, and the users of such databases\n  are aware of the fact that those records may be lost forever if the machine or\n  process dies.\n\u003c/p\u003e\n\u003cp\u003e\n  For the most part though, databases need persistence of those records, and\n  thus cannot use memory. This means that you have to write your data to disk.\n  Anything written to disk will remain through power loss or network partitions,\n  so that’s what is used to keep permanent records.\n\u003c/p\u003e\n\u003cp\u003e\n  Since machines die often in a large scale system, special disk partitions or\n  volumes are used by the database processes, and those volumes can get\n  recovered even if the machine were to go down permanently.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Reverse Proxy",
			   "definition":"\u003cp\u003e\n  A server that sits between clients and servers and acts on behalf of the\n  servers, typically used for logging, load balancing, or caching.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			}
		 ],
		 "num_key_terms":3,
		 "key_terms":[
			{
			   "name":"Replication",
			   "definition":"\u003cp\u003e\n  The act of duplicating the data from one database server to others. This\n  is sometimes used to increase the redundancy of your system and\n  tolerate regional failures for instance. Other times you can use\n  replication to move data closer to your clients, thus decreasing\n  the latency of accessing specific data.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Sharding",
			   "definition":"\u003cp\u003e\n  Sometimes called \u003cb\u003edata partitioning\u003c/b\u003e, sharding is the\n  act of splitting a database into two or more pieces called\n  \u003cb\u003eshards\u003c/b\u003e and is typically done to increase the throughput\n  of your database. Popular sharding strategies include:\n\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003eSharding based on a client's region\u003c/li\u003e\n  \u003cli\u003eSharding based on the type of data being stored (e.g: user data gets\n      stored in one shard, payments data gets stored in another\n      shard)\u003c/li\u003e\n  \u003cli\u003eSharding based on the hash of a column (only for structured\n      data)\u003c/li\u003e\n\u003c/ul\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Hot Spot",
			   "definition":"\u003cp\u003e\n  When distributing a workload across a set of servers, that workload might be\n  spread unevenly. This can happen if your \u003cb\u003esharding key\u003c/b\u003e or your \u003cb\u003ehashing function\u003c/b\u003e\n  are suboptimal, or if your workload is naturally skewed: some servers will\n  receive a lot more traffic than others, thus creating a \"hot spot\".\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			}
		 ]
	  },
	  {
		 "name":"Leader Election",
		 "available":true,
		 "date_added":"",
		 "ready":true,
		 "video_url":"https://player.vimeo.com/video/387241899",
		 "description":"Citizens in a society typically elect a leader by voting for their preferred candidate. But how do servers in a distributed system choose a master node? Via algorithms of course!\n\nThis form of algorithmic democracy is known as \"leader election\", though we personally think \"algorithmocracy\" sounds way cooler.\n",
		 "thumbnail_url":"https://i.vimeocdn.com/video/850129513.jpg?mw=400\u0026mh=225\u0026q=70",
		 "video_duration":30,
		 "free_video_url":null,
		 "release_date":"",
		 "has_code_example":true,
		 "num_pre_reqs":5,
		 "pre_reqs":[
			{
			   "name":"Availability",
			   "definition":"\u003cp\u003e\n  The odds of a particular server or service being up and running at any point\n  in time, usually measured in percentages. A server that has 99% availability\n  will be operational 99% of the time (this would be described as having two\n  \u003cb\u003enines\u003c/b\u003e of availability).\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"High Availability",
			   "definition":"\u003cp\u003e\n  Used to describe systems that have particularly high levels of availability,\n  typically 5 nines or more; sometimes abbreviated \"HA\".\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Redundancy",
			   "definition":"\u003cp\u003e\n  The process of replicating parts of a system in an effort to make it more\n  reliable.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Strong Consistency",
			   "definition":"\u003cp\u003e\nStrong Consistency usually refers to the consistency of ACID transactions, as opposed to \u003cb\u003eEventual Consistency\u003c/b\u003e.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Eventual Consistency",
			   "definition":"\u003cp\u003e\n  A consistency model which is unlike \u003cb\u003eStrong Consistency\u003c/b\u003e. In this model,\n  reads might return a view of the system that is stale. An eventually\n  consistent datastore will give guarantees that the state of the database will\n  eventually reflect writes within a time period (could be 10 seconds, or\n  minutes).\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			}
		 ],
		 "num_key_terms":5,
		 "key_terms":[
			{
			   "name":"Leader Election",
			   "definition":"\u003cp\u003e\n  The process by which nodes in a cluster (for instance, servers in a set of\n  servers) elect a so-called \"leader\" amongst them, responsible for the primary\n  operations of the service that these nodes support. When correctly\n  implemented, leader election guarantees that all nodes in the cluster know\n  which one is the leader at any given time and can elect a new leader if the\n  leader dies for whatever reason.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Consensus Algorithm",
			   "definition":"\u003cp\u003e\n  A type of complex algorithms used to have multiple entities agree on a single\n  data value, like who the \"leader\" is amongst a group of machines. Two popular\n  consensus algorithms are \u003cb\u003ePaxos\u003c/b\u003e and \u003cb\u003eRaft\u003c/b\u003e.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Paxos \u0026 Raft",
			   "definition":"\u003cp\u003e\n  Two consensus algorithms that, when implemented correctly, allow for the\n  synchronization of certain operations, even in a distributed setting.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Etcd",
			   "definition":"\u003cp\u003e\n  Etcd is a strongly consistent and highly available key-value store that's\n  often used to implement leader election in a system.\n\u003c/p\u003e\n",
			   "is_tech":true,
			   "url":"https://etcd.io/"
			},
			{
			   "name":"ZooKeeper",
			   "definition":"\u003cp\u003e\n  ZooKeeper is a strongly consistent, highly available key-value store. It's\n  often used to store important configuration or to perform leader election.\n\u003c/p\u003e\n",
			   "is_tech":true,
			   "url":"https://zookeeper.apache.org/"
			}
		 ]
	  },
	  {
		 "name":"Peer-To-Peer Networks",
		 "available":true,
		 "date_added":"",
		 "ready":true,
		 "video_url":"https://player.vimeo.com/video/388087749",
		 "description":"Equality for all.\nSharing is caring.\nUnity makes strength.\nThe more the merrier.\nTeamwork makes the dream work.\nWelcome to peer-to-peer networks!\n",
		 "thumbnail_url":"https://i.vimeocdn.com/video/851245273.jpg?mw=400\u0026mh=225\u0026q=70",
		 "video_duration":30,
		 "free_video_url":null,
		 "release_date":"",
		 "has_code_example":false,
		 "num_pre_reqs":2,
		 "pre_reqs":[
			{
			   "name":"Client—Server Model",
			   "definition":"\u003cp\u003e\n  The paradigm by which modern systems are designed, which consists of clients\n  requesting data or service from servers and servers providing data or service\n  to clients.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Throughput",
			   "definition":"\u003cp\u003e\n  The number of operations that a system can handle properly per time unit. For\n  instance the throughput of a server can often be measured in requests per\n  second (RPS or QPS).\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			}
		 ],
		 "num_key_terms":2,
		 "key_terms":[
			{
			   "name":"Peer-To-Peer Network",
			   "definition":"\u003cp\u003e\n  A collection of machines referred to as peers that divide a workload between\n  themselves to presumably complete the workload faster than would otherwise be\n  possible. Peer-to-peer networks are often used in file-distribution systems.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Gossip Protocol",
			   "definition":"\u003cp\u003e\n  When a set of machines talk to each other in a uncoordinated manner in a\n  cluster to spread information through a system without requiring a central\n  source of data.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			}
		 ]
	  },
	  {
		 "name":"Polling And Streaming",
		 "available":true,
		 "date_added":"",
		 "ready":true,
		 "video_url":"https://player.vimeo.com/video/388867040",
		 "description":"You can think of polling and streaming kind of like a classroom; sometimes students ask the teacher lots of questions, and other times they quiet down and listen attentively to the teacher's lecture.\n\nNow fire up the video and get ready to stream; you won't be able to poll here. Class is in session!\n",
		 "thumbnail_url":"https://i.vimeocdn.com/video/852315453.jpg?mw=400\u0026mh=225\u0026q=70",
		 "video_duration":26,
		 "free_video_url":null,
		 "release_date":"",
		 "has_code_example":true,
		 "num_pre_reqs":2,
		 "pre_reqs":[
			{
			   "name":"Client—Server Model",
			   "definition":"\u003cp\u003e\n  The paradigm by which modern systems are designed, which consists of clients\n  requesting data or service from servers and servers providing data or service\n  to clients.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Socket",
			   "definition":"\u003cp\u003e\n  A kind of file that acts like a stream. Processes can read and write to\n  sockets and communicate in this manner. Most of the time the sockets are\n  fronts for TCP connection.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			}
		 ],
		 "num_key_terms":2,
		 "key_terms":[
			{
			   "name":"Polling",
			   "definition":"\u003cp\u003e\n  The act of fetching a resource or piece of data regularly at an interval to\n  make sure your data is not too stale.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Streaming",
			   "definition":"\u003cp\u003e\n  In networking, it usually refers to the act of continuously getting a feed of\n  information from a server by keeping an open connection between the two\n  machines or processes.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			}
		 ]
	  },
	  {
		 "name":"Configuration",
		 "available":true,
		 "date_added":"",
		 "ready":true,
		 "video_url":"https://player.vimeo.com/video/386402873",
		 "description":"The config file is like the genome of a computer application; it stores parameters that define your system's critical settings, much like your DNA stores the genes that define your physical characteristics.\n\nUnlike its biological counterpart though, the config file is easily editable. No gene therapy needed!\n",
		 "thumbnail_url":"https://i.vimeocdn.com/video/848930916.jpg?mw=400\u0026mh=225\u0026q=70",
		 "video_duration":13,
		 "free_video_url":null,
		 "release_date":"",
		 "has_code_example":true,
		 "num_pre_reqs":3,
		 "pre_reqs":[
			{
			   "name":"JSON",
			   "definition":"\u003cp\u003eA file format heavily used in APIs and configuration. Stands for \u003cb\u003eJ\u003c/b\u003eava\u003cb\u003eS\u003c/b\u003ecript \u003cb\u003eO\u003c/b\u003ebject \u003cb\u003eN\u003c/b\u003eotation\u003c/b\u003e. Example:\u003c/p\u003e\n\u003cpre\u003e{\n   \"version\": 1.0,\n   \"name\": \"AlgoExpert Configuration\"\n}\u003c/pre\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"YAML",
			   "definition":"\u003cp\u003eA file format mostly used in configuration. Example:\u003c/p\u003e\n\u003cpre\u003e\nversion: 1.0\nname: AlgoExpert Configuration\n\u003c/pre\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Key-Value Store",
			   "definition":"\u003cp\u003e\n  A Key-Value Store is a flexible NoSQL database that's often used for caching\n  and dynamic configuration. Popular options include DynamoDB, Etcd, Redis, and\n  ZooKeeper.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			}
		 ],
		 "num_key_terms":1,
		 "key_terms":[
			{
			   "name":"Configuration",
			   "definition":"\u003cp\u003e\n  A set of parameters or constants that are critical to a system. Configuration\n  is typically written in \u003cb\u003eJSON\u003c/b\u003e or \u003cb\u003eYAML\u003c/b\u003e and can be either \u003cb\u003estatic\u003c/b\u003e, meaning\n  that it's hard-coded in and shipped with your system's application code (like\n  frontend code, for instance), or \u003cb\u003edynamic\u003c/b\u003e, meaning that it lives outside\n  of your system's application code.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			}
		 ]
	  },
	  {
		 "name":"Rate Limiting",
		 "available":true,
		 "date_added":"",
		 "ready":true,
		 "video_url":"https://player.vimeo.com/video/385859779",
		 "description":"*Poke*\n*Poke*\n*Poke*\n*Po——*\n\nToo many pokes! You just got rate limited.\n",
		 "thumbnail_url":"https://i.vimeocdn.com/video/848163464.jpg?mw=400\u0026mh=225\u0026q=70",
		 "video_duration":17,
		 "free_video_url":null,
		 "release_date":"",
		 "has_code_example":true,
		 "num_pre_reqs":2,
		 "pre_reqs":[
			{
			   "name":"Availability",
			   "definition":"\u003cp\u003e\n  The odds of a particular server or service being up and running at any point\n  in time, usually measured in percentages. A server that has 99% availability\n  will be operational 99% of the time (this would be described as having two\n  \u003cb\u003enines\u003c/b\u003e of availability).\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Key-Value Store",
			   "definition":"\u003cp\u003e\n  A Key-Value Store is a flexible NoSQL database that's often used for caching\n  and dynamic configuration. Popular options include DynamoDB, Etcd, Redis, and\n  ZooKeeper.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			}
		 ],
		 "num_key_terms":4,
		 "key_terms":[
			{
			   "name":"Rate Limiting",
			   "definition":"\u003cp\u003e\n  The act of limiting the number of requests sent to or from a system. Rate\n  limiting is most often used to limit the number of incoming requests in order\n  to prevent \u003cb\u003eDoS attacks\u003c/b\u003e and can be enforced at the IP-address level, at the\n  user-account level, or at the region level, for example. Rate limiting can\n  also be implemented in tiers; for instance, a type of network request could be\n  limited to 1 per second, 5 per 10 seconds, and 10 per minute.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"DoS Attack",
			   "definition":"\u003cp\u003e\n  Short for \"denial-of-service attack\", a DoS attack is an attack in which a\n  malicious user tries to bring down or damage a system in order to render it\n  unavailable to users. Much of the time, it consists of flooding it with\n  traffic. Some DoS attacks are easily preventable with rate limiting, while\n  others can be far trickier to defend against.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"DDoS Attack",
			   "definition":"\u003cp\u003e\n  Short for \"distributed denial-of-service attack\", a DDoS attack is a DoS\n  attack in which the traffic flooding the target system comes from many\n  different sources (like thousands of machines), making it much harder to\n  defend against.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Redis",
			   "definition":"\u003cp\u003e\n  An in-memory key-value store. Does offer some persistent storage options but is\n  typically used as a really fast, best-effort caching solution. Redis is also often\n  used to implement \u003cb\u003erate limiting\u003c/b\u003e.\n\u003c/p\u003e\n",
			   "is_tech":true,
			   "url":"https://redis.io/"
			}
		 ]
	  },
	  {
		 "name":"Logging And Monitoring",
		 "available":true,
		 "date_added":"",
		 "ready":true,
		 "video_url":"https://player.vimeo.com/video/386654608",
		 "description":"In order to properly understand and diagnose issues that crop up within a system, it’s critical to have mechanisms in place that create audit trails of various events that occur within said system.\n\nSo go ahead, unleash your inner Orwell and go full Big Brother on your application.\n",
		 "thumbnail_url":"https://i.vimeocdn.com/video/849289123.jpg?mw=400\u0026mh=225\u0026q=70",
		 "video_duration":12,
		 "free_video_url":null,
		 "release_date":"",
		 "has_code_example":false,
		 "num_pre_reqs":1,
		 "pre_reqs":[
			{
			   "name":"JSON",
			   "definition":"\u003cp\u003eA file format heavily used in APIs and configuration. Stands for \u003cb\u003eJ\u003c/b\u003eava\u003cb\u003eS\u003c/b\u003ecript \u003cb\u003eO\u003c/b\u003ebject \u003cb\u003eN\u003c/b\u003eotation\u003c/b\u003e. Example:\u003c/p\u003e\n\u003cpre\u003e{\n   \"version\": 1.0,\n   \"name\": \"AlgoExpert Configuration\"\n}\u003c/pre\u003e\n",
			   "is_tech":false,
			   "url":null
			}
		 ],
		 "num_key_terms":3,
		 "key_terms":[
			{
			   "name":"Logging",
			   "definition":"\u003cp\u003e\n  The act of collecting and storing logs--useful information about events in\n  your system. Typically your programs will output log messages to its STDOUT\n  or STDERR pipes, which will automatically get aggregated into a \u003cb\u003ecentralized\n  logging solution\u003c/b\u003e.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Monitoring",
			   "definition":"\u003cp\u003e\n  The process of having visibility into a system's key metrics, monitoring is\n  typically implemented by collecting important events in a system and\n  aggregating them in human-readable charts.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Alerting",
			   "definition":"\u003cp\u003e\n  The process through which system administrators get notified when critical\n  system issues occur. Alerting can be be set up by defining specific thresholds\n  on monitoring charts, past which alerts are sent to a communication channel\n  like Slack.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			}
		 ]
	  },
	  {
		 "name":"Publish/Subscribe Pattern",
		 "available":true,
		 "date_added":"",
		 "ready":true,
		 "video_url":"https://player.vimeo.com/video/413872495",
		 "description":"Publish/Subscribe. Press/Tug. Produce/Consume. Push/Pull. Send/Receive. Throw/Catch. Thrust/Retrieve.\n\nThree of these can be used interchangeably in the context of systems design. The others cannot.",
		 "thumbnail_url":"https://i.vimeocdn.com/video/886694798.jpg?mw=400\u0026mh=225\u0026q=70",
		 "video_duration":38,
		 "free_video_url":null,
		 "release_date":"",
		 "has_code_example":true,
		 "num_pre_reqs":3,
		 "pre_reqs":[
			{
			   "name":"Polling",
			   "definition":"\u003cp\u003e\n  The act of fetching a resource or piece of data regularly at an interval to\n  make sure your data is not too stale.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Streaming",
			   "definition":"\u003cp\u003e\n  In networking, it usually refers to the act of continuously getting a feed of\n  information from a server by keeping an open connection between the two\n  machines or processes.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Persistent Storage",
			   "definition":"\u003cp\u003e\n  Usually refers to disk, but in general it is any form of storage that persists\n  if the process in charge of managing it dies.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			}
		 ],
		 "num_key_terms":4,
		 "key_terms":[
			{
			   "name":"Publish/Subscribe Pattern",
			   "definition":"\u003cp\u003e\n  Often shortened as \u003cb\u003ePub/Sub\u003c/b\u003e, the Publish/Subscribe pattern is a popular\n  messaging model that consists of \u003cb\u003epublishers\u003c/b\u003e and \u003cb\u003esubscribers\u003c/b\u003e.\n  Publishers publish messages to special \u003cb\u003etopics\u003c/b\u003e (sometimes called\n  \u003cb\u003echannels\u003c/b\u003e) without caring about or even knowing who will read those\n  messages, and subscribers subscribe to topics and read messages coming through\n  those topics.\n\u003c/p\u003e\n\u003cp\u003e\n  Pub/Sub systems often come with very powerful guarantees like\n  \u003cb\u003eat-least-once delivery\u003c/b\u003e, \u003cb\u003epersistent storage\u003c/b\u003e, \n  \u003cb\u003eordering\u003c/b\u003e of messages, and \u003cb\u003ereplayability\u003c/b\u003e of messages.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Idempotent Operation",
			   "definition":"\u003cp\u003e\n  An operation that has the same ultimate outcome regardless of how many times\n  it's performed. If an operation can be performed multiple times without\n  changing its overall effect, it's idempotent. Operations performed through a\n  \u003cb\u003ePub/Sub\u003c/b\u003e messaging system typically have to be idempotent, since Pub/Sub\n  systems tend to allow the same messages to be consumed multiple times.\n\u003c/p\u003e\n\u003cp\u003e\n  For example, increasing an integer value in a database is \u003ci\u003enot\u003c/i\u003e an\n  idempotent operation, since repeating this operation will not have the same\n  effect as if it had been performed only once. Conversly, setting a value to\n  \"COMPLETE\" \u003ci\u003eis\u003c/i\u003e an idempotent operation, since repeating this operation\n  will always yield the same result: the value will be \"COMPLETE\".\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Apache Kafka",
			   "definition":"\u003cp\u003e\n  A distributed messaging system created by LinkedIn. Very useful\n  when using the \u003cb\u003estreaming\u003c/b\u003e paradigm as opposed to \u003cb\u003epolling\u003c/b\u003e.\n\u003c/p\u003e\n",
			   "is_tech":true,
			   "url":"https://kafka.apache.org/"
			},
			{
			   "name":"Cloud Pub/Sub",
			   "definition":"\u003cp\u003e\n  A highly-scalable Pub/Sub messaging service created by Google. Guarantees\n  \u003cb\u003eat-least-once delivery\u003c/b\u003e of messages and supports \"rewinding\" in order to\n  reprocess messages.\n\u003c/p\u003e\n",
			   "is_tech":true,
			   "url":"https://cloud.google.com/pubsub/"
			}
		 ]
	  },
	  {
		 "name":"MapReduce",
		 "available":true,
		 "date_added":"",
		 "ready":true,
		 "video_url":"https://player.vimeo.com/video/421310590",
		 "description":"\"MapReduce is a programming model for processing and generating big data sets with a parallel, distributed algorithm on a cluster.\"\n\nDoes Wikipedia's nebulous definition confuse you? Of course it does. In this video, we'll map out this complex topic and reduce it to clear, easily-understood concepts. See what we did there?  ( ͡~ ͜ʖ ͡°)",
		 "thumbnail_url":"https://i.vimeocdn.com/video/896916985.jpg?mw=400\u0026mh=225\u0026q=70",
		 "video_duration":37,
		 "free_video_url":null,
		 "release_date":"",
		 "has_code_example":true,
		 "num_pre_reqs":2,
		 "pre_reqs":[
			{
			   "name":"File System",
			   "definition":"\u003cp\u003e\n  An abstraction over a storage medium that defines how to manage data. While\n  there exist many different types of file systems, most follow a hierarchical\n  structure that consists of directories and files, like the\n  \u003cb\u003eUnix file system\u003c/b\u003e's structure.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Idempotent Operation",
			   "definition":"\u003cp\u003e\n  An operation that has the same ultimate outcome regardless of how many times\n  it's performed. If an operation can be performed multiple times without\n  changing its overall effect, it's idempotent. Operations performed through a\n  \u003cb\u003ePub/Sub\u003c/b\u003e messaging system typically have to be idempotent, since Pub/Sub\n  systems tend to allow the same messages to be consumed multiple times.\n\u003c/p\u003e\n\u003cp\u003e\n  For example, increasing an integer value in a database is \u003ci\u003enot\u003c/i\u003e an\n  idempotent operation, since repeating this operation will not have the same\n  effect as if it had been performed only once. Conversly, setting a value to\n  \"COMPLETE\" \u003ci\u003eis\u003c/i\u003e an idempotent operation, since repeating this operation\n  will always yield the same result: the value will be \"COMPLETE\".\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			}
		 ],
		 "num_key_terms":3,
		 "key_terms":[
			{
			   "name":"MapReduce",
			   "definition":"\u003cp\u003e\n  A popular framework for processing very large datasets in a distributed\n  setting efficiently, quickly, and in a fault-tolerant manner. A MapReduce job\n  is comprised of 3 main steps:\n\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003e\n    the \u003cb\u003eMap\u003c/b\u003e step, which runs a \u003cb\u003emap function\u003c/b\u003e on the various chunks\n    of the dataset and transforms these chunks into intermediate\n    \u003cb\u003ekey-value pairs\u003c/b\u003e.\n  \u003c/li\u003e\n  \u003cli\u003e\n    the \u003cb\u003eShuffle\u003c/b\u003e step, which reorganizes the intermediate\n    \u003cb\u003ekey-value pairs\u003c/b\u003e such that pairs of the same key are routed\n    to the same machine in the final step.\n  \u003c/li\u003e\n  \u003cli\u003e\n    the \u003cb\u003eReduce\u003c/b\u003e step, which runs a \u003cb\u003ereduce function\u003c/b\u003e on the newly\n    shuffled \u003cb\u003ekey-value pairs\u003c/b\u003e and transforms them into more meaningful\n    data.\n  \u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\n  The canonical example of a MapReduce use case is counting the number of\n  occurrences of words in a large text file.\n\u003c/p\u003e\n\u003cp\u003e\n  When dealing with a MapReduce library, engineers and/or systems administrators\n  only need to worry about the map and reduce functions, as well as their inputs\n  and outputs. All other concerns, including the parallelization of tasks and\n  the fault-tolerance of the MapReduce job, are abstracted away and taken care\n  of by the MapReduce implementation.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Distributed File System",
			   "definition":"\u003cp\u003e\n  A Distributed File System is an abstraction over a (usually large) cluster of\n  machines that allows them to act like one large file system. The two most\n  popular implementations of a DFS are the \u003cb\u003eGoogle File System\u003c/b\u003e (GFS) and\n  the \u003cb\u003eHadoop Distributed File System\u003c/b\u003e (HDFS).\n\u003c/p\u003e\n\u003cp\u003e\n  Typically, DFSs take care of the classic \u003cb\u003eavailability\u003c/b\u003e and\n  \u003cb\u003ereplication\u003c/b\u003e guarantees that can be tricky to obtain in a\n  distributed-system setting. The overarching idea is that files are split into\n  chunks of a certain size (4MB or 64MB, for instance), and those chunks are\n  sharded across a large cluster of machines. A central control plane is in\n  charge of deciding where each chunk resides, routing reads to the right nodes,\n  and handling communication between machines.\n\u003c/p\u003e\n\u003cp\u003e\n  Different DFS implementations have slightly different APIs and semantics, but\n  they achieve the same common goal: extremely large-scale persistent storage.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Hadoop",
			   "definition":"\u003cp\u003e\n  A popular, open-source framework that supports MapReduce jobs and many\n  other kinds of data-processing pipelines. Its central component is \u003cb\u003eHDFS\u003c/b\u003e\n  (Hadoop Distributed File System), on top of which other technologies have\n  been developed.\n\u003c/p\u003e\n",
			   "is_tech":true,
			   "url":"https://hadoop.apache.org/"
			}
		 ]
	  },
	  {
		 "name":"Security And HTTPS",
		 "available":true,
		 "date_added":"",
		 "ready":true,
		 "video_url":"https://player.vimeo.com/video/457651487",
		 "description":"While network security is of critical importance to virtually any system, it's beyond the scope of most system design interviews.\n\nThat being said, having even a cursory understanding of a few key concepts could very well materialize into the edge you need to ace your interview and secure—pun perhaps intended—a job offer.",
		 "thumbnail_url":"https://i.vimeocdn.com/video/957012171.jpg?mw=400\u0026mh=225\u0026q=70",
		 "video_duration":36,
		 "free_video_url":null,
		 "release_date":"2020-09-16T12:00:00-04:00",
		 "has_code_example":true,
		 "num_pre_reqs":4,
		 "pre_reqs":[
			{
			   "name":"Client",
			   "definition":"\u003cp\u003e\n  A machine or process that requests data or service from a server.\n\u003c/p\u003e\n\u003cp\u003e\n  Note that a single machine or piece of software can be both a client and a\n  server at the same time. For instance, a single machine could act as a server\n  for end users and as a client for a database.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Server",
			   "definition":"\u003cp\u003e\n  A machine or process that provides data or service for a client, usually by\n  listening for incoming network calls.\n\u003c/p\u003e\n\u003cp\u003e\n  Note that a single machine or piece of software can be both a client and a\n  server at the same time. For instance, a single machine could act as a server\n  for end users and as a client for a database.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"IP Packet",
			   "definition":"\u003cp\u003e\n  Sometimes more broadly referred to as just a (network) \u003cb\u003epacket\u003c/b\u003e, an IP\n  packet is effectively the smallest unit used to describe data being sent over\n  \u003cb\u003eIP\u003c/b\u003e, aside from bytes. An IP packet consists of:\n\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003e\n    an \u003cb\u003eIP header\u003c/b\u003e, which contains the source and destination\n    \u003cb\u003eIP addresses\u003c/b\u003e as well as other information related to the network\n  \u003c/li\u003e\n  \u003cli\u003ea \u003cb\u003epayload\u003c/b\u003e, which is just the data being sent over the network\u003c/li\u003e\n\u003c/ul\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"HTTP",
			   "definition":"\u003cp\u003e\n  The \u003cb\u003eH\u003c/b\u003eyper\u003cb\u003eT\u003c/b\u003eext \u003cb\u003eT\u003c/b\u003eransfer \u003cb\u003eP\u003c/b\u003erotocol is a very common network protocol implemented on top\n  of TCP. Clients make HTTP requests, and servers respond with a response.\n\u003c/p\u003e\n\u003cp\u003e\n  Requests typically have the following schema: \u003cbr /\u003e\n  \u003cpre\u003ehost: string (example: algoexpert.io)\nport: integer (example: 80 or 443)\nmethod: string (example: GET, PUT, POST, DELETE, OPTIONS or PATCH)\nheaders: \u003ckey, value\u003e pair list (example: \"Content-Type\" =\u003e \"application/json\")\nbody: opaque sequence of bytes\u003c/pre\u003e\n\u003c/p\u003e\n\u003cp\u003e\n  Responses typically have the following schema: \u003cbr /\u003e\n  \u003cpre\u003e\nstatus code: integer (example: 200, 401)\nheaders: \u003ckey, value\u003e pair list (example: \"Content-Length\" =\u003e 1238)\nbody: opaque sequence of bytes\u003c/pre\u003e\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			}
		 ],
		 "num_key_terms":9,
		 "key_terms":[
			{
			   "name":"Man-In-The-Middle Attack",
			   "definition":"\u003cp\u003e\n  An attack in which the attacker intercepts a line of communication that is\n  thought to be private by its two communicating parties.\n\u003c/p\u003e\n\u003cp\u003e\n  If a malicious actor intercepted and mutated an IP packet on its way from a\n  client to a server, that would be a man-in-the-middle attack.\n\u003c/p\u003e\n\u003cp\u003e\n  MITM attacks are the primary threat that encryption and \u003cb\u003eHTTPS\u003c/b\u003e aim to\n  defend against.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Symmetric Encryption",
			   "definition":"\u003cp\u003e\n  A type of encryption that relies on only a single key to both encrypt and\n  decrypt data. The key must be known to all parties involved in communication\n  and must therefore typically be shared between the parties at one point or\n  another.\n\u003c/p\u003e\n\u003cp\u003e\n  Symmetric-key algorithms tend to be faster than their asymmetric counterparts.\n\u003c/p\u003e\n\u003cp\u003e\n  The most widely used symmetric-key algorithms are part of the Advanced\n  Encryption Standard (\u003cb\u003eAES\u003c/b\u003e).\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Asymmetric Encryption",
			   "definition":"\u003cp\u003e\n  Also known as public-key encryption, asymmetric encryption relies on two\n  keys—a public key and a private key—to encrypt and decrypt data. The keys are\n  generated using cryptographic algorithms and are mathematically connected such\n  that data encrypted with the public key can only be decrypted with the private\n  key.\n\u003c/p\u003e\n\u003cp\u003e\n  While the private key must be kept secure to maintain the fidelity of this\n  encryption paradigm, the public key can be openly shared.\n\u003c/p\u003e\n\u003cp\u003e\n  Asymmetric-key algorithms tend to be slower than their symmetric counterparts.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"AES",
			   "definition":"\u003cp\u003e\n  Stands for \u003cb\u003eAdvanced Encryption Standard\u003c/b\u003e. AES is a widely used\n  encryption standard that has three symmetric-key algorithms (AES-128, AES-192,\n  and AES-256).\n\u003c/p\u003e\n\u003cp\u003e\n  Of note, AES is considered to be the \"gold standard\" in encryption and is even\n  used by the U.S. National Security Agency to encrypt top secret information.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"HTTPS",
			   "definition":"\u003cp\u003e\n  The \u003cb\u003eH\u003c/b\u003eyper\u003cb\u003eT\u003c/b\u003eext \u003cb\u003eT\u003c/b\u003eransfer \u003cb\u003eP\u003c/b\u003erotocol \u003cb\u003eS\u003c/b\u003eecure is\n  an extension of \u003cb\u003eHTTP\u003c/b\u003e that's used for secure communication online. It\n  requires servers to have trusted certificates (usually\n  \u003cb\u003eSSL certificates\u003c/b\u003e) and uses the Transport Layer Security (\u003cb\u003eTLS\u003c/b\u003e), a\n  security protocol built on top of \u003cb\u003eTCP\u003c/b\u003e, to encrypt data communicated\n  between a client and a server.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"TLS",
			   "definition":"\u003cp\u003e\n  The \u003cb\u003eT\u003c/b\u003eransport \u003cb\u003eL\u003c/b\u003eayer \u003cb\u003eS\u003c/b\u003eecurity is a security protocol over\n  which \u003cb\u003eHTTP\u003c/b\u003e runs in order to achieve secure communication online. \"HTTP\n  over TLS\" is also known as \u003cb\u003eHTTPS\u003c/b\u003e.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"SSL Certificate",
			   "definition":"\u003cp\u003e\n  A digital certificate granted to a server by a \u003cb\u003ecertificate authority\u003c/b\u003e.\n  Contains the server's public key, to be used as part of the\n  \u003cb\u003eTLS handshake\u003c/b\u003e process in an \u003cb\u003eHTTPS\u003c/b\u003e connection.\n\u003c/p\u003e\n\u003cp\u003e\n  An SSL certificate effectively confirms that a public key belongs to the\n  server claiming it belongs to them. SSL certificates are a crucial defense\n  against \u003cb\u003eman-in-the-middle attacks\u003c/b\u003e.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"Certificate Authority",
			   "definition":"\u003cp\u003e\n  A trusted entity that signs digital certificates—namely, SSL certificates that\n  are relied on in \u003cb\u003eHTTPS\u003c/b\u003e connections.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"TLS Handshake",
			   "definition":"\u003cp\u003e\n  The process through which a client and a server communicating over\n  \u003cb\u003eHTTPS\u003c/b\u003e exchange encryption-related information and establish a secure\n  communication. The typical steps in a TLS handshake are roughly as follows:\n\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003e\n    The client sends a \u003cb\u003eclient hello\u003c/b\u003e—a string of random bytes—to the\n    server.\n  \u003c/li\u003e\n  \u003cli\u003e\n    The server responds with a \u003cb\u003eserver hello\u003c/b\u003e—another string of random\n    bytes—as well as its \u003cb\u003eSSL certificate\u003c/b\u003e, which contains its\n    \u003cb\u003epublic key\u003c/b\u003e.\n  \u003c/li\u003e\n  \u003cli\u003e\n    The client verifies that the certificate was issued by a\n    \u003cb\u003ecertificate authority\u003c/b\u003e and sends a \u003cb\u003epremaster secret\u003c/b\u003e—yet another\n    string of random bytes, this time encrypted with the server's public key—to\n    the server.\n  \u003c/li\u003e\n  \u003cli\u003e\n    The client and the server use the client hello, the server hello, and the\n    premaster secret to the generate the same \u003cb\u003esymmetric-encryption\u003c/b\u003e session keys,\n    to be used to encrypt and decrypt all data communicated during the remainder\n    of the connection.\n  \u003c/li\u003e\n\u003c/ul\u003e\n",
			   "is_tech":false,
			   "url":null
			}
		 ]
	  },
	  {
		 "name":"API Design",
		 "available":true,
		 "date_added":"",
		 "ready":true,
		 "video_url":"https://player.vimeo.com/video/458471340",
		 "description":"So you've mastered all lessons hitherto on SystemsExpert, and you now feel confident you could ace any systems design interview. That's wonderful and all, but...could you pass an API design interview?\n\nIf you're sweating bullets, then sweat no more. This final video is the last piece of the puzzle you need to become a true Systems Expert.",
		 "thumbnail_url":"https://i.vimeocdn.com/video/958356995.jpg?mw=400\u0026mh=225\u0026q=70",
		 "video_duration":29,
		 "free_video_url":null,
		 "release_date":"2020-09-16T12:00:00-04:00",
		 "has_code_example":true,
		 "num_pre_reqs":4,
		 "pre_reqs":[
			{
			   "name":"HTTP",
			   "definition":"\u003cp\u003e\n  The \u003cb\u003eH\u003c/b\u003eyper\u003cb\u003eT\u003c/b\u003eext \u003cb\u003eT\u003c/b\u003eransfer \u003cb\u003eP\u003c/b\u003erotocol is a very common network protocol implemented on top\n  of TCP. Clients make HTTP requests, and servers respond with a response.\n\u003c/p\u003e\n\u003cp\u003e\n  Requests typically have the following schema: \u003cbr /\u003e\n  \u003cpre\u003ehost: string (example: algoexpert.io)\nport: integer (example: 80 or 443)\nmethod: string (example: GET, PUT, POST, DELETE, OPTIONS or PATCH)\nheaders: \u003ckey, value\u003e pair list (example: \"Content-Type\" =\u003e \"application/json\")\nbody: opaque sequence of bytes\u003c/pre\u003e\n\u003c/p\u003e\n\u003cp\u003e\n  Responses typically have the following schema: \u003cbr /\u003e\n  \u003cpre\u003e\nstatus code: integer (example: 200, 401)\nheaders: \u003ckey, value\u003e pair list (example: \"Content-Length\" =\u003e 1238)\nbody: opaque sequence of bytes\u003c/pre\u003e\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"JSON",
			   "definition":"\u003cp\u003eA file format heavily used in APIs and configuration. Stands for \u003cb\u003eJ\u003c/b\u003eava\u003cb\u003eS\u003c/b\u003ecript \u003cb\u003eO\u003c/b\u003ebject \u003cb\u003eN\u003c/b\u003eotation\u003c/b\u003e. Example:\u003c/p\u003e\n\u003cpre\u003e{\n   \"version\": 1.0,\n   \"name\": \"AlgoExpert Configuration\"\n}\u003c/pre\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"YAML",
			   "definition":"\u003cp\u003eA file format mostly used in configuration. Example:\u003c/p\u003e\n\u003cpre\u003e\nversion: 1.0\nname: AlgoExpert Configuration\n\u003c/pre\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"ACL",
			   "definition":"\u003cp\u003e\n  Short for \u003cb\u003eAccess-Control List\u003c/b\u003e. This term is often used to refer to a\n  permissioning model: which users in a system can perform which operations. For\n  instance, APIs often come with ACLs defining which users can delete, edit, or\n  view certain entities.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			}
		 ],
		 "num_key_terms":2,
		 "key_terms":[
			{
			   "name":"Pagination",
			   "definition":"\u003cp\u003e\n  When a network request potentially warrants a really large response, the\n  relevant API might be designed to return only a single \u003cb\u003epage\u003c/b\u003e\n  of that response (i.e., a limited portion of the response), accompanied by an\n  identifier or token for the client to request the next page if desired.\n\u003c/p\u003e\n\u003cp\u003e\n  Pagination is often used when designing \u003cb\u003eList\u003c/b\u003e endpoints. For instance,\n  an endpoint to list videos on the YouTube Trending page could return a huge\n  list of videos. This wouldn't perform very well on mobile devices due to the\n  lower network speeds and simply wouldn't be optimal, since most users will\n  only ever scroll through the first ten or twenty videos. So, the API could be\n  designed to respond with only the first few videos of that list; in this case,\n  we would say that the API response is \u003cb\u003epaginated\u003c/b\u003e.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			},
			{
			   "name":"CRUD Operations",
			   "definition":"\u003cp\u003e\n  Stands for \u003cb\u003eCreate\u003c/b\u003e, \u003cb\u003eRead\u003c/b\u003e, \u003cb\u003eUpdate\u003c/b\u003e,\n  \u003cb\u003eDelete\u003c/b\u003e Operations. These four operations often serve as the bedrock of a\n  functioning system and therefore find themselves at the core of many APIs.\n  The term \u003cb\u003eCRUD\u003c/b\u003e is very likely to come up during an API-design interview.\n\u003c/p\u003e\n",
			   "is_tech":false,
			   "url":null
			}
		 ]
	  }
   ],
   "design":[
	  {
		 "name":"Design A Code-Deployment System",
		 "available":true,
		 "date_added":"",
		 "ready":true,
		 "video_url":"https://player.vimeo.com/video/391631170",
		 "release_date":"2020-02-15T12:00:00-05:00",
		 "prompt":"\u003cp\u003eDesign a global and fast code-deployment system.\u003c/p\u003e\n",
		 "walkthrough":[
			{
			   "title":"Gathering System Requirements",
			   "content":"\u003cp\u003e\n  As with any systems design interview question, the first thing that we want to\n  do is to gather system requirements; we need to figure out what system we're\n  building exactly.\n\u003c/p\u003e\n\u003cp\u003e\n  From the answers we were given to our clarifying questions (see Prompt Box),\n  we're building a system that involves repeatedly (in the order of thousands of\n  times per day) building and deploying code to \u003cb\u003ehundreds of thousands\u003c/b\u003e of\n  machines spread out across \u003cb\u003e5-10 regions\u003c/b\u003e around the world.\n\u003c/p\u003e\n\u003cp\u003e\n  Building code will involve grabbing snapshots of source code using commit SHA\n  identifiers; beyond that, we can assume that the actual implementation details\n  of the building action are taken care of. In other words, we don't need to\n  worry about how we would build JavaScript code or C++ code; we just need to\n  design the system that enables the repeated building of code.\n\u003c/p\u003e\n\u003cp\u003e\n  Building code will take up to \u003cb\u003e15 minutes\u003c/b\u003e, it'll result in a binary file of up\n  to \u003cb\u003e10GB\u003c/b\u003e, and we want to have the entire deployment process (building and\n  deploying code to our target machines) take at most \u003cb\u003e30 minutes\u003c/b\u003e.\n\u003c/p\u003e\n\u003cp\u003e\n  Each build will need a clear end-state (\u003cb\u003eSUCCESS\u003c/b\u003e or \u003cb\u003eFAILURE\u003c/b\u003e), and though we\n  care about availability (2 to 3 nines), we don't need to optimize too much on\n  this dimension.\n\u003c/p\u003e"
			},
			{
			   "title":"Coming Up With A Plan",
			   "content":"\u003cp\u003e\n  It's important to organize ourselves and to lay out a clear plan regarding how\n  we're going to tackle our design. What are the major, distinguishable\n  components of our how system?\n\u003c/p\u003e\n\u003cp\u003e\n  It seems like this system can actually very simply be divided into two clear\n  subsystems:\n\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003ethe Build System that builds code into binaries\u003c/li\u003e\n  \u003cli\u003e\n    the Deployment System that deploys binaries to our machines across the\n    world\n  \u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\n  Note that these subsystems will of course have many components themselves, but\n  this is a very straightforward initial way to approach our problem.\n\u003c/p\u003e"
			},
			{
			   "title":"Build System -- General Overview",
			   "content":"\u003cp\u003e\n  From a high-level perspective, we can call the process of building code into a\n  binary a \u003cb\u003ejob\u003c/b\u003e, and we can design our build system as a queue of jobs.\n  Jobs get added to the queue, and each job has a commit identifier (the commit\n  SHA) for what version of the code it should build and the name of the artifact\n  that will be created (the name of the resulting binary). Since we're agnostic\n  to the type of the code being built, we can assume that all languages are\n  handled automatically here.\n\u003c/p\u003e\n\u003cp\u003e\n  We can have a pool of servers (workers) that are going to handle all of these\n  jobs. Each worker will repeatedly take jobs off the queue (in a\n  \u003cb\u003eFIFO manner\u003c/b\u003e—no prioritization for now), build the relevant binaries\n  (again, we're assuming that the actual implementation details of building code are\n  given to us), and write the resulting binaries to blob storage (\u003cb\u003eGoogle\n  Cloud Storage\u003c/b\u003e or \u003cb\u003eS3\u003c/b\u003e for instance). Blob storage makes\n  sense here, because binaries are literally blobs of data.\n\u003c/p\u003e"
			},
			{
			   "title":"Build System -- Job Queue",
			   "content":"\u003cp\u003e\n  A naive design of the job queue would have us implement it in memory (just as\n  we would implement a queue in coding interviews), but this implementation is\n  very problematic; if there's a failure in our servers that hold this queue, we\n  lose the entire state of our jobs: queued jobs and past jobs.\n\u003c/p\u003e\n\u003cp\u003e\n  It seems like we would be unnecessarily complicating matters by trying to\n  optimize around this in-memory type of storage, so we're likely better off\n  implementing the queue using a SQL database.\n\u003c/p\u003e"
			},
			{
			   "title":"Build System -- SQL Job Queue",
			   "content":"\u003cp\u003e\n  We can have a \u003cb\u003ejobs\u003c/b\u003e table in our SQL database where every record in the\n  database represents a job, and we can use record-creation timestamps as the\n  queue's ordering mechanism.\n\u003c/p\u003e\n\u003cp\u003e\n  Our table will be:\n\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003e\n    id: \u003ci\u003estring\u003c/i\u003e, the ID of the job, autogenerated\n  \u003c/li\u003e\n  \u003cli\u003e\n    created_at: \u003ci\u003etimestamp\u003c/i\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    commit_sha: \u003ci\u003estring\u003c/i\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n      name: \u003ci\u003estring\u003c/i\u003e, the pointer to the job's eventual binary in blob storage\n  \u003c/li\u003e\n  \u003cli\u003e\n    status: \u003ci\u003estring\u003c/i\u003e, \u003cb\u003eQUEUED\u003c/b\u003e, \u003cb\u003eRUNNING\u003c/b\u003e, \u003cb\u003eSUCCEEDED\u003c/b\u003e, \u003cb\u003eFAILED\u003c/b\u003e\n  \u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\n  We can implement the actual dequeuing mechanism by looking at the oldest\n  creation_timestamp with a QUEUED status. This means that we'll likely want to\n  index our table on both created_at and status.\n\u003c/p\u003e"
			},
			{
			   "title":"Build System -- Concurrency",
			   "content":"\u003cp\u003e\n  \u003cb\u003eACID transactions\u003c/b\u003e will make it safe for potentially hundreds of workers\n  to grab jobs off the queue without unintentionally running the same job twice\n  (we'll avoid race conditions). Our actual transaction will look like this:\n\u003c/p\u003e\n\n\u003cpre\u003e\n  BEGIN TRANSACTION;\n  SELECT * FROM jobs_table WHERE status = 'QUEUED' ORDER BY created_at ASC LIMIT 1;\n  // if there's none, we ROLLBACK;\n  UPDATE jobs_table SET status = 'RUNNING' WHERE id = id from previous query;\n  COMMIT;\n\u003c/pre\u003e\n\u003cp\u003e\n  All of the workers will be running this transaction every so often to dequeue\n  the next job; let's say every 5 seconds. If we arbitrarily assume that we'll\n  have 100 workers sharing the same queue, we'll have 100/5 = 20 reads per\n  second, which is very easy to handle for a SQL database.\n\u003c/p\u003e"
			},
			{
			   "title":"Build System -- Lost Jobs",
			   "content":"\u003cp\u003e\n  Since we're designing a large-scale system, we have to expect and handle edge\n  cases. Here, what if there's a network partition with our workers or one of\n  our workers dies mid-build? Since builds last around 15 minutes on average,\n  this will very likely happen. In this case, we want to avoid having a \"lost\n  job\" that we were never made aware of, and with our current design, the job\n  will remain RUNNING forever. How do we handle this?\n\u003c/p\u003e\n\u003cp\u003e\n  We could have an extra column on our \u003cb\u003ejobs\u003c/b\u003e table called last_heartbeat.\n  This will be updated in a heartbeat fashion by the worker running a particular\n  job, where that worker will update the relevant row in the table every 3-5\n  minutes to just let us know that it's still running the job.\n\u003c/p\u003e\n\u003cp\u003e\n  We can then have a completely separate service that polls the table every so\n  often (say, every 5 minutes, depending on how responsive we want this build\n  system to be), checks all of the \u003cb\u003eRUNNING\u003c/b\u003e jobs, and if their last_heartbeat was\n  last modified longer than 2 heartbeats ago (we need some margin of error\n  here), then something's likely wrong, and this service can reset the status of\n  the relevant jobs to \u003cb\u003eQUEUED\u003c/b\u003e, which would effectively bring them back to the\n  front of the queue.\n\u003c/p\u003e\n\u003cp\u003e\n  The transaction that this auxiliary service will perform will look something\n  like this:\n\u003c/p\u003e\n\u003cpre\u003e\n  UPDATE jobs_table SET status = 'QUEUED' WHERE\n    status = 'RUNNING' AND\n    last_heartbeat \u003c NOW() - 10 minutes;\n\u003c/pre\u003e"
			},
			{
			   "title":"Build System -- Scale Estimation",
			   "content":"\u003cp\u003e\n  We previously arbitrarily assumed that we would have 100 workers, which made\n  our SQL-database queue able to handle the expected load. We should try to\n  estimate if this number of workers is actually realistic.\n\u003c/p\u003e\n\u003cp\u003e\n  With some back-of-the-envelope math, we can see that, since a build can take\n  up to 15 minutes, a single worker can run 4 jobs per hour, or ~100 (96) jobs\n  per day. Given thousands of builds per day (say, 5000-10000), this means that\n  we would need \u003cb\u003e50-100 workers\u003c/b\u003e (5000 / 100). So our arbitrary figure was\n  accurate.\n\u003c/p\u003e\n\u003cp\u003e\n  Even if the builds aren't uniformly spread out (in other words, they peak\n  during work hours), our system scales horizontally very easily. We can\n  automatically add or remove workers whenever the load warrants it. We can also\n  scale our system vertically by making our workers more powerful, thereby\n  reducing the build time.\n\u003c/p\u003e"
			},
			{
			   "title":"Build System -- Storage",
			   "content":"\n\u003cp\u003e\n  We previously mentioned that we would store binaries in blob storage\n  (\u003cb\u003eGCS\u003c/b\u003e). Where does this storage fit into our queueing system exactly?\n\u003c/p\u003e\n\u003cp\u003e\n  When a worker completes a build, it can store the binary in GCS\n  \u003ci\u003ebefore\u003c/i\u003e updating the relevant row in the \u003cb\u003ejobs\u003c/b\u003e table. This will\n  ensure that a binary has been persisted before its relevant job is marked as\n  \u003cb\u003eSUCCEEDED\u003c/b\u003e.\n\u003c/p\u003e\n\u003cp\u003e\n  Since we're going to be deploying our binaries to machines spread across the\n  world, it'll likely make sense to have regional storage rather than just a\n  single global blob store.\n\u003c/p\u003e\n\u003cp\u003e\n  We can design our system based on regional clusters around the world (in our\n  5-10 global regions). Each region can have a blob store (a regional GCS\n  bucket). Once a worker successfully stores a binary in our main blob store,\n  the worker is released and can run another job, while the main blob store\n  performs some asynchronous replication to store the binary in all of the\n  regional GCS buckets. Given 5-10 regions and 10GB files, this step should take\n  no more than 5-10 minutes, bringing our total build-and-deploy duration so far\n  to roughly 20-25 minutes (15 minutes for a build and 5-10 minutes for global\n  replication of the binary).\n\u003c/p\u003e\n\u003cp\u003e\u003c/p\u003e"
			},
			{
			   "title":"Deployment System -- General Overview",
			   "content":"\u003cp\u003e\n  From a high-level perspective, our actual deployment system will need to allow\n  for the very fast distribution of 10GB binaries to hundreds of thousands of\n  machines across all of our global regions. We're likely going to want some\n  service that tells us when a binary has been replicated in all regions,\n  another service that can serve as the source of truth for what binary should\n  currently be run on all machines, and finally a peer-to-peer-network design\n  for our actual machines across the world.\n\u003c/p\u003e"
			},
			{
			   "title":"Deployment System -- Replication-Status Service",
			   "content":"\u003cp\u003e\n  We can have a global service that continuously checks all regional GCS buckets\n  and aggregates the replication status for successful builds (in other words,\n  checks that a given binary in the main blob store has been replicated across\n  all regions). Once a binary has been replicated across all regions, this\n  service updates a separate SQL database with rows containing the name of a\n  binary and a \u003cb\u003ereplication_status\u003c/b\u003e. Once a binary has a \"complete\"\n  \u003cb\u003ereplication_status\u003c/b\u003e, it's officially deployable.\n\u003c/p\u003e"
			},
			{
			   "title":"Deployment System -- Blob Distribution",
			   "content":"\n\u003cp\u003e\n  Since we're going to deploy 10 GBs to hundreds of thousands of machines, even\n  with our regional clusters, having each machine download a 10GB file one after\n  the other from a regional blob store is going to be extremely slow. A\n  peer-to-peer-network approach will be much faster and will allow us to hit our\n  30-minute time frame for deployments. All of our regional clusters will behave\n  as peer-to-peer networks.\n\u003c/p\u003e"
			},
			{
			   "title":"Deployment System -- Trigger",
			   "content":"\n\u003cp\u003e\n  Let's describe what happens when an engineer presses a button on some internal\n  UI that says \"Deploy build/binary B1 to every machine globally\". This is the\n  action that triggers the binary downloads on all the regional peer-to-peer\n  networks.\n\u003c/p\u003e\n\u003cp\u003e\n  To simplify this process and to support having multiple builds getting\n  deployed concurrently, we can design this in a goal-state oriented manner.\n\u003c/p\u003e\n\u003cp\u003e\n  The goal-state will be the desired build version at any point in time and\n  will look something like: \"current_build: \u003cb\u003eB1\u003c/b\u003e\", and this can be stored\n  in some dynamic configuration service (a \u003cb\u003ekey-value store\u003c/b\u003e like\n  \u003cb\u003eEtcd\u003c/b\u003e or \u003cb\u003eZooKeeper\u003c/b\u003e). We'll have a global goal-state as well as\n  regional goal-states.\n\u003c/p\u003e\n\u003cp\u003e\n  Each regional cluster will have a K-V store that holds configuration for that\n  cluster about what builds should be running on that cluster, and we'll also\n  have a global K-V store.\n\u003c/p\u003e\n\u003cp\u003e\n  When an engineer clicks the \"Deploy build/binary B1\" button, our global K-V\n  store's build_version will get updated. Regional K-V stores will be\n  continuously polling the global K-V store (say, every 10 seconds) for updates\n  to the build_version and will update themselves accordingly.\n\u003c/p\u003e\n\u003cp\u003e\n  Machines in the clusters/regions will be polling the relevant regional K-V\n  store, and when the build_version changes, they'll try to fetch that build\n  from the P2P network and run the binary.\n\u003c/p\u003e"
			},
			{
			   "title":"System Diagram",
			   "content":"\u003cimg\n  width=\"100%\"\n  src=\"https://prod.api.algoexpert.io/api/problems/v1/assets?path=code_deployment.png\"\n  alt=\"Final Systems Architecture\"\n/\u003e",
			   "image": {
				   "name": "code_deployment",
				   "extension": "png"
			   }
			}
		 ],
		 "hints":[
			{
			   "question":"What exactly do we mean by a code-deployment system? Are we talking about building, testing, and shipping code? ",
			   "answer":"We want to design a system that takes code, builds it into a binary (an opaque blob of data—the compiled code), and deploys the result globally in an efficient and scalable way. We don't need to worry about testing code; let's assume that's already covered."
			},
			{
			   "question":"What part of the software-development lifecycle, so to speak, are we designing this for? Is this process of building and deploying code happening when code is being submitted for code review, when code is being merged into a codebase, or when code is being shipped?",
			   "answer":"Once code is merged into the trunk or master branch of a central code repository, engineers should be able to trigger a build and deploy that build (through a UI, which we're not designing). At that point, the code has already been reviewed and is ready to ship. So to clarify, we're not designing the system that handles code being submitted for review or being merged into a master branch—just the system that takes merged code, builds it, and deploys it."
			},
			{
			   "question":"Are we essentially trying to ship code to production by sending it to, presumably, all of our application servers around the world?",
			   "answer":"Yes, exactly."
			},
			{
			   "question":"How many machines are we deploying to? Are they located all over the world?",
			   "answer":"We want this system to scale massively to hundreds of thousands of machines spread across 5-10 regions throughout the world."
			},
			{
			   "question":"This sounds like an internal system. Is there any sense of urgency in deploying this code? Can we afford failures in the deployment process? How fast do we want a single deployment to take?",
			   "answer":"This is an internal system, but we'll want to have decent availability, because many outages are resolved by rolling forward or rolling back buggy code, so this part of the infrastructure may be necessary to avoid certain terrible situations. In terms of failure tolerance, any build should eventually reach a SUCCESS or FAILURE state. Once a binary has been successfully built, it should be shippable to all machines globally within 30 minutes."
			},
			{
			   "question":"So it sounds like we want our system to be available, but not necessarily highly available, we want a clear end-state for builds, and we want the entire process of building and deploying code to take roughly 30 minutes. Is that correct?",
			   "answer":"Yes, that's correct."
			},
			{
			   "question":"How often will we be building and deploying code, how long does it take to build code, and how big can the binaries that we'll be deploying get?",
			   "answer":"Engineering teams deploy hundreds of services or web applications, thousands of times per day; building code can take up to 15 minutes; and the final binaries can reach sizes of up to 10 GB. The fact that we might be dealing with hundreds of different applications shouldn't matter though; you're just designing the build pipeline and deployment system, which are agnostic to the types of applications that are getting deployed."
			},
			{
			   "question":"When building code, how do we have access to the actual code? Is there some sort of reference that we can use to grab code to build?",
			   "answer":"Yes; you can assume that you'll be building code from commits that have been merged into a master branch. These commits have SHA identifiers (effectively arbitrary strings) that you can use to download the code that needs to be built."
			}
		 ]
	  },
	  {
		 "name":"Design AlgoExpert",
		 "available":false,
		 "date_added":"",
		 "ready":true,
		 "video_url":"https://player.vimeo.com/video/390429504",
		 "release_date":"2020-02-10T00:00:00-05:00",
		 "prompt":"",
		 "walkthrough":[
			{
			   "title":"Gathering System Requirements",
			   "content":"\u003cp\u003e\n  As with any systems design interview question, the first thing that we want to\n  do is to gather system requirements; we need to figure out what system we're\n  building exactly.\n\u003c/p\u003e\n\u003cp\u003e\n  From the answers we were given to our clarifying questions (see Prompt Box),\n  we're building the core AlgoExpert user flow, which includes users landing on\n  the website, accessing questions, marking them as complete, writing code,\n  running code, and having their code saved.\n\u003c/p\u003e\n\u003cp\u003e\n  We don't need to worry about payments or authentication, and we don't need to\n  go too deep into the code-execution engine.\n\u003c/p\u003e\n\u003cp\u003e\n  We're building this platform for a global audience, with an emphasis on U.S.\n  and India users, and we don't need to overly optimize our system's\n  availability. We probably don't need more than two or three nines, because\n  we're not building a health or security system, and this gets us somewhere\n  between \u003cb\u003e8 hours and 3 days\u003c/b\u003e of downtime per year, which is reasonable.\n  All in all, this means that we don't need to worry \u003ci\u003etoo\u003c/i\u003e much about\n  availability.\n\u003c/p\u003e\n\u003cp\u003e\n  We care about latency and throughput within reason, but apart from the\n  code-execution engine, this doesn't seem like a particularly difficult aspect\n  of our system.\n\u003c/p\u003e"
			},
			{
			   "title":"Coming Up With A Plan",
			   "content":"\u003cp\u003e\n  It's important to organize ourselves and to lay out a clear plan regarding how\n  we're going to tackle our design. What are the major, distinguishable\n  components of our how system?\n\u003c/p\u003e\n\u003cp\u003e\n  On the one hand, AlgoExpert has a lot of static content; the entire home page,\n  for instance, is static, and it has a lot of images. On the other hand,\n  AlgoExpert isn't \u003ci\u003ejust\u003c/i\u003e a static website; it clearly has a lot of dynamic\n  content that users themselves can generate (code that they can write, for\n  example). So we'll need to have a robust API backing our UI, and given that\n  user content gets saved on the website, we'll also need a database backing our\n  API.\n\u003c/p\u003e\n\u003cp\u003e\n  We can divide our system into 3 core components:\n\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003eStatic UI content\u003c/li\u003e\n  \u003cli\u003e\n    Accessing and interacting with questions (question completion status,\n    saving solutions, etc.)\n  \u003c/li\u003e\n  \u003cli\u003eAbility to run code\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\n  Note that the second bullet point will likely get further divided.\n\u003c/p\u003e"
			},
			{
			   "title":"Static UI Content",
			   "content":"\u003cp\u003e\n  For the UI static content, we can put public assets like images and JavaScript\n  bundles in a blob store: \u003cb\u003eS3 or Google Cloud Storage\u003c/b\u003e. Since we're catering to a\n  global audience and we care about having a responsive website (especially the\n  home page of the website), we might want to use a\n  \u003cb\u003eContent Delivery Network\u003c/b\u003e (CDN) to serve that content. This is\n  especially important for a better mobile experience because of the slow\n  connections that phones use.\n\u003c/p\u003e"
			},
			{
			   "title":"Main Clusters And Load Balancing",
			   "content":"\u003cp\u003e\n  For our main backend servers, we can have \u003cb\u003e2 primary clusters\u003c/b\u003e in the 2\n  important regions: U.S. and India.\n\u003c/p\u003e\n\u003cp\u003e\n  We can have some DNS load balancing to route API requests to the cluster\n  closest to the user issuing the requests, and within a region, we can have\n  some \u003cb\u003epath-based load balancing\u003c/b\u003e to separate our services (payments,\n  authentication, code execution, etc.), especially since the code execution\n  platform will probably need to run on different kinds of servers compared to\n  those of the rest of the API. Each service can probably have a set of servers,\n  and we can do some round-robin load balancing at that level (this is probably\n  handled directly at the path-based load balancing layer).\n\u003c/p\u003e"
			},
			{
			   "title":"Static API Content",
			   "content":"\u003cp\u003e\n  There's a lot of static API content on AlgoExpert: namely, the list of\n  questions and all of their solutions. We can store all of this data in a blob\n  store for simplicity.\n\u003c/p\u003e"
			},
			{
			   "title":"Caching",
			   "content":"\u003cp\u003e\n  We can implement 2 layers of caching for this static API content.\n\u003c/p\u003e\n\u003cp\u003e\n  We can have client-side caching; this will improve the user experience on the\n  platform (users will only need to load questions once per session), and this\n  will reduce the load on our backend servers (this will probably save 2-3\n  network calls per session).\n\u003c/p\u003e\n\u003cp\u003e\n  We can also have some in-memory caching on our servers. If we approximate 100\n  questions with 10 languages and 5KB per solution, this should be less than\n  \u003cb\u003e100 * 10 * 5000 bytes = 5MB\u003c/b\u003e of total data to keep in memory, which\n  is perfectly fine.\n\u003c/p\u003e\n\n\u003cp\u003e\n  Since we were told that we want to make changes to static API content every\n  couple of days and that we want those changes to be reflected in production as\n  soon as possible, we can invalidate, evict and replace the data in our\n  server-side caches every 30 minutes or so.\n\u003c/p\u003e"
			},
			{
			   "title":"Access Control",
			   "content":"\u003cp\u003e\n  Whenever you're desiging a system, it's important to think about any potential\n  access control that needs to be implemented. In the case of AlgoExpert,\n  there's straightforward access control with regards to question content: users\n  who haven't purchased AlgoExpert can't access individual questions. We can\n  implement this fairly easily by just making some internal API call whenever a\n  user requests our static API content to figure out if the user owns the\n  product before returning the full content for questions.\n\u003c/p\u003e"
			},
			{
			   "title":"User Data Storage",
			   "content":"\u003cp\u003e\n  For user data, we have to design the storage of question completion status and\n  of user solutions to questions. Since this data will have to be queried a lot,\n  a SQL database like \u003cb\u003ePostgres or MySQL\u003c/b\u003e seems like a good choice.\n\u003c/p\u003e\n\u003cp\u003e\n  We can have 2 tables. The first table might be\n  \u003cb\u003equestion_completion_status\u003c/b\u003e, which would probably have the following\n  columns:\n\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003eid: \u003ci\u003einteger\u003c/i\u003e, primary key (an auto-incremented integer for instance)\u003c/li\u003e\n  \u003cli\u003e\n    user_id: \u003ci\u003estring\u003c/i\u003e, references the id of the user (can be obtained from\n    auth)\n  \u003c/li\u003e\n  \u003cli\u003equestion_id: \u003ci\u003estring\u003c/i\u003e, references the id of the question\u003c/li\u003e\n  \u003cli\u003e\n    completion_status: \u003ci\u003estring\u003c/i\u003e, enum to represent the completion status of the\n    question\n  \u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\n  We could have a uniqueness constraint on (user_id, question_id) and an index\n  on user_id for fast querying.\n\u003c/p\u003e\n\n\u003cp\u003eThe second table might be \u003cb\u003euser_solutions\u003c/b\u003e:\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003eid: \u003ci\u003einteger\u003c/i\u003e, primary key (an auto-incremented integer for instance)\u003c/li\u003e\n  \u003cli\u003e\n    user_id: \u003ci\u003estring\u003c/i\u003e, references the id of the user (can be obtained from\n    auth)\n  \u003c/li\u003e\n  \u003cli\u003equestion_id: \u003ci\u003estring\u003c/i\u003e,references the id of the question\u003c/li\u003e\n  \u003cli\u003elanguage: \u003ci\u003estring\u003c/i\u003e, references the language of the solution\u003c/li\u003e\n  \u003cli\u003esolution: \u003ci\u003estring\u003c/i\u003e, contains the user's solution\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\n  We could have a uniqueness constraint on (\u003cb\u003euser_id, question_id, language\u003c/b\u003e) and\n  an index on \u003cb\u003euser_id\u003c/b\u003e as well as one on \u003cb\u003equestion_id\u003c/b\u003e. If the number of\n  languages goes up significantly, we might also want to index on language to allow\n  for fast per-language querying so that the UI doesn't fetch all of a user's\n  solutions at the same time (this might be a lot of data for slow connections).\n\u003c/p\u003e"
			},
			{
			   "title":"Storage Performance",
			   "content":"\u003cp\u003e\n  Marking questions as complete and typing code in the coding workspace (with a\n  1-3 second \u003cb\u003edebounce\u003c/b\u003e for performance reasons) will issue API calls that\n  write to the database. We likely won't get more than \u003cb\u003e1000 writes per second\u003c/b\u003e\n  given our user numbers (assuming roughly 10,000 users on the platform at any\n  given point in time), which SQL databases can definitely handle.\n\u003c/p\u003e\n\n\u003cp\u003e\n  We can have 2 major database servers, each serving our 2 main regions: 1 in\n  North America and 1 in India (perhaps serving Southeast Asia). If need be, we\n  can add a 3rd cluster serving Europe exclusively (or other parts of the world,\n  as our platform grows).\n\u003c/p\u003e"
			},
			{
			   "title":"Inter-Region Replication",
			   "content":"\u003cp\u003e\n  Since we'll have 2 primary database servers, we'll need to keep them up to\n  date with each other. Fortunately, users on AlgoExpert don't share\n  user-generated content; this means that we don't need data that's written to 1\n  database server to immediately be written to the other database server (this\n  would likely have eliminated the latency improvements we got by having\n  regional databases).\n\u003c/p\u003e\n\u003cp\u003e\n  That being said, we \u003ci\u003edo\u003c/i\u003e need to keep our databases up to date with each\n  other, since users might travel around the world and hit a different database\n  server than their typical one.\n\u003c/p\u003e\n\u003cp\u003e\n  For this, we can have some async replication between our database servers. The\n  replication can occur every 12 hours, and we can adjust this according to\n  behavior in the system and amount of data that gets replicated across continents.\n\u003c/p\u003e"
			},
			{
			   "title":"Code Execution",
			   "content":"\u003cp\u003e\n  First of all, we should implement some rate limiting. A service like code\n  execution lends itself perfectly to rate limiting, and we can implement some\n  tier-based rate limiting using a K-V Store like \u003cb\u003eRedis\u003c/b\u003e to easily prevent DoS\n  attacks. We can limit the number of code runs to once every second, 3 times\n  per 5 seconds, and 5 times per minute. This will prevent DoS attacks through\n  the code-execution API, but it'll still allow for a good user experience when\n  running code.\n\u003c/p\u003e\n\n\u003cp\u003e\n  Since we want 1-3 seconds of latency for running code, we need to keep a set\n  of special servers--our \"workers\"-- ready to run code at all times. They can\n  each clean up after running user code (remove extra generated files as a\n  result of compilation, for example) so that they don’t need to be killed at\n  any point. Our backend servers can contact a free worker and get the response\n  from that worker when it's done running code (or if the code timed out), and\n  our servers can return that to the UI in the same request.\n\u003c/p\u003e\n\n\u003cp\u003e\n  Given that certain languages need to be compiled, we can estimate that it\n  would take on average 1 second to compile and run the code for each language.\n  People don’t run code that often, so we can expect 10 run-codes per second in\n  total given roughly 10,000 users on the website at once, so we'll probably\n  need 10-100 machines to satisfy our original latency requirement of 1-3\n  seconds per run-code (10 machines if 10 run-codes per second is accurate, more\n  if we experience higher load).\n\u003c/p\u003e\n\n\u003cp\u003e\n  This design scales horizontally with our number of users, and it can scale\n  vertically to make running code even faster (more CPU == faster runs).\n\u003c/p\u003e\n\u003cp\u003e\n  Lastly, we can have some logging and monitoring in our system, especially for\n  running code (tracking run-code events per language, per user, per question,\n  average response time, etc.). This will help us automatically scale our\n  clusters when user demand goes up or down. This can also be useful to know if\n  any malicious behavior is happening with the code-execution engine.\n\u003c/p\u003e"
			},
			{
			   "title":"System Diagram",
			   "content":"\u003cimg\n  width=\"100%\"\n  src=\"https://prod.api.algoexpert.io/api/problems/v1/assets?path=algoexpert.png\"\n  alt=\"Final Systems Architecture\"\n/\u003e",
			   "image": {
				   "name": "algoexpert",
				   "extension": "png"
			   }
			}
		 ],
		 "hints":[
			{
			   "question":"Are we designing the entire AlgoExpert platform or just a specific part of it, like the coding workspace?",
			   "answer":"Since we only have about 45 minutes, you should just design the core user flow of the AlgoExpert platform. The core user flow includes users landing on the home page of the website, going to the questions list, marking questions as complete or in progress, and then writing and running code in various languages for each language. Don't worry about payments or authentication; you can just assume that you have these services working already (by the way, we mainly rely on third-party services here, like Stripe, PayPal, and OAuth2)."
			},
			{
			   "question":"AlgoExpert doesn't seem like a system of utmost criticality (like a hospital system or airplane software); are we okay with 2 to 3 nines of availability for the system?",
			   "answer":"Yes, this seems fine--no need to focus too much on making the system highly available."
			},
			{
			   "question":"How many customers should we be building this for? Is AlgoExpert's audience global or limited to one country?",
			   "answer":"AlgoExpert’s website receives hundreds of thousands of users every month, and tens of thousands of users may be on the website at any point in time. We want the website to feel very responsive to people everywhere in the world, and the U.S. and India are the platform's top 2 markets that we especially want to cater to."
			},
			{
			   "question":"Does AlgoExpert make changes to its content (questions list and question solutions) often?",
			   "answer":"Yes--every couple of days on average. And we like to have our changes reflected in production globallywithin the hour."
			},
			{
			   "question":"How much of the code-execution engine behind the coding workspace should we be designing? Do we have to worry about the security aspect of running random user code on our servers?",
			   "answer":"You can disregard the security aspects of the code-execution engine and just focus on its core functionality--the ability to run code in various languages at any given time with acceptable latency."
			},
			{
			   "question":"While we'll care about latency across the entire system, the code-execution engine seems like the place where we'll care about it most, since it's very interactive, and it also seems like the toughest part of our system to support low latencies; are we okay with anywhere between 1 and 3 seconds for the average run-code latency?",
			   "answer":"Yes--this seems reasonable and acceptable from a product point of view."
			}
		 ]
	  },
	  {
		 "name":"Design A Stockbroker",
		 "available":true,
		 "date_added":"",
		 "ready":true,
		 "video_url":"https://player.vimeo.com/video/392163922",
		 "release_date":"2020-02-18T12:00:00-05:00",
		 "prompt":"\u003cp\u003eDesign a stockbroker: a platform that acts as the intermediary between end-customers and some central stock exchange.\u003c/p\u003e\n",
		 "walkthrough":[
			{
			   "title":"Gathering System Requirements",
			   "content":"\u003cp\u003e\n  As with any systems design interview question, the first thing that we want to\n  do is to gather system requirements; we need to figure out what system we're\n  building exactly.\n\u003c/p\u003e\n\u003cp\u003e\n  We're building a stock-brokerage platform like Robinhood that functions as the\n  intermediary between end-customers and some central stock exchange. The idea\n  is that the central stock exchange is the platform that actually executes\n  stock trades, whereas the stockbroker is just the platform that customers talk\n  to when they want to place a trade--the stock brokerage is \"simpler\" and more\n  \"human-readable\", so to speak.\n\u003c/p\u003e\n\n\u003cp\u003e\n  We only care about supporting market trades--trades that are executed at the\n  current stock price--and we can assume that our system stores customer\n  balances (i.e., funds that customers may have previously deposited) in a\n  SQL table.\n\u003c/p\u003e\n\n\u003cp\u003e\n  We need to design a \u003ci\u003ePlaceTrade\u003c/i\u003e API call, and we know that the central\n  exchange's equivalent API method will take in a callback that's guaranteed to\n  be executed upon completion of a call to that API method.\n\u003c/p\u003e\n\n\u003cp\u003e\n  We're designing this system to support millions of trades per day coming from\n  millions of customers in a single region (the U.S., for example). We want the\n  system to be highly available.\n\u003c/p\u003e"
			},
			{
			   "title":"Coming Up With A Plan",
			   "content":"\u003cp\u003e\n  It's important to organize ourselves and to lay out a clear plan regarding how\n  we're going to tackle our design. What are the major, distinguishable\n  components of our how system?\n\u003c/p\u003e\n\u003cp\u003e\n  We'll approach the design front to back:\n\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003e\n    the \u003ci\u003ePlaceTrade\u003c/i\u003e API call that clients will make\n  \u003c/li\u003e\n  \u003cli\u003ethe API server(s) handling client API calls\u003c/li\u003e\n  \u003cli\u003ethe system in charge of executing orders for each customer\u003c/li\u003e\n\u003c/ul\u003e\nWe'll need to make sure that the following hold:\n\u003cul\u003e\n  \u003cli\u003e\n    trades can never be stuck forever without either succeeding or failing to be\n    executed\n  \u003c/li\u003e\n  \u003cli\u003e\n    a single customer's trades have to be executed in the order in which they\n    were placed\n  \u003c/li\u003e\n  \u003cli\u003ebalances can never go in the negatives\u003c/li\u003e\n\u003c/ul\u003e"
			},
			{
			   "title":"API Call",
			   "content":"\u003cp\u003e\n  The core API call that we have to implement is \u003ci\u003ePlaceTrade\u003c/i\u003e.\n\u003c/p\u003e\n\u003cp\u003eWe'll define its signature as:\u003c/p\u003e\n\u003cpre\u003e\n  PlaceTrade(\n    customerId: string,\n    stockTicker: string,\n    type: string (\u003cb\u003eBUY\u003c/b\u003e/\u003cb\u003eSELL\u003c/b\u003e),\n    quantity: integer,\n  ) =\u003e (\n    tradeId: string,\n    stockTicker: string,\n    type: string (\u003cb\u003eBUY\u003c/b\u003e/\u003cb\u003eSELL\u003c/b\u003e),\n    quantity: integer,\n    createdAt: timestamp,\n    status: string (\u003cb\u003ePLACED\u003c/b\u003e),\n    reason: string,\n  )\n\u003c/pre\u003e\n\n\u003cp\u003e\n  The customer ID can be derived from an authentication token that's only known\n  to the user and that's passed into the API call.\n\u003c/p\u003e\n\n\u003cp\u003e\n  The status can be one of:\n\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003e\u003cb\u003ePLACED\u003c/b\u003e\u003c/li\u003e\n  \u003cli\u003e\u003cb\u003eIN PROGRESS\u003c/b\u003e\u003c/li\u003e\n  \u003cli\u003e\u003cb\u003eFILLED\u003c/b\u003e\u003c/li\u003e\n  \u003cli\u003e\u003cb\u003eREJECTED\u003c/b\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\n  That being said,\n  \u003cb\u003ePLACED\u003c/b\u003e will actually be the defacto status here, because the other\n  statuses will be asynchronously set once the exchange executes our callback.\n  In other words, the trade status will always be \u003cb\u003ePLACED\u003c/b\u003e when the\n  \u003ci\u003ePlaceTrade\u003c/i\u003e API call returns, but we can imagine that a\n  \u003ci\u003eGetTrade\u003c/i\u003e API call could return statuses other than \u003cb\u003ePLACED\u003c/b\u003e.\n\u003c/p\u003e\n\n\u003cp\u003ePotential reasons for a \u003cb\u003eREJECTED\u003c/b\u003e trade might be:\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003einsufficient funds\u003c/li\u003e\n  \u003cli\u003erandom error\u003c/li\u003e\n  \u003cli\u003epast market hours\u003c/li\u003e\n\u003c/ul\u003e"
			},
			{
			   "title":"API Server(s)",
			   "content":"\u003cp\u003e\n  We'll need multiple API servers to handle all of the incoming requests. Since\n  we don't need any caching when making trades, we don't need any server\n  stickiness, and we can just use some \u003cb\u003eround-robin load balancing\u003c/b\u003e to\n  distribute incoming requests between our API servers.\n\u003c/p\u003e\n\n\u003cp\u003e\n  Once API servers receive a \u003ci\u003ePlaceTrade\u003c/i\u003e call, they'll store the trade in\n  a SQL table. This table needs to be in the same SQL database as the one that the balances table is in, because we'll need to use ACID transactions to alter\n  both tables in an atomic way.\n\u003c/p\u003e\n\n\u003cp\u003eThe SQL table for \u003cu\u003etrades\u003c/u\u003e will look like this:\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003eid: \u003ci\u003estring\u003c/i\u003e, a random, auto-generated string\u003c/li\u003e\n  \u003cli\u003ecustomer_id: \u003ci\u003estring\u003c/i\u003e, the id of the customer making the trade\u003c/li\u003e\n  \u003cli\u003e\n    stockTicker: \u003ci\u003estring\u003c/i\u003e, the ticker symbol of the stock being traded\n  \u003c/li\u003e\n  \u003cli\u003etype: \u003ci\u003estring\u003c/i\u003e, either \u003cb\u003eBUY\u003c/b\u003e or \u003cb\u003eSELL\u003c/b\u003e\u003c/li\u003e\n  \u003cli\u003e\n    quantity: \u003ci\u003einteger\u003c/i\u003e (no fractional shares), the number of shares to\n    trade\n  \u003c/li\u003e\n  \u003cli\u003estatus: \u003ci\u003estring\u003c/i\u003e, the status of the trade; starts as \u003cb\u003ePLACED\u003c/b\u003e\u003c/li\u003e\n  \u003cli\u003ecreated_at: \u003ci\u003etimestamp\u003c/i\u003e, the time when the trade was created\u003c/li\u003e\n  \u003cli\u003e\n    reason: \u003ci\u003estring\u003c/i\u003e, the human-readable justification of the trade's\n    status\n  \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eThe SQL table for \u003cu\u003ebalances\u003c/u\u003e will look like this:\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003eid: \u003ci\u003estring\u003c/i\u003e, a random, auto-generated string\u003c/li\u003e\n  \u003cli\u003e\n    customer_id: \u003ci\u003estring\u003c/i\u003e, the id of the customer related to te balance\n  \u003c/li\u003e\n  \u003cli\u003e\n    amount: \u003ci\u003efloat\u003c/i\u003e, the amount of money that the customer has in USD\n  \u003c/li\u003e\n  \u003cli\u003e\n    last_modified: \u003ci\u003etimestamp\u003c/i\u003e, the time when the balance was last modified\n  \u003c/li\u003e\n\u003c/ul\u003e"
			},
			{
			   "title":"Trade-Execution Queue",
			   "content":"\u003cp\u003e\n  With hundreds of orders placed every second, the trades table will be pretty\n  massive. We'll need to figure out a robust way to actually execute our trades\n  and to update our table, all the while making sure of a couple of things:\n\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003e\n    We want to make sure that for a single customer, we only process a single\n    \u003cb\u003eBUY\u003c/b\u003e trade at any time, because we need to prevent the customer's balance\n    from ever reaching negative values.\n  \u003c/li\u003e\n  \u003cli\u003e\n    Given the nature of market orders, we never know the exact dollar value that\n    a trade will get executed at in the exchange until we get a response from\n    the exchange, so we have to speak to the exchange in order to know whether the\n    trade can go through.\n  \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e\n  We can design this part of our system with a Publish/Subscribe pattern. The\n  idea is to use a message queue like Apache Kafka or Google Cloud Pub/Sub and\n  to have a set of topics that customer ids map to. This gives us at-least-once\n  delivery semantics to make sure that we don't miss new trades. When a customer\n  makes a trade, the API server writes a row to the database and also creates a\n  message that gets routed to a topic for that customer (using hashing),\n  notifying the topic's subscriber that there's a new trade.\n\u003c/p\u003e\n\n\u003cp\u003e\n  This gives us a guarantee that for a single customer, we only have a single\n  thread trying to execute their trades at any time.\n\u003c/p\u003e\n\n\u003cp\u003e\n  Subscribers of topics can be rings of 3 workers (clusters of\n  servers, essentially) that use leader election to have 1 master worker do the\n  work for the cluster (this is for our system's high availability)--the leader\n  grabs messages as they get pushed to the topic and executes the trades for the\n  customers contained in the messages by calling the exchange. As mentioned\n  above, a single customer's trades are only ever handled by the same cluster of\n  workers, which makes our logic and our SQL queries cleaner.\n\u003c/p\u003e\n\n\u003cp\u003e\n  As far as how many topics and clusters of workers we'll need, we can do some rough estimation. If we\n  plan to execute millions of trades per day, that comes down to about 10-100\n  trades per second given open trading hours during a third of a day and\n  non-uniform trading patterns. If we assume that the core execution logic lasts\n  about a second, then we should have roughly 10-100 topics and clusters of workers to process trades in\n  parallel.\n\u003c/p\u003e\n\u003cpre\u003e\n  ~100,000 seconds per day (3600 * 24)\n  ~1,000,000 trades per day\n  trades bunched in 1/3rd of the day\n  --\u003e (1,000,000 / 100,000) * 3 = ~30 trades per second\n\u003c/pre\u003e"
			},
			{
			   "title":"Trade-Execution Logic",
			   "content":"\u003cp\u003e\n  The subscribers (our workers) are streaming / waiting for\n  messages. Imagine the following message were to arrive in the topic queue:\n\u003c/p\u003e\n\u003cpre\u003e\n  {\"customerId\": \"c1\"}\n\u003c/pre\u003e\n\u003cp\u003e\n  The following would be pseudo-code for the worker logic:\n\u003c/p\u003e\n\u003cpre\u003e\n    // We get the oldest trade that isn't in a terminal state.\n    trade = SELECT * FROM trades WHERE\n        customer_id = 'c1' AND\n        (status = 'PLACED' OR status = 'IN PROGRESS')\n        ORDER BY created_at ASC LIMIT 1;\n\n    // If the trade is PLACED, we know that it's effectively\n    // ready to be executed. We set it as IN PROGRESS.\n    if trade.status == \"PLACED\" {\n        UPDATE trades SET status = 'IN PROGRESS' WHERE id = trade.id;\n    }\n\n    // In the event that the trade somehow already exists in the\n    // exchange, the callback will do the work for us.\n    if exchange.TradeExists(trade.id) {\n        return;\n    }\n\n    // We get the balance for the customer.\n    balance = SELECT amount FROM balances WHERE\n        customer_id = 'c1';\n\n    // This is the callback that the exchange will execute once\n    // the trade actually completes. We'll define it further down\n    // in the walkthrough.\n    callback = ...\n\n    exchange.Execute(\n        trade.stockTicker,\n        trade.type,\n        trade.quantity,\n        max_price = balance,\n        callback,\n    )\n\u003c/pre\u003e"
			},
			{
			   "title":"Exchange Callback",
			   "content":"\u003cp\u003e\n  Below is some pseudo code for the exchange callback:\n\u003c/p\u003e\n\u003cpre\u003e\nfunction exchange_callback(exchange_trade) {\n    if exchange_trade.status == 'FILLED' {\n        BEGIN TRANSACTION;\n        trade = SELECT * FROM trades WHERE id = database_trade.id;\n        if trade.status \u003c\u003e 'IN PROGRESS' {\n            ROLLBACK;\n            pubsub.send({customer_id: database_trade.customer_id});\n            return;\n        }\n        UPDATE balances SET amount -= exchange_trade.amount WHERE customer_id = database_trade.customer_id;\n        UPDATE trades SET status = 'FILLED' WHERE id = database_trade.id;\n        COMMIT;\n    } else if exchange_trade.status == 'REJECTED' {\n        BEGIN TRANSACTION;\n        UPDATE trades SET status = 'REJECTED' WHERE id = database_trade.id;\n        UPDATE trades SET reason = exchange_trade.reason WHERE id = database_trade.id;\n        COMMIT;\n    }\n    pubsub.send({customer_id: database_trade.customer_id});\n    return http.status(200);\n}\n\u003c/pre\u003e"
			},
			{
			   "title":"System Diagram",
			   "content":"\u003cimg width=\"100%\" src=\"https://prod.api.algoexpert.io/api/problems/v1/assets?path=stockbroker.png\" alt=\"Final Systems Architecture\"/\u003e",
			   "image": {
				   "name": "stockbroker",
				   "extension": "png"
			   }
			}
		 ],
		 "hints":[
			{
			   "question":"What do we mean exactly by a stock broker? Is this something like Robinhood or Etrade?",
			   "answer":"Yes, exactly."
			},
			{
			   "question":"What is the platform supposed to support exactly? Are we just supporting the ability for customers to buy and sell stocks, or are we supporting more? For instance, are we allowing other types of securities like options and futures to be traded on our platform? Are we supporting special types of orders like limit orders and stop losses?",
			   "answer":"We're only supporting market orders on stocks in this design. A market order means that, given a placed order to buy or sell a stock, we should try to execute the order as soon as possible regardless of the stock price. We also aren't designing any “margin” system, so the available balance is the source of truth for what can be bought."
			},
			{
			   "question":"Are we designing any of the auxiliary aspects of the stock brokerage, like depositing and withdrawing funds, downloading tax documents, etc.?",
			   "answer":"No -- we're just designing the core trading aspect of the platform."
			},
			{
			   "question":"Are we just designing the system to place trades? Do we want to support other trade-related operations like getting trade statuses? In other words, how comprehensive should the API that's going to support this platform be?",
			   "answer":"In essence, you're only designing a system around a PlaceTrade API call from the user, but you should define that API call (inputs, response, etc.)."
			},
			{
			   "question":"Where does a customer's balance live? Is the platform pulling a customer's money directly from their bank account, or are we expecting that customers will have already deposited funds into the platform somehow? In other words, are we ever directly interacting with banks?",
			   "answer":"No, you won't be interacting with banks. You can assume that customers have already deposited funds into the platform, and you can further assume that you have a SQL table with the balance for each customer who wants to make a trade."
			},
			{
			   "question":"How many customers are we building this for? And is our customer-base a global one?",
			   "answer":"Millions of customers, millions of trades a day. Let's assume that our customers are only located in 1 region -- the U.S., for instance."
			},
			{
			   "question":"What kind of availability are we looking for?",
			   "answer":"As high as possible, with this kind of service people can lose a lot of money if the system is down even for a few minutes."
			},
			{
			   "question":"Are we also designing the UI for this platform? What kinds of clients can we assume we have to support?",
			   "answer":"You don't have to design the UI, but you should design the PlaceTrade API call that a UI would be making to your backend. Clients would be either a mobile app or a webapp."
			},
			{
			   "question":"So we want to design the API for the actual brokerage, that itself interacts with some central stock exchange on behalf of customers. Does this exchange have an API? If yes, do we know what it looks like, and do we have any guarantees about it?",
			   "answer":"Yes, the exchange has an API, and your platform's API (the PlaceTrade call) will have to interact with the exchange's API. As far as that's concerned, you can assume that the call to the exchange to make an actual trade will take in a callback (in addition to the info about the trade) that will get executed when that trade completes at the exchange level (meaning, when the trade either gets FILLED or REJECTED, this callback will be executed). You can also assume that the exchange's system is highly available--your callback will always get executed at least once."
			}
		 ]
	  },
	  {
		 "name":"Design Facebook News Feed",
		 "available":true,
		 "date_added":"",
		 "ready":true,
		 "video_url":"https://player.vimeo.com/video/392908210",
		 "release_date":"2020-02-21T12:00:00-05:00",
		 "prompt":"",
		 "walkthrough":[
			{
			   "title":"Gathering System Requirements",
			   "content":"\u003cp\u003e\n  As with any systems design interview question, the first thing that we want to\n  do is to gather system requirements; we need to figure out what system we're\n  building exactly.\n\u003c/p\u003e\n\u003cp\u003e\n  We're designing the core user flow of the \u003cb\u003eFacebook News Feed\u003c/b\u003e. This\n  consists of loading a user's news feed, scrolling through the list of posts\n  that are relevant to them, posting status updates, and having their friends'\n  news feeds get updated in real time. We're Specifically designing the pipeline\n  that generates and serves news feeds and the system that handles what happens\n  when a user posts and news feeds have to be updated.\n\u003c/p\u003e\n\n\u003cp\u003e\n  We're dealing with about 1 billion users, each with 500 friends on average.\n\u003c/p\u003e\n\n\u003cp\u003e\n  Getting a news feed should feel fairly instant, and creating a post should\n  update all of a user's friends' news feeds within a minute. We can have some\n  variance with regards to feed updates depending on user locations.\n\u003c/p\u003e\n\n\u003cp\u003e\n  Additionally, we can't be satisfied with a single cluster serving everyone on\n  earth because of large \u003cb\u003elatencies\u003c/b\u003e that would occur between that cluster\n  and the user in some parts of the world, so we need a mechanism to make sure\n  the feed gets updated within a minute in the regions other than the one the\n  post was created in.\n\u003c/p\u003e\n\n\u003cp\u003e\n  We can assume that the ranking algorithms used to generate news feeds with the\n  most relevant posts is taken care of for us by some other system that we have\n  access to.\n\u003c/p\u003e"
			},
			{
			   "title":"Coming Up With A Plan",
			   "content":"\u003cp\u003e\n  We'll start with the extremities of our system and work inward, first talking\n  about the two API calls, \u003ci\u003eCreatePost\u003c/i\u003e and \u003ci\u003eGetNewsFeed\u003c/i\u003e, then,\n  getting into the feed creation and storage strategy, our cross-region design,\n  and finally tying everything together in a fast and scalable way.\n\u003c/p\u003e"
			},
			{
			   "title":"CreatePost API",
			   "content":"\u003cp\u003e\n  For the purpose of this design, the \u003ci\u003eCreatePost\u003c/i\u003e API call will be very\n  simple and look something like this:\n\u003c/p\u003e\n\u003cpre\u003e\n    CreatePost(\n        user_id: string,\n        post: data\n    )\n\u003c/pre\u003e\n\u003cp\u003e\n  When a user creates a post, the API call goes through some load balancing\n  before landing on one of many API servers (which are stateless). Those API\n  servers then create a message on a Pub/Sub topic, notifying its subscribers of\n  the new post that was just created. Those subscribers will do a few things, so\n  let's call them S1 for future reference. Each of the subscribers S1 reads from\n  the topic and is responsible for creating the facebook post inside a\n  relational database.\n\u003c/p\u003e"
			},
			{
			   "title":"Post Storage",
			   "content":"\u003cp\u003e\n  We can have one main relational database to store most of our system's data,\n  including posts and users. This database will have \u003ci\u003every large\u003c/i\u003e tables.\n\u003c/p\u003e"
			},
			{
			   "title":"GetNewsFeed API",
			   "content":"\u003cp\u003eThe \u003ci\u003eGetNewsFeed\u003c/i\u003e API call will most likely look like this:\u003c/p\u003e\n\u003cpre\u003e\n    GetNewsFeed(\n        user_id: string,\n        pageSize: integer,\n        nextPageToken: integer,\n    ) =\u003e (\n        posts: []{\n            user_id: string,\n            post_id: string,\n            post: data,\n        },\n        nextPageToken: string,\n    )\n\u003c/pre\u003e\n\u003cp\u003e\n  The \u003ci\u003epageSize\u003c/i\u003e and \u003ci\u003enextPageToken\u003c/i\u003e fields are used to\n  \u003cb\u003epaginate\u003c/b\u003e the newsfeed; pagination is necessary when dealing with large\n  amounts of listed data, and since we'll likely want each news feed to have up\n  to 1000 posts, pagination is very appropriate here.\n\u003c/p\u003e"
			},
			{
			   "title":"Feed Creation And Storage",
			   "content":"\u003cp\u003e\n  Since our databases tables are going to be so large, with billions of millions\n  of users and tens of millions of posts every week, fetching news feeds from\n  our main database every time a \u003ci\u003eGetNewsFeed\u003c/i\u003e call is made isn't going to\n  be ideal. We can't expect low latencies when building news feeds from scratch\n  because querying our huge tables takes time, and sharding the main database\n  holding the posts wouldn't be particularly helpful since news feeds would\n  likely need to aggregate posts across shards, which would require us to\n  perform cross-shard joins when generating news feeds; we want to avoid this.\n\u003c/p\u003e\n\n\u003cp\u003e\n  Instead, we can store news feeds separately from our main database across an\n  array of shards. We can have a separate cluster of machines that can act as a\n  proxy to the relational database and be in charge of aggregating posts,\n  ranking them via the ranking algorithm that we're given, generating news\n  feeds, and sending them to our shards every so often (every 5, 10, 60 minutes,\n  depending on how often we want news feeds to be updated).\n\u003c/p\u003e\n\nIf we average each post at 10kB, and a newsfeed comprises of the top 1000 posts\nthat are relevant to a user, that's 10MB per user, or\n\u003cb\u003e10 000TB\u003c/b\u003e of data total. We assume that it's loaded 10 times per day per\nuser, which averages at \u003cb\u003e10k QPS\u003c/b\u003e for the newsfeed fetching.\n\u003cp\u003e\n  Assuming 1 billion news feeds (for 1 billion users) containing 1000 posts of\n  up to 10 KB each, we can estimate that we'll need 10 PB (petabytes) of storage\n  to store all of our users' news feeds. We can use 1000 machines of 10 TB each\n  as our news-feed shards.\n\u003c/p\u003e\n\u003cpre\u003e\n  ~10 KB per post\n  ~1000 posts per news feed\n  ~1 billion news feeds\n  ~10 KB * 1000 * 1000^3 = 10 PB = 1000 * 10 TB\n\u003c/pre\u003e\n\n\u003cp\u003e\n  To distribute the newsfeeds roughly evently, we can shard based on the user\n  id.\n\u003c/p\u003e\n\n\u003cp\u003e\n  When a \u003ci\u003eGetNewsFeed\u003c/i\u003e request comes in, it gets load balanced to the right\n  news feed machine, which returns it by reading on local disk. If the newsfeed\n  doesn't exist locally, we then go to the source of truth (the main database,\n  but going through the proxy ranking service) to gather the relevant posts.\n  This will lead to increased latency but shouldn't happen frequently.\n\u003c/p\u003e"
			},
			{
			   "title":"Wiring Updates Into Feed Creation",
			   "content":"\u003cp\u003e\n  We now need to have a notification mechanism that lets the feed shards know\n  that a new relevant post was just created and that they should incorporate it\n  into the feeds of impacted users.\n\u003c/p\u003e\n\u003cp\u003e\n  We can once again use a Pub/Sub service for this. Each one of the shards will\n  subscribe to its own topic--we'll call these topics the Feed Notification\n  Topics (FNT)--and the original subscribers S1 will be the publishers for the\n  FNT. When S1 gets a new message about a post creation, it searches the main\n  database for all of the users for whom this post is relevant (i.e., it\n  searches for all of the friends of the user who created the post), it filters\n  out users from other regions who will be taken care of asynchronously, and it\n  maps the remaining users to the FNT using the same hashing function that our\n  \u003ci\u003eGetNewsFeed\u003c/i\u003e load balancers rely on.\n\u003c/p\u003e\n\u003cp\u003e\n  For posts that impact too many people, we can cap the number of FNT topics\n  that get messaged to reduce the amount of internal traffic that gets generated\n  from a single post. For those big users we can rely on the asynchronous feed\n  creation to eventually kick in and let the post appear in feeds of users whom\n  we've skipped when the feeds get refreshed manually.\n\u003c/p\u003e"
			},
			{
			   "title":"Cross-Region Strategy",
			   "content":"\u003cp\u003e\n  When \u003ci\u003eCreatePost\u003c/i\u003e gets called and reaches our Pub/Sub subscribers,\n  they'll send a message to another Pub/Sub topic that some forwarder service in\n  between regions will subscribe to. The forwarder's job will be, as its name\n  implies, to forward messages to other regions so as to replicate all of the\n  \u003ci\u003eCreatePost\u003c/i\u003e logic in other regions. Once the forwarder receives the\n  message, it'll essentially mimic what would happen if that same\n  \u003ci\u003eCreatePost\u003c/i\u003e were called in another region, which will start the entire\n  feed-update logic in those other regions. We can have some additional logic\n  passed to the forwarder to prevent other regions being replicated to from\n  notifying other regions about the \u003ci\u003eCreatePost\u003c/i\u003e call in question, which\n  would lead to an infinite chain of replications; in other words, we can make\n  it such that only the region where the post originated from is in charge of\n  notifying other regions.\n\u003c/p\u003e\n\u003cp\u003e\n  Several open-source technologies from big companies like Uber and Confluent\n  are designed in part for this kind of operation.\n\u003c/p\u003e"
			},
			{
			   "title":"System Diagram",
			   "content":"\u003cimg\n  width=\"100%\"\n  src=\"https://prod.api.algoexpert.io/api/problems/v1/assets?path=facebook.png\"\n  alt=\"Final Systems Architecture\"\n/\u003e",
			   "image": {
				   "name": "facebook",
				   "extension": "png"
			   }
			}
		 ],
		 "hints":[
			{
			   "question":"Facebook News Feed consists of multiple major features, like loading a user's news feed, interacting with it (i.e., posting status updates, liking posts, etc.), and updating it in real time (i.e., adding new status updates that are being posted to the top of the feed, in real time). What part of Facebook News Feed are we designing exactly?",
			   "answer":"We're designing the core functionality of the feed itself, which we'll define as follows: loading a user's news feed and updating it in real time, as well as posting status updates. But for posting status updates, we don't need to worry about the actual API or the type of information that a user can post; we just want to design what happens once an API call to post a status update has been made. Ultimately, we primarily want to design the feed generation/refreshing piece of the data pipeline (i.e, how/when does it get constructed, and how/when does it get updated with new posts)."
			},
			{
			   "question":"To clarify, posts on Facebook can be pretty complicated, with pictures, videos, special types of status updates, etc.. Are you saying that we're not concerned with this aspect of the system? For example, should we not focus on how we'll be storing this type of information?",
			   "answer":"That's correct. For the purpose of this question, we can treat posts as opaque entities that we'll certainly want to store, but without worrying about the details of the storage, the ramifications of storing and serving large files like videos, etc.."
			},
			{
			   "question":"Are we designing the relevant-post curation system (i.e., the system that decides what posts will show up on a user's news feed)?",
			   "answer":"No. We're not designing this system or any ranking algorithms; you can assume that you have access to a ranking algorithm that you can simply feed a list of relevant posts to in order to generate an actual news feed to display."
			},
			{
			   "question":"Are we concerned with showing ads in a user's news feed at all? Ads seem like they would behave a little bit differently than posts, since they probably rely on a different ranking algorithm.",
			   "answer":"You can treat ads as a bonus part of the design; if you find a way to incorporate them in, great (and yes, you'd have some other ads-serving algorithm to determine what ads need to be shown to a user at any point in time). But don't focus on ads to start."
			},
			{
			   "question":"Are we serving a global audience, and how big is our audience?",
			   "answer":"Yes -- we're serving a global audience, and let's say that the news feed will be loaded in the order of 100 million times a day, by 100 million different users, with 1 million new status updates posted every day."
			},
			{
			   "question":"How many friends does a user have on average? This is important to know, since a user's status updates could theoretically have to show up on all of the user's friends' news feeds at once.",
			   "answer":"You can expect each user to have, on average, 500 friends on the social network. You can treat the number of friends per user as a bell-shaped distribution, with some users who have very few friends, and some users who have a lot more than 500 friends."
			},
			{
			   "question":"How quickly does a status update have to appear on a news feed once it's posted, and is it okay if this varies depending on user locations with respect to the location of the user submitting a post?",
			   "answer":"When a user posts something, you probably want it to show up on other news feeds fairly quickly. This speed can indeed vary depending on user locations. For instance, we'd probably want a local friend within the same region to see the new post within a few seconds, but we'd likely be okay with a user on the other side of the world seeing the same post within a minute."
			},
			{
			   "question":"What kind of availability are we aiming for?",
			   "answer":"Your design shouldn't be completely unavailable from a single machine failure, but this isn't a high availability requirement. However, posts shouldn’t ever just disappear. Once the user’s client gets confirmation that the post was created, you cannot lose it."
			}
		 ]
	  },
	  {
		 "name":"Design Google Drive",
		 "available":true,
		 "date_added":"",
		 "ready":true,
		 "video_url":"https://player.vimeo.com/video/393365928",
		 "release_date":"2020-02-24T12:00:00-05:00",
		 "prompt":"",
		 "walkthrough":[
			{
			   "title":"Gathering System Requirements",
			   "content":"\u003cp\u003e\n  As with any systems design interview question, the first thing that we want to\n  do is to gather system requirements; we need to figure out what system we're\n  building exactly.\n\u003c/p\u003e\n\n\u003cp\u003e\n  We're designing the core user flow of the \u003cb\u003eGoogle Drive\u003c/b\u003e web application.\n  This consists of storing two main entities: folders and files. More\n  specifically, the system should allow users to create folders, upload and\n  download files, and rename and move entities once they're stored. We don't\n  have to worry about ACLs, sharing entities, or any other auxiliary Google\n  Drive features.\n\u003c/p\u003e\n\n\u003cp\u003e\n  We're going to be building this system at a very large scale, assuming 1\n  billion users, each with \u003cb\u003e15GB\u003c/b\u003e of data stored in Google Drive on\n  average. This adds up to approximately \u003cb\u003e15,000 PB\u003c/b\u003e of data in total,\n  without counting any metadata that we might store for each entity, like its\n  name or its type.\n\u003c/p\u003e\n\n\u003cp\u003e\n  We need this service to be \u003cb\u003eHighly Available\u003c/b\u003e and also very redundant. No\n  data that's successfully stored in Google Drive can ever be lost, even through\n  catastrophic failures in an entire region of the world.\n\u003c/p\u003e"
			},
			{
			   "title":"Coming Up With A Plan",
			   "content":"\u003cp\u003e\n  It's important to organize ourselves and to lay out a clear plan regarding how\n  we're going to tackle our design. What are the major, distinguishable\n  components of our how system?\n\u003c/p\u003e\n\u003cp\u003e\n  First of all, we'll need to support the following operations:\n\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003e\n    For \u003cb\u003eFiles\u003c/b\u003e\n    \u003cul\u003e\n      \u003cli\u003e\u003ci\u003eUploadFile\u003c/i\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ci\u003eDownloadFile\u003c/i\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ci\u003eDeleteFile\u003c/i\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ci\u003eRenameFile\u003c/i\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ci\u003eMoveFile\u003c/i\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    For \u003cb\u003eFolders\u003c/b\u003e\n    \u003cul\u003e\n      \u003cli\u003e\u003ci\u003eCreateFolder\u003c/i\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ci\u003eGetFolder\u003c/i\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ci\u003eDeleteFolder\u003c/i\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ci\u003eRenameFolder\u003c/i\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ci\u003eMoveFolder\u003c/i\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\n  Secondly, we'll have to come up with a proper storage solution for two types\n  of data:\n\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003e\n    File Contents: The contents of the files uploaded to Google Drive. These are\n    opaque bytes with no particular structure or format.\n  \u003c/li\u003e\n  \u003cli\u003e\n    Entity Info: The metadata for each entity. This might include fields like\n    \u003cb\u003eentityID, ownerID, lastModified, entityName, entityType\u003c/b\u003e. This list is\n    non-exhaustive, and we'll most likely add to it later on.\n  \u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\n  Let's start by going over the storage solutions that we want to use, and then\n  we'll go through what happens when each of the operations outlined above is\n  performed.\n\u003c/p\u003e"
			},
			{
			   "title":"Storing Entity Info",
			   "content":"\u003cp\u003e\n  To store entity information, we can use key-value stores. Since we need high\n  availability and data replication, we need to use something like Etcd,\n  Zookeeper, or Google Cloud Spanner (as a K-V store) that gives us both of\n  those guarantees as well as consistency (as opposed to DynamoDB, for instance,\n  which would give us only eventual consistency).\n\u003c/p\u003e\n\n\u003cp\u003e\n  Since we're going to be dealing with many gigabytes of entity information\n  (given that we're serving a billion users), we'll need to shard this data\n  across multiple clusters of these K-V stores. Sharding on entityID means that\n  we'll lose the ability to perform batch operations, which these key-value\n  stores give us out of the box and which we'll need when we move entities\n  around (for instance, moving a file from one folder to another would involve\n  editing the metadata of 3 entities; if they were located in 3 different shards\n  that wouldn't be great). Instead, we can shard based on the \u003cb\u003eownerID\u003c/b\u003e of\n  the entity, which means that we can edit the metadata of multiple entities\n  atomically with a transaction, so long as the entities belong to the same\n  user.\n\u003c/p\u003e\n\n\u003cp\u003e\n  Given the traffic that this website needs to serve, we can have a layer of\n  proxies for entity information, load balanced on a hash of the \u003cb\u003eownerID\u003c/b\u003e.\n  The proxies could have some caching, as well as perform \u003cb\u003eACL\u003c/b\u003e checks when\n  we eventually decide to support them. The proxies would live at the regional\n  level, whereas the source-of-truth key-value stores would be accessed\n  globally.\n\u003c/p\u003e"
			},
			{
			   "title":"Storing File Data",
			   "content":"\u003cp\u003e\n  When dealing with potentially very large uploads and data storage, it's often\n  advantageous to split up data into blobs that can be pieced back together to\n  form the origial data. When uploadig a file, the request will be load balanced\n  across multiple servers that we'll call \"blob splitters\", and these blob\n  splitters will have the job of splitting files into blobs and storing these\n  blobs in some global blob-storage solution like\n  \u003cb\u003eGCS\u003c/b\u003e or \u003cb\u003eS3\u003c/b\u003e (since we're designing \u003cb\u003eGoogle\u003c/b\u003e Drive, it might\n  not be a great idea to pick S3 over GCS :P).\n\u003c/p\u003e\n\n\u003cp\u003e\n  One thing to keep in mind is that we need a lot of redundancy for the data\n  that we're uploading in order to prevent data loss. So we'll probably want to\n  adopt a strategy like: try pushing to 3 different GCS \u003cb\u003ebuckets\u003c/b\u003e and\n  consider a write successful only if it went through in at least 2 buckets.\n  This way we always have redundancy without necessarily sacrificing\n  availability. In the background, we can have an extra service in charge of\n  further replicating the data to other buckets in an async manner. For our main\n  3 buckets, we'll want to pick buckets in 3 different availability zones to\n  avoid having all of our redudant storage get wiped out by potential\n  catastrophic failures in the event of a natural disaster or huge power\n  outtage.\n\u003c/p\u003e\n\n\u003cp\u003e\n  In order to avoid having multiple identical blobs stored in our blob stores,\n  we'll name the blobs after a hash of their content. This technique is called\n  \u003cb\u003eContent-Addressible Storage\u003c/b\u003e, and by using it, we essentially make all\n  blobs immutable in storage. When a file changes, we simply upload the entire\n  new resulting blobs under their new names computed by hashing their new\n  contents.\n\u003c/p\u003e\n\n\u003cp\u003e\n  This immutability is \u003ci\u003every\u003c/i\u003e powerful, in part because it means that we\n  can very easily introduce a caching layer between the blob splitters and the\n  buckets, without worrying about keeping caches in sync with the main source of\n  truth when edits are made--an edit just means that we're dealing with a\n  completely different blob.\n\u003c/p\u003e"
			},
			{
			   "title":"Entity Info Structure",
			   "content":"\u003cp\u003e\n  Since folders and files will both have common bits of metadata, we can have\n  them share the same structure. The difference will be that folders will have\n  an\n  \u003cb\u003eis_folder\u003c/b\u003e flag set to true and a list of \u003cb\u003echildren_ids\u003c/b\u003e, which\n  will point to the entity information for the folders and files within the\n  folder in question. Files will have an \u003cb\u003eis_folder\u003c/b\u003e flag set to false and\n  a \u003cb\u003eblobs\u003c/b\u003e field, which will have the IDs of all of the blobs that make up\n  the data within the relevant file. Both entities can also have a\n  \u003cb\u003eparent_id\u003c/b\u003e field, which will point to the entity information of the\n  entity's parent folder. This will help us quickly find parents when moving\n  files and folders.\n\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003eFile Info\u003c/li\u003e\n  \u003cpre\u003e\n{\n  blobs: ['blob_content_hash_0', 'blob_content_hash_1'],\n  id: 'some_unique_entity_id'\n  is_folder: false,\n  name: 'some_file_name',\n  owner_id: 'id_of_owner',\n  parent_id: 'id_of_parent',\n}\u003c/pre\u003e\n  \u003cli\u003eFolder Info\u003c/li\u003e\n  \u003cpre\u003e\n{\n  children_ids: ['id_of_child_0', 'id_of_child_1'],\n  id: 'some_unique_entity_id'\n  is_folder: true,\n  name: 'some_folder_name',\n  owner_id: 'id_of_owner',\n  parent_id: 'id_of_parent',\n}\u003c/pre\u003e\n\u003c/ul\u003e"
			},
			{
			   "title":"Garbage Collection",
			   "content":"\u003cp\u003e\n  Any change to an existing file will create a whole new blob and de-reference\n  the old one. Furthermore, any deleted file will also de-reference the file's\n  blobs. This means that we'll eventually end up with a lot of\n  \u003cb\u003eorphaned\u003c/b\u003e blobs that are basically unused and taking up storage for no\n  reason. We'll need a way to get rid of these blobs to free some space.\n\u003c/p\u003e\n\n\u003cp\u003e\n  We can have a \u003cb\u003eGarbage Collection\u003c/b\u003e service that watches the entity-info\n  K-V stores and keeps counts of the number of times every blob is referenced by\n  files; these counts can be stored in a SQL table.\n\u003c/p\u003e\n\n\u003cp\u003e\n  Reference counts will get updated whenever files are uploaded and deleted.\n  When the reference count for a particular blob reaches 0, the Garbage\n  Collector can mark the blob in question as orphaned in the relevant blob\n  stores, and the blob will be safely deleted after some time if it hasn't been\n  accessed.\n\u003c/p\u003e"
			},
			{
			   "title":"End To End API Flow",
			   "content":"\u003cp\u003e\n  Now that we've designed the entire system, we can walk through what happens\n  when a user performs any of the operations we listed above.\n\u003c/p\u003e\n\n\u003cp\u003e\n  \u003ci\u003eCreateFolder\u003c/i\u003e is simple; since folders don't have a blob-storage\n  component, creating a folder just involves storing some metadata in our\n  key-value stores.\n\u003c/p\u003e\n\n\u003cp\u003e\n  \u003ci\u003eUploadFile\u003c/i\u003e works in two steps. The first is to store the blobs that\n  make up the file in the blob storage. Once the blobs are persisted, we can\n  create the file-info object, store the blob-content hashes inside its\n  \u003cb\u003eblobs\u003c/b\u003e field, and write this metadata to our key-value stores.\n\u003c/p\u003e\n\n\u003cp\u003e\n  \u003ci\u003eDownloadFile\u003c/i\u003e fetches the file's metadata from our key-value stores\n  given the file's ID. The metadata contains the hashes of all of the blobs that\n  make up the content of the file, which we can use to fetch all of the blobs\n  from blob storage. We can then assemble them into the file and save it onto\n  local disk.\n\u003c/p\u003e\n\n\u003cp\u003e\n  All of the \u003ci\u003eGet\u003c/i\u003e, \u003ci\u003eRename\u003c/i\u003e, \u003ci\u003eMove\u003c/i\u003e, and\n  \u003ci\u003eDelete\u003c/i\u003e operations atomically change the metadata of one or several\n  entities within our key-value stores using the \u003cb\u003etransaction\u003c/b\u003e\n  guarantees that they give us.\n\u003c/p\u003e"
			},
			{
			   "title":"System Diagram",
			   "content":"\u003cimg\n  width=\"100%\"\n  src=\"https://prod.api.algoexpert.io/api/problems/v1/assets?path=google_drive.png\"\n  alt=\"Final Systems Architecture\"\n/\u003e",
			   "image": {
				   "name": "google_drive",
				   "extension": "png"
			   }
			}
		 ],
		 "hints":[
			{
			   "question":"Are we just designing the storage aspect of Google Drive, or are we also designing some of the related products like Google Docs, Sheets, Slides, Drawings, etc.?",
			   "answer":"We're just designing the core Google Drive product, which is indeed the storage product. In other words, users can create folders and upload files, which effectively stores them in the cloud. Also, for simplicity, we can refer to folders and files as \"entities\"."
			},
			{
			   "question":"There are a lot of features on Google Drive, like shared company drives vs. personal drives, permissions on entities (ACLs), starred files, recently-accessed files, etc.. Are we designing all of these features or just some of them?",
			   "answer":"Let's keep things narrow and imagine that we're designing a personal Google Drive (so you can forget about shared company drives). In a personal Google Drive, users can store entities, and that's all that you should take care of. Ignore any feature that isn't core to the storage aspect of Google Drive; ignore things like starred files, recently-accessed files, etc.. You can even ignore sharing entities for this design."
			},
			{
			   "question":"Since we're primarily concerned with storing entities, are we supporting all basic CRUD operations like creating, deleting, renaming, and moving entities?",
			   "answer":"Yes, but to clarify, creating a file is actually uploading a file, folders have to be created (they can't be uploaded), and we also want to support downloading files."
			},
			{
			   "question":"Are we just designing the Google Drive web application, or are we also designing a desktop client for Google drive?",
			   "answer":"We're just designing the functionality of the Google Drive web application."
			},
			{
			   "question":"Since we're not dealing with sharing entities, should we handle multiple users in a single folder at the same time, or can we assume that this will never happen? ",
			   "answer":"While we're not designing the sharing feature, let's still handle what would happen if  multiple clients were in a single folder at the same time (two tabs from the same browser, for example). In this case, we would want changes made in that folder to be reflected to all clients within 10 seconds. But for the purpose of this question, let's not worry about conflicts or anything like that (i.e., assume that two clients won't make changes to the same file or folder at the same time)."
			},
			{
			   "question":"How many people are we building this system for?",
			   "answer":"This system should serve about a billion users and handle 15GB per user on average."
			},
			{
			   "question":"What kind of reliability or guarantees does this Google Drive service give to its users?",
			   "answer":"First and foremost, data loss isn't tolerated at all; we need to make sure that once a file is uploaded or a folder is created, it won't disappear until the user deletes it. As for availability, we need this system to be highly available."
			}
		 ]
	  },
	  {
		 "name":"Design The Reddit API",
		 "available":true,
		 "date_added":"",
		 "ready":true,
		 "video_url":"https://player.vimeo.com/video/394019091",
		 "release_date":"2020-02-27T12:00:00-05:00",
		 "prompt":"\u003cp\u003eDesign an API for Reddit subreddits given the following information.\u003c/p\u003e\n\n\u003cp\u003eThe API includes these 2 entities:\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003e\u003cb\u003eUser\u003c/b\u003e | \u003ci\u003euserId: string\u003c/i\u003e, ...\u003c/li\u003e\n  \u003cli\u003e\u003cb\u003eSubReddit\u003c/b\u003e | \u003ci\u003esubredditId: string\u003c/i\u003e, ...\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\n  Both of these entities likely have other fields, but for the purpose of this\n  question, those other fields aren't needed.\n\u003c/p\u003e\n\u003cp\u003eYour API should support the basic functionality of a subreddit on Reddit.\u003c/p\u003e\n",
		 "walkthrough":[
			{
			   "title":"Gathering Requirements",
			   "content":"\u003cp\u003e\n  As with any API design interview question, the first thing that we want to do\n  is to gather API requirements; we need to figure out what API we're building\n  exactly.\n\u003c/p\u003e\n\n\u003cp\u003e\n  We're designing the core user flow of the \u003cb\u003esubreddit\u003c/b\u003e functionality on\n  Reddit. Users can write posts on subreddits, they can comment on posts, and\n  they can upvote / downvote posts and comments.\n\u003c/p\u003e\n\n\u003cp\u003e\n  We're going to be defining three primary entities: \u003ci\u003ePosts\u003c/i\u003e,\n  \u003ci\u003eComments\u003c/i\u003e, and \u003ci\u003eVotes\u003c/i\u003e, as well as their respective CRUD operations.\n\u003c/p\u003e\n\n\u003cp\u003e\n  We're also going to be designing an API for buying and giving awards on\n  Reddit.\n\u003c/p\u003e"
			},
			{
			   "title":"Coming Up With A Plan",
			   "content":"\u003cp\u003e\n  It's important to organize ourselves and to lay out a clear plan regarding how\n  we're going to tackle our design. What are the major, potentially contentious\n  parts of our API? Why are we making certain design decisions?\n\u003c/p\u003e\n\u003cp\u003e\n  The first major point of contention is whether to store votes only on Comments\n  and Posts, and to cast votes by calling the \u003ci\u003eEditComment\u003c/i\u003e and\n  \u003ci\u003eEditPost\u003c/i\u003e methods, or whether to store them as entirely separate\n  entities--siblings of Posts and Comments, so to speak. Storing them as\n  separate entities makes it much more straightforward to edit or remove a\n  particular user's votes (by just calling \u003ci\u003eEditVote\u003c/i\u003e, for instance), so\n  we'll go with this approach.\n\u003c/p\u003e\n\u003cp\u003e\n  We can then plan to tackle Posts, Comments, and Votes in this order, since\n  they'll likely share some common structure.\n\u003c/p\u003e"
			},
			{
			   "title":"Posts",
			   "content":"\u003cp\u003e\n  Posts will have an id, the id of their creator (i.e., the user who writes\n  them), the id of the subreddit that they're on, a description and a title, and\n  a timestamp of when they're created.\n\u003c/p\u003e\n\u003cp\u003e\n  Posts will also have a count of their votes, comments, and awards, to be\n  displayed on the UI. We can imagine that some backend service will be\n  calculating or updating these numbers when some of the Comment, Vote, and\n  Award CRUD operations are performed.\n\u003c/p\u003e\n\u003cp\u003e\n  Lastly, Posts will have optional \u003cb\u003edeletedAt\u003c/b\u003e and\n  \u003cb\u003ecurrentVote\u003c/b\u003e fields. Subreddits display posts that have been removed\n  with a special message; we can use the \u003cb\u003edeletedAt\u003c/b\u003e field to accomplish\n  this. The \u003cb\u003ecurrentVote\u003c/b\u003e field will be used to display to a user whether\n  or not they've cast a vote on a post. This field will likely be populated\n  by the backend upon fetching Posts or when casting Votes.\n\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003epostId: \u003ci\u003estring\u003c/i\u003e\u003c/li\u003e\n  \u003cli\u003ecreatorId: \u003ci\u003estring\u003c/i\u003e\u003c/li\u003e\n  \u003cli\u003esubredditId: \u003ci\u003estring\u003c/i\u003e\u003c/li\u003e\n  \u003cli\u003etitle: \u003ci\u003estring\u003c/i\u003e\u003c/li\u003e\n  \u003cli\u003edescription: \u003ci\u003estring\u003c/i\u003e\u003c/li\u003e\n  \u003cli\u003ecreatedAt: \u003ci\u003etimestamp\u003c/i\u003e\u003c/li\u003e\n  \u003cli\u003evotesCount: \u003ci\u003eint\u003c/i\u003e\u003c/li\u003e\n  \u003cli\u003ecommentsCount: \u003ci\u003eint\u003c/i\u003e\u003c/li\u003e\n  \u003cli\u003eawardsCount: \u003ci\u003eint\u003c/i\u003e\u003c/li\u003e\n  \u003cli\u003edeletedAt?: \u003ci\u003etimestamp\u003c/i\u003e\u003c/li\u003e\n  \u003cli\u003ecurrentVote?: \u003ci\u003eenum UP/DOWN\u003c/i\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\n  Our \u003ci\u003eCreatePost\u003c/i\u003e, \u003ci\u003eEditPost\u003c/i\u003e, \u003ci\u003eGetPost\u003c/i\u003e, and\n  \u003ci\u003eDeletePost\u003c/i\u003e methods will be very straightforward. One thing to note, however, is\n  that all of these operations will take in the \u003cb\u003euserId\u003c/b\u003e of the user\n  performing them; this id, which will likely contain authentication\n  information, will be used for ACL checks to see if the user performing the\n  operations has the necessary permission(s) to do so.\n\u003c/p\u003e\n\u003cpre\u003e\nCreatePost(userId: string, subredditId: string, title: string, description: string)\n  =\u003e Post\n\nEditPost(userId: string, postId: string, title: string, description: string)\n  =\u003e Post\n\nGetPost(userId: string, postId: string)\n  =\u003e Post\n\nDeletePost(userId: string, postId: string)\n  =\u003e Post\n\u003c/pre\u003e\n\u003cp\u003e\n  Since we can expect to have hundreds, if not thousands, of posts on a given\n  subreddit, our \u003ci\u003eListPosts\u003c/i\u003e method will have to be paginated. The method\n  will take in optional \u003cb\u003epageSize\u003c/b\u003e and \u003cb\u003epageToken\u003c/b\u003e parameters and will\n  return a list of posts of at most length \u003cb\u003epageSize\u003c/b\u003e as well as a\n  \u003cb\u003enextPageToken\u003c/b\u003e--the token to be fed to the method to retrieve the next\n  page of posts.\n\u003c/p\u003e\n\u003cpre\u003e\nListPosts(userId: string, subredditId: string, pageSize?: int, pageToken?: string)\n  =\u003e (Post[], nextPageToken?)\n\u003c/pre\u003e"
			},
			{
			   "title":"Comments",
			   "content":"\u003cp\u003e\n  Commments will be similar to Posts. They'll have an id, the id of their\n  creator (i.e., the user who writes them), the id of the post that they're on,\n  a content string, and the same other fields as Posts have. The only difference\n  is that Comments will also have an optional \u003cb\u003eparentId\u003c/b\u003e pointing to the\n  parent post or parent comment of the comment. This id will allow the Reddit UI\n  to reconstruct Comment trees to properly display (indent) replies. The UI can\n  also sort comments within a reply thread by their \u003cb\u003ecreatedAt\u003c/b\u003e timestamps\n  or by their \u003cb\u003evotesCount\u003c/b\u003e.\n\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003ecommentId: \u003ci\u003estring\u003c/i\u003e\u003c/li\u003e\n  \u003cli\u003ecreatorId: \u003ci\u003estring\u003c/i\u003e\u003c/li\u003e\n  \u003cli\u003epostId: \u003ci\u003estring\u003c/i\u003e\u003c/li\u003e\n  \u003cli\u003ecreatedAt: \u003ci\u003etimestamp\u003c/i\u003e\u003c/li\u003e\n  \u003cli\u003econtent: \u003ci\u003estring\u003c/i\u003e\u003c/li\u003e\n  \u003cli\u003evotesCount: \u003ci\u003eint\u003c/i\u003e\u003c/li\u003e\n  \u003cli\u003eawardsCount: \u003ci\u003eint\u003c/i\u003e\u003c/li\u003e\n  \u003cli\u003eparentId?: \u003ci\u003estring\u003c/i\u003e\u003c/li\u003e\n  \u003cli\u003edeletedAt?: \u003ci\u003etimestamp\u003c/i\u003e\u003c/li\u003e\n  \u003cli\u003ecurrentVote?: \u003ci\u003eenum UP/DOWN\u003c/i\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\n  Our CRUD operations for Comments will be very similar to those for Posts,\n  except that the \u003ci\u003eCreateComment\u003c/i\u003e method will also take in an optional\n  \u003cb\u003eparentId\u003c/b\u003e pointing to the comment that it's replying to, if relevant.\n\u003c/p\u003e\n\u003cpre\u003e\nCreateComment(userId: string, postId: string, content: string, parentId?: string)\n  =\u003e Comment\n\nEditComment(userId: string, commentId: string, content: string)\n  =\u003e Comment\n\nGetComment(userId: string, commentId: string)\n  =\u003e Comment\n\nDeleteComment(userId: string, commentId: string)\n  =\u003e Comment\n\nListComments(userId: string, postId: string, pageSize?: int, pageToken?: string)\n  =\u003e (Comment[], nextPageToken?)\n\u003c/pre\u003e"
			},
			{
			   "title":"Votes",
			   "content":"\u003cp\u003e\n  Votes will have an id, the id of their creator (i.e., user who casts them),\n  the id of their target (i.e., the post or comment that they're on), and a\n  type, which will be a simple UP/DOWN enum. They could also have a\n  \u003cb\u003ecreatedAt\u003c/b\u003e timestamp for good measure.\n\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003evoteId: \u003ci\u003estring\u003c/i\u003e\u003c/li\u003e\n  \u003cli\u003ecreatorId: \u003ci\u003estring\u003c/i\u003e\u003c/li\u003e\n  \u003cli\u003etargetId: \u003ci\u003estring\u003c/i\u003e\u003c/li\u003e\n  \u003cli\u003etype: \u003ci\u003eenum UP/DOWN\u003c/i\u003e\u003c/li\u003e\n  \u003cli\u003ecreatedAt: \u003ci\u003etimestamp\u003c/i\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\n  Since it doesn't seem like getting a single vote or listing votes would be\n  very useful for our feature, we'll skip designing those endpoints (though they\n  would be straightforward).\n\u003c/p\u003e\n\u003cp\u003e\n  Our \u003ci\u003eCreateVote\u003c/i\u003e, \u003ci\u003eEditVote\u003c/i\u003e, and \u003ci\u003eDeleteVote\u003c/i\u003e methods will be\n  simple and useful. The \u003ci\u003eCreateVote\u003c/i\u003e method will be used when a user casts\n  a new vote on a post or comment; the \u003ci\u003eEditVote\u003c/i\u003e method will be used when\n  a user has already cast a vote on a post or comment and casts the opposite\n  vote on that same post or comment; and the \u003ci\u003eDeleteVote\u003c/i\u003e method will be\n  used when a user has already cast a vote on a post or comment and just removes\n  that same vote.\n\u003c/p\u003e\n\u003cpre\u003e\nCreateVote(userId: string, targetId: string, type: enum UP/DOWN)\n  =\u003e Vote\n\nEditVote(userId: string, voteId: string, type: enum UP/DOWN)\n  =\u003e Vote\n\nDeleteVote(userId: string, voteId: string)\n  =\u003e Vote\n\u003c/pre\u003e"
			},
			{
			   "title":"Awards",
			   "content":"\u003cp\u003e\n  We can define two simple endpoints to handle buying and giving awards. The\n  endpoint to buy awards will take in a \u003cb\u003epaymentToken\u003c/b\u003e, which will be a\n  string that contains all of the necessary information to process a payment. The endpoint to give an award will take in a \u003cb\u003etargetId\u003c/b\u003e, which will be the id of the post or comment that the award is being given to.\n\u003c/p\u003e\n\u003cpre\u003e\nBuyAwards(userId: string, paymentToken: string, quantity: int)\n\nGiveAward(userId: string, targetId: string)\n\u003c/pre\u003e"
			}
		 ],
		 "hints":[
			{
			   "question":"To make sure that we're on the same page: a subreddit is an online community where users can write posts, comment on posts, upvote / downvote posts, share posts, report posts, become moderators, etc..--is this correct, and are we designing all of this functionality?",
			   "answer":"Yes, that's correct, but let's keep things simple and focus only on writing posts, writing comments, and upvoting / downvoting. You can forget about all of the auxiliary features like sharing, reporting, moderating, etc.."
			},
			{
			   "question":"So we're really focusing on the very narrow but core aspect of a subreddit: writing posts, commenting on them, and voting on them.",
			   "answer":"Yes."
			},
			{
			   "question":"I'm thinking of defining the schemas for the main entities that live within a subreddit and then defining their CRUD operations -- methods like Create/Get/Edit/Delete/List\u003cEntity\u003e -- is this in line with what you're asking me to do?",
			   "answer":"Yes, and make sure to include method signatures -- what each method takes in and what each method returns. Also include the types of each argument."
			},
			{
			   "question":"The entities that I've identified are Posts, Comments, and Votes (upvotes and downvotes). Does this seem accurate?",
			   "answer":"Yes. These are the 3 core entities that you should be defining and whose APIs you're designing."
			},
			{
			   "question":"Is there any other functionality of a subreddit that we should design?",
			   "answer":"Yes, you should also allow people to award posts. Awards are a special currency that can be bought for real money and gifted to comments and posts. Users can buy some quantity of awards in exchange for real money, and they can give awards to posts and comments (one award per post / comment)."
			}
		 ]
	  },
	  {
		 "name":"Design Netflix",
		 "available":true,
		 "date_added":"",
		 "ready":true,
		 "video_url":"https://player.vimeo.com/video/424259575",
		 "release_date":"2020-06-03T12:00:00-05:00",
		 "prompt":"",
		 "walkthrough":[
			{
			   "title":"Gathering System Requirements",
			   "content":"\u003cp\u003e\n  As with any systems design interview question, the first thing that we want to\n  do is to gather system requirements; we need to figure out what system we're\n  building exactly.\n\u003c/p\u003e\n\u003cp\u003e\n  We're designing the core Netflix service, which allows users to stream movies\n  and shows from the Netflix website.\n\u003c/p\u003e\n\u003cp\u003e\n  Specifially, we'll want to focus on:\n\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003e\n    Delivering large amounts of high-definition video content to hundreds of\n    millions of users around the globe without too much buffering.\n  \u003c/li\u003e\n  \u003cli\u003e\n    Processing large amounts of user-activity data to support Netflix's\n    recommendation engine.\n  \u003c/li\u003e\n\u003c/ul\u003e"
			},
			{
			   "title":"Coming Up With A Plan",
			   "content":"\u003cp\u003e\n  We'll tackle this system by dividing it into four main sections:\n\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003eStorage (Video Content, Static Content, and User Metadata)\u003c/li\u003e\n  \u003cli\u003eGeneral Client-Server Interaction (i.e., the life of a query)\u003c/li\u003e\n  \u003cli\u003eVideo Content Delivery\u003c/li\u003e\n  \u003cli\u003eUser-Activity Data Processing\u003c/li\u003e\n\u003c/ul\u003e"
			},
			{
			   "title":"Video-Content Storage",
			   "content":"\u003cp\u003e\n  Since Netflix's service, which caters to millions of customers, is centered\n  around video content, we might need \u003ci\u003ea lot\u003c/i\u003e of storage space and a\n  complex storage solution. Let's start by estimating how much space we'll need.\n\u003c/p\u003e\n\u003cp\u003e\n  We were told that Netflix has about 200 million users; we can make a few\n  assumptions about other Netflix metrics (alternatively, we can ask our\n  interviewer for guidance here):\n\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003eNetflix offers roughly 10 thousand movies and shows at any given time\u003c/li\u003e\n  \u003cli\u003e\n    Since movies can be up to 2+ hours in length and shows tend to be between 20\n    and 40 minutes per episode, we can assume an average video length of 1 hour\n  \u003c/li\u003e\n  \u003cli\u003e\n    Each movie / show will have a \u003cb\u003eStandard Definition\u003c/b\u003e version and a\n    \u003cb\u003eHigh Definition\u003c/b\u003e\n    version. Per hour, SD will take up about 10GB of space, while HD will take\n    about 20GB.\n  \u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\n  ~10K videos (stored in SD \u0026 HD)\n  ~1 hour average video length\n  ~10 GB/h for SD + ~20 GB/h for HD = 30 GB/h per video\n  ~30 GB/h * 10K videos = 300,000 GB = 300 TB\n\u003c/pre\u003e\n\u003cp\u003e\n  This number highlights the importance of estimations. Naively, one might think\n  that Netflix stores many petabytes of video, since its core product revolves\n  around video content; but a simple back-of-the-napkin estimation shows us that\n  it actually stores a very modest amount of video.\n\u003c/p\u003e\n\u003cp\u003e\n  This is because Netflix, unlike other servies like YouTube, Google Drive, and\n  Facebook, has a bounded amount of video content: the movies and shows that it\n  offers; those other services allow users to upload unlimited amounts of video.\n\u003c/p\u003e\n\u003cp\u003e\n  Since we're only dealing with a few hundred terabytes of data, we can use a\n  simple blob storage solution like \u003cb\u003eS3\u003c/b\u003e or \u003cb\u003eGCS\u003c/b\u003e to reliably handle\n  the storage and replication of Netflix's video content; we don't need a more\n  complex data-storage solution.\n\u003c/p\u003e"
			},
			{
			   "title":"Static-Content Storage",
			   "content":"\u003cp\u003e\n  Apart from video content, we'll want to store various pieces of static content\n  for Netflix's movies and shows, including video titles, descriptions, and cast\n  lists.\n\u003c/p\u003e\n\u003cp\u003e\n  This content will be bounded in size by the size of the video content, since\n  it'll be tied to the number of movies and shows, just like the video content,\n  and since it'll naturally take up less space than the video data.\n\u003c/p\u003e\n\u003cp\u003e\n  We can easily store all of this static content in a relational database or\n  even in a document store, and we can cache most of it in our API servers.\n\u003c/p\u003e"
			},
			{
			   "title":"User Metadata Storage",
			   "content":"\u003cp\u003e\n  We can expect to store some user metadata for each video on the Netflix\n  platform. For instance, we might want to store the timestamp that a user left\n  a video at, a user's rating on a video, etc..\n\u003c/p\u003e\n\u003cp\u003e\n  Just like the static content mentioned above, this user metadata will be tied\n  to the number of videos on Netflix. However, unlike the static content, this\n  user metadata will grow with the Netflix userbase, since each user will have\n  user metadata.\n\u003c/p\u003e\n\u003cp\u003eWe can quickly estimate how much space we'll need for this user metadata:\u003c/p\u003e\n\u003cpre\u003e\n  ~200M users\n  ~1K videos watched per user per lifetime (~10% of total content)\n  ~100 bytes/video/user\n  ~100 bytes * 1K videos * 200M users = 100 KB * 200M = 1 GB * 20K = 20 TB\n\u003c/pre\u003e\n\u003cp\u003e\n  Perhaps surprisingly, we'll be storing an amount of user metadata in the same\n  ballpark as the amount of video content that we'll be storing. Once again,\n  this is because of the bounded nature of Netflix's video content, which is in\n  stark contrast with the unbounded nature of its userbase.\n\u003c/p\u003e\n\u003cp\u003e\n  We'll likely need to query this metadata, so storing it in a classic\n  relational database like Postgres makes sense.\n\u003c/p\u003e\n\u003cp\u003e\n  Since Netflix users are effectively isolated from one another (they aren't\n  connected like they would be on a social-media platform, for example), we can\n  expect all of our latency-sensitive database operations to only relate to\n  individual users. In other words, potential operations like\n  \u003ci\u003eGetUserInfo\u003c/i\u003e and \u003ci\u003eGetUserWatchedVideos\u003c/i\u003e, which would require fast\n  latencies, are specific to a particular users; on the other hand, complicated\n  database operations involving multiple users' metadata will likely be part of\n  background data-engineering jobs that don't care about latency.\n\u003c/p\u003e\n\u003cp\u003e\n  Given this, we can split our user-metadata database into a handful of shards,\n  each managing anywhere between 1 and 10 TB of indexed data. This will maintain\n  very quick reads and writes for a given user.\n\u003c/p\u003e"
			},
			{
			   "title":"General Client-Server Interaction",
			   "content":"\u003cp\u003e\n  The part of the system that handles serving user metadata and static content\n  to users shouldn't be too complicated.\n\u003c/p\u003e\n\u003cp\u003e\n  We can use some simple round-robin load balancing to distribute end-user\n  network requests across our API servers, which can then load-balance database\n  requests according to userId (since our database will be sharded based on\n  userId).\n\u003c/p\u003e\n\u003cp\u003e\n  As mentioned above, we can cache our static content in our API servers,\n  periodically updating it when new movies and shows are released, and we can\n  even cache user metadata there, using a write-through caching mechanism.\n\u003c/p\u003e"
			},
			{
			   "title":"Video Content Delivery",
			   "content":"\u003cp\u003e\n  We need to figure out how we'll be delivering Netflix's video content across\n  the globe with little latency. To start, we'll estimate the maximum amount of\n  bandwidth consumption that we could expect at any point in time. We'll assume\n  that, at peak traffic, like when a popular movie comes out, a fairly large\n  number of Netflix users might be streaming video content concurrently.\n\u003c/p\u003e\n\u003cpre\u003e\n  ~200M total users\n  ~5% of total users streaming concurrently during peak hours\n  ~20 GB/h of HD video ~= 5 MB/s of HD video\n  ~5% of 200M * 5 MB/s = 10M * 5 MB/s = 50 TB/s \n\u003c/pre\u003e\n\u003cp\u003e\n  This level of bandwidth consumption means we can't just naively serve the\n  video content out of a single data center or even dozens of data centers. We\n  need many thousands of locations around the world to be distributing this\n  content for us. Thankfully, \u003cb\u003eCDN\u003c/b\u003es solve this precise problem, since they\n  have many thousands of \u003cb\u003ePoints of Presence\u003c/b\u003e around the world. We can thus\n  use a CDN like Cloudflare and serve our video content out of the CDN's PoPs.\n\u003c/p\u003e\n\u003cp\u003e\n  Since the PoPs can't keep the entirety of Netflix's video content in cache, we\n  can have an external service that periodically repopulates CDN PoPs with the\n  most important content (the movies and shows most likely to be watched).\n\u003c/p\u003e"
			},
			{
			   "title":"User-Activity Data Processing",
			   "content":"\u003cp\u003e\n  We need to figure out how we'll process vast amounts of user-activity data to\n  feed into Netflix's recommendation engine. We can imagine that this\n  user-activity data will be gathered in the form of logs that are generated by\n  all sorts of user actions; we can expect terabytes of these logs to be\n  generated every day.\n\u003c/p\u003e\n\u003cp\u003e\n  \u003cb\u003eMapReduce\u003c/b\u003e can help us here. We can store the logs in a distributed file\n  system like \u003cb\u003eHDFS\u003c/b\u003e and run MapReduce jobs to process massive amounts of\n  data in parallel. The results of these jobs can then be fed into some machine\n  learning pipelines or simply stored in a database.\n\u003c/p\u003e\n\n\u003cb\u003eMap Inputs\u003c/b\u003e\n\u003cp\u003e\n  Our Map inputs can be our raw logs, which might look like:\n\u003c/p\u003e\n\u003cpre\u003e\n{\"userId\": \"userId1\", \"videoId\": \"videoId1\", \"event\": \"CLICK\"}\n{\"userId\": \"userId2\", \"videoId\": \"videoId2\", \"event\": \"PAUSE\"}\n{\"userId\": \"userId3\", \"videoId\": \"videoId3\", \"event\": \"MOUSE_MOVE\"}\n\u003c/pre\u003e\n\n\u003cb\u003eMap Outputs / Reduce Inputs\u003c/b\u003e\n\u003cp\u003e\n  Our Map function will aggregate logs based on userId and return intermediary\n  key-value pairs indexed on each userId, pointing to lists of tuples with\n  videoIds and relevant events.\n\u003c/p\u003e\n\u003cp\u003e\n  These intermediary k/v pairs will be shuffled appropriately and fed into our\n  Reduce functions.\n\u003c/p\u003e\n\u003cpre\u003e\n{\"userId1\": [(\"CLICK\", \"videoId1\"), (\"CLICK\", \"videoId1\"), ..., (\"PAUSE\", \"videoId2\")]}\n{\"userId2\": [(\"PLAY\", \"videoId1\"), (\"MOUSE_MOVE\", \"videoId2\"), ..., (\"MINIMIZE\", \"videoId3\")]}\n\u003c/pre\u003e\n\n\u003cb\u003eReduce Outputs\u003c/b\u003e\n\u003cp\u003e\n  Our Reduce functions could return many different outputs. They could return\n  k/v pairs for each userId|videoId combination, pointing to a computed score\n  for that user/video pair; they could return k/v pairs indexed at each userId,\n  pointing to lists of (videoId, score) tuples; or they could return k/v pairs\n  also indexed at eacher userId but pointing to stack-rankings of videoIds,\n  based on their computed score.\n\u003c/p\u003e\n\u003cpre\u003e\n(\"userId1|videoId1\", score)\n(\"userId1|videoId2\", score)\n\nOR\n\n{\"userId1\": [(\"videoId1\", score), (\"videoId2\", score), ..., (\"videoId3\", score)]}\n{\"userId2\": [(\"videoId1\", score), (\"videoId2\", score), ..., (\"videoId3\", score)]}  \n\nOR\n\n(\"userId1\", [\"videoId1\", \"videoId2\", ..., \"videoId3\"])\n(\"userId2\", [\"videoId1\", \"videoId2\", ..., \"videoId3\"])\n\u003c/pre\u003e"
			},
			{
			   "title":"System Diagram",
			   "content":"\u003cimg\n  width=\"100%\"\n  src=\"https://assets.algoexpert.io/se-diagrams/netflix.svg\"\n  alt=\"Final Systems Architecture\"\n/\u003e",
			   "image": {
				   "name": "netflix",
				   "extension": "png"
			   }
			}
		 ],
		 "hints":[
			{
			   "question":"From a high-level point of view, Netflix is a fairly straightforward service: users go on the platform, they're served movies and shows, and they watch them. Are we designing this high-level system entirely, or would you like me to focus on a particular subsystem, like the Netflix home page?",
			   "answer":"We're just designing the core Netflix product--so the overarching system / product that you described."
			},
			{
			   "question":"Should we worry about auxiliary services like authentication and payments?",
			   "answer":"You can ignore those auxiliary services; focus on the primary user flow. That being said, one thing to note is that, by nature of the product, we're going to have access to a lot of user-activity data that's going to need to be processed in order to enable Netflix's recommendation system. You'll need to come up with a way to aggregate and process user-activity data on the website."
			},
			{
			   "question":"For this recommendation system, should I think about the kind of algorithm that'll fuel it?",
			   "answer":"No, you don't need to worry about implementing any algorithm or formula for the recommendation engine. You just need to think about how user-activity data will be gathered and processed.It sounds like there are 2 main points of focus in this system: the video-serving service and the recommendation engine. Regarding the video-serving service, I'm assuming that we're looking for high availability and fast latencies globally; is this correct?Yes, but just to clarify, the video-streaming service is actually the only part of the system for which we care about fast latencies."
			},
			{
			   "question":"So is the recommendation engine a system that consumes the user-activity data you mentioned and operates asynchronously in the background?",
			   "answer":"Yes."
			},
			{
			   "question":"How many users do we expect to be building this for?",
			   "answer":"Netflix has about 100M to 200M users, so let's go with 200M."
			},
			{
			   "question":"Should we worry about designing this for various clients, like desktop clients, mobile clients, etc.?",
			   "answer":"Even though we're indeed designing Netflix to be used by all sorts of clients, let's focus purely on the distributed-system component--so no need to get into details about clients or to optimize for certain clients."
			}
		 ]
	  },
	  {
		 "name":"Design The Uber API",
		 "available":true,
		 "date_added":"",
		 "ready":true,
		 "video_url":"https://player.vimeo.com/video/425290208",
		 "release_date":"2020-06-03T12:00:00-05:00",
		 "prompt":"",
		 "walkthrough":[
			{
			   "title":"Gathering Requirements",
			   "content":"\u003cp\u003e\n  As with any API design interview question, the first thing that we want to do\n  is to gather API requirements; we need to figure out what API we're building\n  exactly.\n\u003c/p\u003e\n\n\u003cp\u003e\n  We're designing the core ride-hailing service that Uber offers. Passengers can\n  book a ride from their phone, at which point they're matched with a driver;\n  they can track their driver's location throughout the ride, up until the ride\n  is finished or cancelled; and they can also see the price of the ride as well\n  as the estimated time to destination throughout the trip, amongst other\n  things.\n\u003c/p\u003e\n\n\u003cp\u003e\n  The core taxiing service that Uber offers has a passenger-facing side and a\n  driver-facing side; we're going to be designing the API for both sides.\n\u003c/p\u003e"
			},
			{
			   "title":"Coming Up With A Plan",
			   "content":"\u003cp\u003e\n  It's important to organize ourselves and to lay out a clear plan regarding how\n  we're going to tackle our design. What are the major, potentially contentious\n  parts of our API? Why are we making certain design decisions?\n\u003c/p\u003e\n\u003cp\u003e\n  We're going to center our API around a \u003ci\u003eRide\u003c/i\u003e entity; every Uber ride\n  will have an associated \u003ci\u003eRide\u003c/i\u003e containing information about the ride,\n  including information about its passenger and driver.\n\u003c/p\u003e\n\u003cp\u003e\n  Since a normal Uber ride can only have one passenger (one passenger\n  account--the one that hails the ride) and one driver, we're going to cleverly\n  handle all permissioning related to ride operations through passenger and\n  driver IDs. In other words, operations like \u003ci\u003eGetRide\u003c/i\u003e and\n  \u003ci\u003eEditRide\u003c/i\u003e will purely rely on a passed userId, the userId of the\n  passenger or driver calling them, to return the appropriate ride tied to that\n  passenger or driver.\n\u003c/p\u003e\n\u003cp\u003e\n  We'll start by defining the \u003ci\u003eRide\u003c/i\u003e entity before designing the\n  passenger-facing API and then the driver-facing API.\n\u003c/p\u003e"
			},
			{
			   "title":"Entities",
			   "content":"\u003cb\u003eRide\u003c/b\u003e\n\u003cp\u003e\n  The \u003ci\u003eRide\u003c/i\u003e entity will have a unique id, info about its passenger and its\n  driver, a status, and other details about the ride.\n\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003erideId: \u003ci\u003estring\u003c/i\u003e\u003c/li\u003e\n  \u003cli\u003epassengerInfo: \u003ci\u003ePassengerInfo\u003c/i\u003e\u003c/li\u003e\n  \u003cli\u003edriverInfo?: \u003ci\u003eDriverInfo\u003c/i\u003e\u003c/li\u003e\n  \u003cli\u003e\n    rideStatus: \u003ci\u003eRideStatus\u003c/i\u003e - enum\n    CREATED/MATCHED/STARTED/FINISHED/CANCELLED\n  \u003c/li\u003e\n  \u003cli\u003estart: \u003ci\u003eGeoLocation\u003c/i\u003e\u003c/li\u003e\n  \u003cli\u003edestination: \u003ci\u003eGeoLocation\u003c/i\u003e\u003c/li\u003e\n  \u003cli\u003ecreatedAt: \u003ci\u003etimestamp\u003c/i\u003e\u003c/li\u003e\n  \u003cli\u003estartTime: \u003ci\u003etimestamp\u003c/i\u003e\u003c/li\u003e\n  \u003cli\u003eestimatedPrice: \u003ci\u003eint\u003c/i\u003e\u003c/li\u003e\n  \u003cli\u003etimeToDestination: \u003ci\u003eint\u003c/i\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\n  We'll explain why the \u003ci\u003edriverInfo\u003c/i\u003e is optional when we get to the API\n  endpoints.\n\u003c/p\u003e\n\u003cbr /\u003e\n\u003cb\u003ePassengerInfo\u003c/b\u003e\n\u003cul\u003e\n  \u003cli\u003eid: \u003ci\u003estring\u003c/i\u003e\u003c/li\u003e\n  \u003cli\u003ename: \u003ci\u003estring\u003c/i\u003e\u003c/li\u003e\n  \u003cli\u003erating: \u003ci\u003eint\u003c/i\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cb\u003eDriverInfo\u003c/b\u003e\n\u003cul\u003e\n  \u003cli\u003eid: \u003ci\u003estring\u003c/i\u003e\u003c/li\u003e\n  \u003cli\u003ename: \u003ci\u003estring\u003c/i\u003e\u003c/li\u003e\n  \u003cli\u003erating: \u003ci\u003eint\u003c/i\u003e\u003c/li\u003e\n  \u003cli\u003eridesCount: \u003ci\u003eint\u003c/i\u003e\u003c/li\u003e\n  \u003cli\u003evehicleInfo: \u003ci\u003eVehicleInfo\u003c/i\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cb\u003eVehicleInfo\u003c/b\u003e\n\u003cul\u003e\n  \u003cli\u003elicensePlate: \u003ci\u003estring\u003c/i\u003e\u003c/li\u003e\n  \u003cli\u003edescription: \u003ci\u003estring\u003c/i\u003e\u003c/li\u003e\n\u003c/ul\u003e"
			},
			{
			   "title":"Passenger API",
			   "content":"\u003cp\u003e\n  The passenger-facing API will be fairly straightforward. It'll consist of\n  simple CRUD operations around the \u003ci\u003eRide\u003c/i\u003e entity, as well as an endpoint\n  to stream a driver's location throughout a ride.\n\u003c/p\u003e\n\u003cpre\u003e\nCreateRide(userId: string, pickup: Geolocation, destination: Geolocation)\n  =\u003e Ride\n\u003c/pre\u003e\n\u003cp\u003e\n  Usage: called when a passenger books a ride; a \u003ci\u003eRide\u003c/i\u003e is created with no\n  \u003ci\u003eDriverInfo\u003c/i\u003e and with a \u003cb\u003eCREATED\u003c/b\u003e \u003ci\u003eRideStatus\u003c/i\u003e; the Uber\n  backend calls an internal \u003ci\u003eFindDriver\u003c/i\u003e API that uses an algorithm to find\n  the most appropriate driver; once a driver is found and accepts the ride, the\n  backend calls \u003ci\u003eEditRide\u003c/i\u003e with the driver's info and with a\n  \u003cb\u003eMATCHED\u003c/b\u003e \u003ci\u003eRideStatus\u003c/i\u003e.\n\u003c/p\u003e\n\u003cpre\u003e\nGetRide(userId: string)\n  =\u003e Ride\n\u003c/pre\u003e\n\u003cp\u003e\n  Usage: polled every couple of seconds after a ride has been created and until\n  the ride has a status of \u003cb\u003eMATCHED\u003c/b\u003e; afterwards, polled every 20-90\n  seconds throughout the trip to update the ride's estimated price, its time to\n  destination, its \u003ci\u003eRideStatus\u003c/i\u003e if it's been cancelled by the driver, etc..\n\u003c/p\u003e\n\u003cpre\u003e\nEditRide(userId: string, [...params?: all properties on the Ride object that need to be edited])\n  =\u003e Ride\n\u003c/pre\u003e\n\u003cpre\u003e\nCancelRide(userId: string)\n  =\u003e void\n\u003c/pre\u003e\n\u003cp\u003e\n  Wrapper around \u003ci\u003eEditRide\u003c/i\u003e -- effectively calls\n  \u003ci\u003eEditRide(userId: string, rideStatus: CANCELLED)\u003c/i\u003e.\n\u003c/p\u003e\n\u003cpre\u003e\nStreamDriverLocation(userId: string)\n  =\u003e Geolocation\n\u003c/pre\u003e\n\u003cp\u003e\n  Usage: continuously streams the location of a driver over a long-lived\n  websocket connection; the driver whose location is streamed is the one\n  associated with the \u003ci\u003eRide\u003c/i\u003e tied to the passed \u003ci\u003euserId\u003c/i\u003e.\n\u003c/p\u003e"
			},
			{
			   "title":"Driver API",
			   "content":"\u003cp\u003e\n  The driver-facing API will rely on some of the same CRUD operations around the\n  \u003ci\u003eRide\u003c/i\u003e entity, and it'll also have a \u003ci\u003eSetDriverStatus\u003c/i\u003e endpoint as\n  well as an endpoint to push the driver's location to passengers who are\n  streaming it.\n\u003c/p\u003e\n\u003cpre\u003e\nSetDriverStatus(userId: string, driverStatus: DriverStatus)\n  =\u003e void\n\nDriverStatus: enum UNAVAILABLE/IN RIDE/STANDBY\n\u003c/pre\u003e\n\u003cp\u003e\n  Usage: called when a driver wants to look for a ride, is starting a ride, or\n  is done for the day; when called with \u003cb\u003eSTANDBY\u003c/b\u003e, the Uber backend calls\n  an internal \u003ci\u003eFindRide\u003c/i\u003e API that uses an algorithm to enqueue the driver\n  in a queue of drivers waiting for rides and to find the most appropriate ride;\n  once a ride is found, the ride is internally locked to the driver for 30\n  seconds, during which the driver can accept or reject the ride; once the\n  driver accepts the ride, the internal backend calls \u003ci\u003eEditRide\u003c/i\u003e with the\n  driver's info and with a \u003cb\u003eMATCHED\u003c/b\u003e \u003ci\u003eRideStatus\u003c/i\u003e.\n\u003c/p\u003e\n\u003cpre\u003e\nGetRide(userId: string)\n  =\u003e Ride\n\u003c/pre\u003e\n\u003cp\u003e\n  Usage: polled every 20-90 seconds throughout the trip to update the ride's\n  estimated price, its time to destination, whether it's been cancelled, etc..\n\u003c/p\u003e\n\u003cpre\u003e\nEditRide(userId: string, [...params?: all properties on the Ride object that need to be edited])\n  =\u003e Ride\n\u003c/pre\u003e\n\u003cpre\u003e\nAcceptRide(userId: string)\n  =\u003e void\n\u003c/pre\u003e\n\u003cp\u003e\n  Calls \u003ci\u003eEditRide(userId, MATCHED)\u003c/i\u003e and\n  \u003ci\u003eSetDriverStatus(userId, IN_RIDE)\u003c/i\u003e.\n\u003c/p\u003e\n\u003cpre\u003e\nCancelRide(userId: string)\n  =\u003e void\n\u003c/pre\u003e\n\u003cp\u003e\n  Wrapper around \u003ci\u003eEditRide\u003c/i\u003e -- effectively calls\n  \u003ci\u003eEditRide(userId, CANCELLED)\u003c/i\u003e.\n\u003c/p\u003e\n\u003cpre\u003e\nPushLocation(userId: string, location: Geolocation)\n  =\u003e void\n\u003c/pre\u003e\n\u003cp\u003e\n  Usage: continuously called by a driver's phone throughout a ride; pushes the\n  driver's location to the relevant passenger who's streaming the location; the\n  passenger is the one associated with the \u003ci\u003eRide\u003c/i\u003e tied to the passed\n  \u003ci\u003euserId\u003c/i\u003e.\n\u003c/p\u003e"
			},
			{
			   "title":"UberPool",
			   "content":"\u003cp\u003e\n  As a stretch goal, your interviewer might ask you to think about how you'd\n  expand your design to handle UberPool rides.\n\u003c/p\u003e\n\u003cp\u003e\n  UberPool rides allow multiple passengers (different Uber accounts) to share an\n  Uber ride for a cheaper price.\n\u003c/p\u003e\n\u003cp\u003e\n  One way to handle UberPool rides would be to allow \u003ci\u003eRide\u003c/i\u003e objects to have\n  multiple \u003ci\u003epassengerInfo\u003c/i\u003es. In this case, \u003ci\u003eRide\u003c/i\u003es would also have to\n  maintain a list of all destinations that the ride will stop at, as well as the\n  relevant final destinations for individual passengers.\n\u003c/p\u003e\n\u003cp\u003e\n  Perhaps a cleaner way to handle UberPool rides would be to introduce an\n  entirely new entity, a \u003ci\u003ePoolRide\u003c/i\u003e entity, which would have a list of\n  \u003ci\u003eRide\u003c/i\u003es attached to it. Passengers would still call the\n  \u003ci\u003eCreateRide\u003c/i\u003e endpoint when booking an UberPool ride, and so they would\n  still have their own, normal \u003ci\u003eRide\u003c/i\u003e entity, but this entity would be\n  attached to a \u003ci\u003ePoolRide\u003c/i\u003e entity with the rest of the UberPool ride\n  information.\n\u003c/p\u003e\n\u003cp\u003e\n  Drivers would likely have an extra \u003ci\u003eDriverStatus\u003c/i\u003e value to indicate if\n  they were in a ride but still accepting new UberPool passengers.\n\u003c/p\u003e\n\u003cp\u003e\n  Most of the other functionality would remain the same; passengers and drivers\n  would still continuously poll the \u003ci\u003eGetRide\u003c/i\u003e endpoint for updated\n  information about the ride, passengers would still stream their driver's\n  location, passengers would still be able to cancel their individual rides,\n  etc..\n\u003c/p\u003e"
			}
		 ],
		 "hints":[
			{
			   "question":"Uber has a lot of different services: there’s the core ride-hailing Uber service, there’s UberEats, there’s UberPool--are we designing the API for all of these services, or just for one of them?",
			   "answer":"Let’s just design the core rides API -- not UberEats or UberPool."
			},
			{
			   "question":"At first thought, it seems like we're going to need both a passenger-facing API and a driver-facing API--does that make sense, and if yes, should we design both?",
			   "answer":"Yes, that totally makes sense. And yes, let’s design both, starting with the passenger-facing API."
			},
			{
			   "question":"To make sure we’re on the same page, this is the functionality that I'm envisioning this API will support: A user (a passenger) goes on their phone and hails a ride; they get matched with a driver; then they can track their ride as it’s in progress, until they reach their destination, at which point the ride is complete. Throughout this process, there are a few more features to support, like being able to track where the passenger's driver is before the passenger gets picked up, maybe being able to cancel rides, etc.. Does this capture most of what you had in mind?",
			   "answer":"Yes, this is precisely what I had in mind. And you can work out the details as you start designing the API."
			},
			{
			   "question":"Do we need to handle things like creating an Uber account, setting up payment preferences, contacting Uber, etc..? What about things like rating a driver, tipping a driver, etc.?",
			   "answer":"For now, let’s skip those and really focus on the core taxiing service."
			},
			{
			   "question":"Just to confirm, you want me to write out function signatures for various API endpoints, including parameters, their types, return values, etc., right?",
			   "answer":"Yup, exactly."
			}
		 ]
	  },
	  {
		 "name":"Design Slack",
		 "available":true,
		 "date_added":"",
		 "ready":true,
		 "video_url":"https://player.vimeo.com/video/454259139",
		 "release_date":"2020-09-16T12:00:00-04:00",
		 "prompt":"",
		 "walkthrough":[
			{
			   "title":"Gathering System Requirements",
			   "content":"\u003cp\u003e\n  As with any systems design interview question, the first thing that we want to\n  do is to gather system requirements; we need to figure out what system we're\n  building exactly.\n\u003c/p\u003e\n\u003cp\u003e\n  We're designing the core communication system behind Slack, which allows users\n  to send instant messages in Slack channels.\n\u003c/p\u003e\n\u003cp\u003eSpecifially, we'll want to support:\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003e\n    Loading the most recent messages in a Slack channel when a user clicks on\n    the channel.\n  \u003c/li\u003e\n  \u003cli\u003e\n    Immediately seeing which channels have unread messages for a particular user\n    when that user loads Slack.\n  \u003c/li\u003e\n  \u003cli\u003e\n    Immediately seeing which channels have unread mentions of a particular user,\n    for that particular user, when that user loads Slack, and more specifically,\n    the number of these unread mentions in each relevant channel.\n  \u003c/li\u003e\n  \u003cli\u003eSending and receiving Slack messages instantly, in real time.\u003c/li\u003e\n  \u003cli\u003e\n    Cross-device synchronization: if a user has both the Slack desktop app and\n    the Slack mobile app open, with an unread channel in both, and if they read\n    this channel on one device, the second device should immediately be updated\n    and no longer display the channel as unread.\n  \u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\n  The system should have low latencies and high availability, catering to a\n  single region of roughly 20 million users. The largest Slack organizations\n  will have as many as 50,000 users, with channels of the same size within them.\n\u003c/p\u003e\n\u003cp\u003e\n  That being said, for the purpose of this design, we should primarily focus on\n  latency and core functionality; availability and regionality can be\n  disregarded, within reason.\n\u003c/p\u003e"
			},
			{
			   "title":"Coming Up With A Plan",
			   "content":"\u003cp\u003eWe'll tackle this system by dividing it into two main sections:\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003eHandling what happens when a Slack app loads.\u003c/li\u003e\n  \u003cli\u003eHandling real-time messaging as well as cross-device synchronization.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWe can further divide the first section as follows:\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003eSeeing all of the channels that a user is a part of.\u003c/li\u003e\n  \u003cli\u003eSeeing messages in a particular channel.\u003c/li\u003e\n  \u003cli\u003eSeeing which channels have unread messages.\u003c/li\u003e\n  \u003cli\u003eSeeing which channels have unread mentions and how many they have.\u003c/li\u003e\n\u003c/ul\u003e"
			},
			{
			   "title":"Persistent Storage Solution \u0026 App Load",
			   "content":"\u003cp\u003e\n  While a large component of our design involves real-time communication,\n  another large part of it involves retrieving data (channels, messages, etc.)\n  at any given time when the Slack app loads. To support this, we'll need a\n  persistent storage solution.\n\u003c/p\u003e\n\u003cp\u003e\n  Specifically, we'll opt for a SQL database since we can expect this data to be\n  structured and to be queried frequently.\n\u003c/p\u003e\n\u003cp\u003eWe can start with a simple table that'll store every Slack channel.\u003c/p\u003e\n\u003cp\u003e\u003cb\u003eChannels\u003c/b\u003e\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n    \u003ctr\u003e\n      \u003cth\u003eid (channelId): \u003ci\u003euuid\u003c/i\u003e\u003c/th\u003e\n      \u003cth\u003eorgId: \u003ci\u003euuid\u003c/i\u003e\u003c/th\u003e\n      \u003cth\u003ename: \u003ci\u003estring\u003c/i\u003e\u003c/th\u003e\n      \u003cth\u003edescription: \u003ci\u003estring\u003c/i\u003e\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e...\u003c/td\u003e\n      \u003ctd\u003e...\u003c/td\u003e\n      \u003ctd\u003e...\u003c/td\u003e\n      \u003ctd\u003e...\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003cp\u003e\n  Then, we can have another simple table representing channel-member pairs: each\n  row in this table will correspond to a particular user who is in a particular\n  channel. We'll use this table, along with the one above, to fetch a user's\n  relevant when the app loads.\n\u003c/p\u003e\n\u003cp\u003e\u003cb\u003eChannel Members\u003c/b\u003e\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n    \u003ctr\u003e\n      \u003cth\u003eid: \u003ci\u003euuid\u003c/i\u003e\u003c/th\u003e\n      \u003cth\u003eorgId: \u003ci\u003euuid\u003c/i\u003e\u003c/th\u003e\n      \u003cth\u003echannelId: \u003ci\u003euuid\u003c/i\u003e\u003c/th\u003e\n      \u003cth\u003euserId: \u003ci\u003euuid\u003c/i\u003e\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e...\u003c/td\u003e\n      \u003ctd\u003e...\u003c/td\u003e\n      \u003ctd\u003e...\u003c/td\u003e\n      \u003ctd\u003e...\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003cp\u003e\n  We'll naturally need a table to store all historical messages sent on Slack.\n  This will be our largest table, and it'll be queried every time a user fetches\n  messages in a particular channel. The API endpoint that'll interact with this\n  table will return a paginated response, since we'll typically only want the 50\n  or 100 most recent messages per channel.\n\u003c/p\u003e\n\u003cp\u003e\n  Also, this table will only be queried when a user clicks on a channel; we\n  don't want to fetch messages for all of a user's channels on app load, since\n  users will likely never look at most of their channels.\n\u003c/p\u003e\n\u003cp\u003e\u003cb\u003eHistorical Messages\u003c/b\u003e\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n    \u003ctr\u003e\n      \u003cth\u003eid: \u003ci\u003euuid\u003c/i\u003e\u003c/th\u003e\n      \u003cth\u003eorgId: \u003ci\u003euuid\u003c/i\u003e\u003c/th\u003e\n      \u003cth\u003echannelId: \u003ci\u003euuid\u003c/i\u003e\u003c/th\u003e\n      \u003cth\u003esenderId: \u003ci\u003euuid\u003c/i\u003e\u003c/th\u003e\n      \u003cth\u003esentAt: \u003ci\u003etimestamp\u003c/i\u003e\u003c/th\u003e\n      \u003cth\u003ebody: \u003ci\u003estring\u003c/i\u003e\u003c/th\u003e\n      \u003cth\u003ementions: \u003ci\u003eList\u0026lt;uuid\u0026gt;\u003c/i\u003e\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e...\u003c/td\u003e\n      \u003ctd\u003e...\u003c/td\u003e\n      \u003ctd\u003e...\u003c/td\u003e\n      \u003ctd\u003e...\u003c/td\u003e\n      \u003ctd\u003e...\u003c/td\u003e\n      \u003ctd\u003e...\u003c/td\u003e\n      \u003ctd\u003e...\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003cp\u003e\n  In order not to fetch recent messages for every channel on app load, all the\n  while supporting the feature of showing which channels have unread messages,\n  we'll need to store two extra tables: one for the latest activity in each\n  channel (this table will be updated whenever a user sends a message in a\n  channel), and one for the last time a particular user has read a channel (this\n  table will be updated whenever a user opens a channel).\n\u003c/p\u003e\n\u003cp\u003e\u003cb\u003eLatest Channel Timestamps\u003c/b\u003e\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n    \u003ctr\u003e\n      \u003cth\u003eid: \u003ci\u003euuid\u003c/i\u003e\u003c/th\u003e\n      \u003cth\u003eorgId: \u003ci\u003euuid\u003c/i\u003e\u003c/th\u003e\n      \u003cth\u003echannelId: \u003ci\u003euuid\u003c/i\u003e\u003c/th\u003e\n      \u003cth\u003elastActive: \u003ci\u003etimestamp\u003c/i\u003e\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e...\u003c/td\u003e\n      \u003ctd\u003e...\u003c/td\u003e\n      \u003ctd\u003e...\u003c/td\u003e\n      \u003ctd\u003e...\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003cp\u003e\u003cb\u003eChannel Read Receipts\u003c/b\u003e\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n    \u003ctr\u003e\n      \u003cth\u003eid: \u003ci\u003euuid\u003c/i\u003e\u003c/th\u003e\n      \u003cth\u003eorgId: \u003ci\u003euuid\u003c/i\u003e\u003c/th\u003e\n      \u003cth\u003echannelId: \u003ci\u003euuid\u003c/i\u003e\u003c/th\u003e\n      \u003cth\u003euserId: \u003ci\u003euuid\u003c/i\u003e\u003c/th\u003e\n      \u003cth\u003elastSeen: \u003ci\u003etimestamp\u003c/i\u003e\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e...\u003c/td\u003e\n      \u003ctd\u003e...\u003c/td\u003e\n      \u003ctd\u003e...\u003c/td\u003e\n      \u003ctd\u003e...\u003c/td\u003e\n      \u003ctd\u003e...\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003cp\u003e\n  For the number of unread user mentions that we want to display next to channel\n  names, we'll have another table similar to the read-receipts one, except this\n  one will have a count of unread user mentions instead of a timestamp. This\n  count will be updated (incremented) whenever a user tags another user in a\n  channel message, and it'll also be updated (reset to 0) whenever a user opens\n  a channel with unread mentions of themself.\n\u003c/p\u003e\n\u003cp\u003e\u003cb\u003eUnread Channel-User-Mention Counts\u003c/b\u003e\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n    \u003ctr\u003e\n      \u003cth\u003eid: \u003ci\u003euuid\u003c/i\u003e\u003c/th\u003e\n      \u003cth\u003eorgId: \u003ci\u003euuid\u003c/i\u003e\u003c/th\u003e\n      \u003cth\u003echannelId: \u003ci\u003euuid\u003c/i\u003e\u003c/th\u003e\n      \u003cth\u003euserId: \u003ci\u003euuid\u003c/i\u003e\u003c/th\u003e\n      \u003cth\u003ecount: \u003ci\u003eint\u003c/i\u003e\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e...\u003c/td\u003e\n      \u003ctd\u003e...\u003c/td\u003e\n      \u003ctd\u003e...\u003c/td\u003e\n      \u003ctd\u003e...\u003c/td\u003e\n      \u003ctd\u003e...\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u003c/p\u003e"
			},
			{
			   "title":"Load Balancing",
			   "content":"\u003cp\u003e\n  For all of the API calls that clients will issue on app load, including writes\n  to our database (when sending a message or marking a channel as read), we're\n  going to want to load balance.\n\u003c/p\u003e\n\u003cp\u003e\n  We can have a simple round-robin load balancer, forwarding requests to a set\n  of server clusters that will then handle passing requests to our database.\n\u003c/p\u003e"
			},
			{
			   "title":"\"Smart\" Sharding",
			   "content":"\u003cp\u003e\n  Since our tables will be very large, especially the messages table, we'll need\n  to have some sharding in place.\n\u003c/p\u003e\n\u003cp\u003e\n  The natural approach is to shard based on organization size: we can have the\n  biggest organizations (with the biggest channels) in their individual shards,\n  and we can have smaller organizations grouped together in other shards.\n\u003c/p\u003e\n\u003cp\u003e\n  An important point to note here is that, over time, organization sizes and\n  Slack activity within organizations will change. Some organizations might\n  double in size overnight, others might experience seemingly random surges of\n  activity, etc.. This means that, despite our relatively sound sharding\n  strategy, we might still run into hot spots, which is very bad considering the\n  fact that we care about latency so much.\n\u003c/p\u003e\n\u003cp\u003e\n  To handle this, we can add a \"smart\" sharding solution: a subsystem of our\n  system that'll asynchronously measure organization activity and \"rebalance\"\n  shards accordingly. This service can be a strongly consistent key-value store\n  like Etcd or ZooKeeper, mapping orgIds to shards. Our API servers will\n  communicate with this service to know which shard to route requests to.\n\u003c/p\u003e"
			},
			{
			   "title":"Pub/Sub System for Real-Time Behavior",
			   "content":"\u003cp\u003eThere are two types of real-time behavior that we want to support:\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003eSending and receiving messages in real time.\u003c/li\u003e\n  \u003cli\u003e\n    Cross-device synchronization (instantly marking a channel as read if you\n    have Slack open on two devices and read the channel on one of them).\n  \u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\n  For both of these functionalities, we can rely on a Pub/Sub messaging system,\n  which itself will rely on our previously described \"smart\" sharding strategy.\n\u003c/p\u003e\n\u003cp\u003e\n  Every Slack organization or group of organizations will be assigned to a Kafka\n  topic, and whenever a user sends a message in a channel or marks a channel as\n  read, our previously mentioned API servers, which handle speaking to our\n  database, will also send a Pub/Sub message to the appropriate Kafka topic.\n\u003c/p\u003e\n\u003cp\u003eThe Pub/Sub messages will look like:\u003c/p\u003e\n\u003cpre\u003e\n{\n  \"type\": \"chat\",\n  \"orgId\": \"AAA\",\n  \"channelId\": \"BBB\",\n  \"userId\": \"CCC\",\n  \"messageId\": \"DDD\",\n  \"timestamp\": \"2020-08-31T01:17:02\",\n  \"body\": \"this is a message\",\n  \"mentions\": [\"CCC\", \"EEE\"]\n},\n{\n  \"type\": \"read-receipt\",\n  \"orgId\": \"AAA\",\n  \"channelId\": \"BBB\",\n  \"userId\": \"CCC\",\n  \"timestamp\": \"2020-08-31T01:17:02\"\n}\n\u003c/pre\u003e\n\u003cp\u003e\n  We'll then have a different set of API servers who subscribe to the various\n  Kakfa topics (probably one API server cluster per topic), and our clients\n  (Slack users) will establish long-lived TCP connections with these API server\n  clusters to receive Pub/Sub messages in real time.\n\u003c/p\u003e\n\u003cp\u003e\n  We'll want a load balancer in between the clients and these API servers, which\n  will also use the \"smart\" sharding strategy to match clients with the\n  appropriate API servers, which will be listening to the appropriate Kafka\n  topics.\n\u003c/p\u003e\n\u003cp\u003e\n  When clients receive Pub/Sub messages, they'll handle them accordingly (mark a\n  channel as unread, for example), and if the clients refresh their browser or\n  their mobile app, they'll go through the entire \"on app load\" system that we\n  described earlier.\n\u003c/p\u003e\n\u003cp\u003e\n  Since each Pub/Sub message comes with a timestamp, and since reading a channel\n  and sending Slack messages involve writing to our persistent storage, the\n  Pub/Sub messages will effectively be idempotent operations.\n\u003c/p\u003e"
			},
			{
			   "title":"System Diagram",
			   "content":"\u003cimg\n  width=\"100%\"\n  src=\"https://assets.algoexpert.io/se-diagrams/slack.svg\"\n  alt=\"Final Systems Architecture\"\n/\u003e",
			   "image": {
				   "name": "slack",
				   "extension": "png"
			   }
			}
		 ],
		 "hints":[
			{
			   "question":"There are a lot of things that you can do on Slack. Primarily, you use Slack to communicate with people in one-on-one channels, private channels, or public channels, all within an organization. But you can also do a bunch of other things on Slack, like create and delete channels, change channel settings, change Slack settings, invite people to channels, etc.. What exactly are we designing here?",
			   "answer":"We're designing the core messaging functionality, which involves communicating in both one-on-one channels and group channels in an organization. You don't have to worry about channel settings and all of those extra functionalities."
			},
			{
			   "question":"Okay. Do you want me to take care of the concept of private channels at all?",
			   "answer":"Let's just focus on users in a channel as far as access control is concerned; we can forget about the concept of a private channel."
			},
			{
			   "question":"Okay. And regarding communication, from my knowledge of Slack, when you load the web app or the desktop / mobile apps, you can obviously access all the messages of channels that you're in (including one-on-one channels), but you're also notified of channels that have unread messages for you and of the number of unread mentions that you have in each channel. Channels with unread messages are bold, if I remember correctly, and the number of unread mentions is simply visible next to channel names. Should we design our system to accommodate this?",
			   "answer":"Yes, we should take care of this. And on that note, one thing we'll want to handle is cross-device synchronization. In other words, if you have both the Slack desktop app and the Slack mobile app open, and both apps are showing that one channel is unread, and you read that channel on one of the apps, the other app should immediately get updated and should mark the channel as read. You'll have to handle this."
			},
			{
			   "question":"Hmm, okay. Speaking of different applications, by the way, are we designing the various device / software apps, or just the backend systems that the frontends / clients communicate with?",
			   "answer":"You'll only really focus on the backend systems for this question."
			},
			{
			   "question":"Okay. Also, there are a lot of different features in actual Slack messages. For example, adding custom emojis, pinning messages, saving messages, writing code snippets or text-blocks, etc.. Do you want me to handle all of this?",
			   "answer":"No, you can just treat messages as pure text for now. Of course, what you'll design will likely be extensible to different types of messages and will eventually be able to handle things like pinning or saving messages, but for this design, don't worry about that."
			},
			{
			   "question":"How many users do we expect to be building this for? And how large is the largest organization on slack? How many users does it have?",
			   "answer":"Slack has about 10 to 20 million users, so let's go with 20 million. And as for organizations, let's say that the largest single Slack customer has 50,000 people in the same organization. We can also approximate that the largest channel will be of that same size if all of an organization's employees are in the same channel (the typical #general channel, for example)."
			},
			{
			   "question":"Since this is a chat application, I'm assuming that low latency is one of our top priorities, and also, since this service impacts millions of users, I'm assuming that we should design with high availability in mind. Are these correct assumptions?",
			   "answer":"Yes to both of those things, but for the sake of being a little more focused, don't worry about optimizing for availability. Let's focus primarily on latency and core functionality."
			},
			{
			   "question":"Okay. And are we building this for a global audience, or should we focus on a single region?",
			   "answer":"Let's handle a single region for this question, but just like with availability, don't focus too much on this aspect of the design."
			}
		 ]
	  },
	  {
		 "name":"Design Airbnb",
		 "available":true,
		 "date_added":"",
		 "ready":true,
		 "video_url":"https://player.vimeo.com/video/458018356",
		 "release_date":"2020-09-16T12:00:00-04:00",
		 "prompt":"",
		 "walkthrough":[
			{
			   "title":"Gathering System Requirements",
			   "content":"\u003cp\u003e\n  As with any systems design interview question, the first thing that we want to\n  do is to gather system requirements; we need to figure out what system we're\n  building exactly.\n\u003c/p\u003e\n\u003cp\u003e\n  We're designing the core system behind Airbnb, which allows hosts to create\n  property listings and renters to browse through these listings and book them.\n\u003c/p\u003e\n\u003cp\u003eSpecifially, we'll want to support:\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003eOn the host side, creating and deleting listings.\u003c/li\u003e\n  \u003cli\u003e\n    On the renter side, browsing through listings, getting individual listings,\n    and \"reserving\" listings.\n  \u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\n  \"Reserving\" listings should happen when a renter presses some \"Book Now\"\n  button and should effectively lock (or reserve) the listing for some\n  predetermined period of time (say, 15 minutes), during which any other renter\n  shouldn't be able to reserve the listing or to even browse it (unless they\n  were already browsing it).\n\u003c/p\u003e\n\u003cp\u003e\n  We don't need to support anything that happens after a reservation is made,\n  except for freeing up the reservation after 15 minutes if the renter doesn't\n  follow through with the booking and making the reservation permanent if the\n  renter does actually book the listing in question.\n\u003c/p\u003e\n\u003cp\u003e\n  Regarding listings, we should focus on browsing and reserving them based on\n  location and available date range; we can ignore any other property\n  characteristics like price, number of bedrooms, etc.. Browsing listings should\n  be as quick as possible, and it should reflect newly created listings as fast\n  as possible. Lastly, reserved and booked listings shoudln't be browsable by\n  renters.\n\u003c/p\u003e\n\u003cp\u003e\n  Our system should serve a U.S.-based audience with approximately 50 million\n  users and 1 million listings.\n\u003c/p\u003e"
			},
			{
			   "title":"Coming Up With A Plan",
			   "content":"\u003cp\u003eWe'll tackle this system by dividing it into two main sections:\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003eThe host side.\u003c/li\u003e\n  \u003cli\u003eThe renter side.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWe can further divide the renter side as follows:\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003eBrowsing (listing) listings.\u003c/li\u003e\n  \u003cli\u003eGetting a single listing.\u003c/li\u003e\n  \u003cli\u003eReserving a listing.\u003c/li\u003e\n\u003c/ul\u003e"
			},
			{
			   "title":"Listings Storage \u0026 Quadtree",
			   "content":"\u003cp\u003e\n  First and foremost, we can expect to store all of our listings in a SQL table.\n  This will be our primary source of truth for listings on Airbnb, and whenever\n  a host creates or deletes a listing, this SQL table will be written to.\n\u003c/p\u003e\n\u003cp\u003e\n  Then, since we care about the latency of browsing listings on Airbnb, and\n  since this browsing will require querying listings based on their location, we\n  can store our listings in a region quadtree, to be traversed for all browsing\n  functionality.\n\u003c/p\u003e\n\u003cp\u003e\n  Since we're optimizing for speed, it'll make sense to store this quadtree in\n  memory on some auxiliary machine, which we can call a \"geo index,\" but we need\n  to make sure that we can actually fit this quadtree in memory.\n\u003c/p\u003e\n\u003cp\u003e\n  In this quadtree, we'll need to store all the information about listings that\n  needs to be displayed on the UI when a renter is browsing through listings: a\n  title, a description, a link pointing to a property image, a unique listing\n  ID, etc..\n\u003c/p\u003e\n\u003cp\u003e\n  Assuming a single listing takes up roughly 10 KB of space (as an upper bound),\n  some simple math confirms that we can store everything we need about listings\n  in memory.\n\u003c/p\u003e\n\u003cpre\u003e\n  ~10 KB per listing\n  ~1 million listings\n  ~10 KB * 1000^2 = 10 GB\n\u003c/pre\u003e\n\u003cp\u003e\n  Since we'll be storing our quadtree in memory, we'll want to make sure that a\n  single machine failure doesn't bring down the entire browsing functionality.\n  To ensure this, we can set up a cluster of machines, each holding an instance\n  of our quadtree in memory, and these machines can use leader election to\n  safeguard us from machine failures.\n\u003c/p\u003e\n\u003cp\u003e\n  Our quadtree solution works as follows: when our system boots up, the\n  geo-index machines create the quadtree by querying our SQL table of listings.\n  When listings are created or deleted, hosts first write to the SQL table, and\n  then they synchronously update the geo-index leader's quadtree. Then, on an\n  interval of say, 10 minutes, the geo-index leader and followers all recreate\n  the quadtree from the SQL table, which allows them to stay up to date with new\n  listings.\n\u003c/p\u003e\n\u003cp\u003e\n  If the leader dies at any point, one of the followers takes its place, and\n  data in the new leader's quadtree will be stale for at most a few minutes\n  until the interval forces the quadtree to be recreated.\n\u003c/p\u003e"
			},
			{
			   "title":"Listing Listings",
			   "content":"\u003cp\u003e\n  When renters browse through listings, they'll have to hit some\n  \u003ci\u003eListListings\u003c/i\u003e API endpoint. This API call will search through the\n  geo-index leader's quadtree for relevant listings based on the location that\n  the renter passes.\n\u003c/p\u003e\n\u003cp\u003e\n  Finding relevant locations should be fairly straightforward and very fast,\n  especially since we can estimate that our quadtree will have a depth of\n  approximately 10, since 4^10 is greater than 1 million.\n\u003c/p\u003e\n\u003cp\u003e\n  That being said, we'll have to make sure that we don't return listings that\n  are unavailable during the date range specified by the renter. In order to\n  handle this, each listing in the quad tree will contain a list of unavailable\n  date ranges, and we can perform a simple binary search on this list for each\n  listing, in order to determine if the listing in question is available and\n  therefore browsable by the renter.\n\u003c/p\u003e\n\u003cp\u003e\n  We can also make sure that our quadtree returns only a subset of relevant\n  listings for pagination purposes, and we can determine this subset by using an\n  offset: the first page of relevant listings would have an offset of 0, the\n  second page would have an offset of 50 (if we wanted pages to have a size of\n  50), the third page would have an offset of 100, and so on and so forth.\n\u003c/p\u003e"
			},
			{
			   "title":"Getting Individual Listings",
			   "content":"\u003cp\u003e\n  This API call should be extremely simple; we can expect to have listing IDs\n  from the list of listings that a renter is browsing through, and we can simply\n  query our SQL table of listings for the given ID.\n\u003c/p\u003e"
			},
			{
			   "title":"Reserving Listings",
			   "content":"\u003cp\u003e\n  Reserved listings will need to be reflected both in our quadtree and in our\n  persistent storage solution. In our quadtree, because they'll have to be\n  excluded from the list of browsable listings; in our persistent storage\n  solution, because if our quadtree needs to have them, then the main source of\n  truth also needs to have them.\n\u003c/p\u003e\n\u003cp\u003e\n  We can have a second SQL table for reservations, holding listing IDs as well\n  as date ranges and timestamps for when their reservations expire. When a\n  renter tries to start the booking process of a listing, the reservation table\n  will first be checked to see if there's currently a reservation for the given\n  listing during the specified date range; if there is, an error is returned to\n  the renter; if there isn't, a reservation is made with an expiration timestamp\n  15 minutes into the future.\n\u003c/p\u003e\n\u003cp\u003e\n  Following the write to the reservation table, we synchronously update the\n  geo-index leader's quadtree with the new reservation. This new reservation\n  will simply be an unavailability interval in the list of unavailabilities on\n  the relevant listing, but we'll also specify an expiration for this\n  unavailability, since it's a reservation.\n\u003c/p\u003e\n\u003cp\u003eA listing in our quadtree might look something like this:\u003c/p\u003e\n\u003cpre\u003e\n{\n  \"unavailabilities\": [\n    {\n      \"range\": [\"2020-09-22T12:00:00-05:00\", \"2020-09-28T12:00:00-05:00\"],\n      \"expiration\": \"2020-09-16T12:00:00-04:00\"\n    }\n    {\n      \"range\": [\"2020-10-02T12:00:00-05:00\", \"2020-10-10T12:00:00-05:00\"],\n      \"expiration\": null\n    },\n  ],\n  \"title\": \"Listing Title\",\n  \"description\": \"Listing Description\",\n  \"thumbnailUrl\": \"Listing Thumbnail URL\",\n  \"id\": \"Listing ID\"\n}\n\u003c/pre\u003e"
			},
			{
			   "title":"Load Balancing",
			   "content":"\u003cp\u003e\n  On the host side, we can load balance requests to create and delete listings\n  across a set of API servers using a simple round-robin approach. The API\n  servers will then be in charge of writing to the SQL database and of\n  communicating with the geo-index leader.\n\u003c/p\u003e\n\u003cp\u003e\n  On the renter side, we can load balance requests to list, get, and reserve\n  listings across a set of API servers using an API-path-based server-selection\n  strategy. Since workloads for these three API calls will be considerably\n  different from one another, it makes sense to separate these calls across\n  different sets of API servers.\n\u003c/p\u003e\n\u003cp\u003e\n  Of note is that we don't want any caching done at our API servers, because\n  otherwise we'll naturally run into stale data as reservations, bookings, and\n  new listings appear.\n\u003c/p\u003e"
			},
			{
			   "title":"System Diagram",
			   "content":"\u003cimg\n  width=\"100%\"\n  src=\"https://assets.algoexpert.io/se-diagrams/airbnb.svg\"\n  alt=\"Final Systems Architecture\"\n/\u003e",
			   "image": {
				   "name": "airbnb",
				   "extension": "png"
			   }
			}
		 ],
		 "hints":[
			{
			   "question":"Like a lot of other sharing-economy products out there, Airbnb has two sides: a host-facing side and a renter-facing side. Are we designing both of these sides or just one of them?",
			   "answer":"Let's design both of these sides of the product."
			},
			{
			   "question":"Okay. So we're probably designing the system for hosts to create and maybe delete listings, and the system for renters to browse through properties, book them, and manage their bookings afterwards. Is that correct?",
			   "answer":"Yes for hosts; but let's actually just focus on browsing through listings and booking them for renters. We can ignore everything that happens after booking on the renter-facing side."
			},
			{
			   "question":"Okay, but for booking, is the idea that, when a user is browsing a property for a specific date range, the property gets temporarily reserved for them if they start the booking process?",
			   "answer":"Yes. More specifically, multiple users should be allowed to look at the same property, for the same date range, concurrently without issues. But once a user starts the booking process for a property, it should be reflected that this property is no longer available for the dates in question if another user tries to book it."
			},
			{
			   "question":"I see. But so, let's say two users are looking at the exact same property for an overlapping date range, and one user presses \"Book Now\", at which point they have to enter credit card information. Should we immediately lock the property for the other user for some predetermined period of time, like maybe 15 minutes, and if the first person actually goes through with booking the property, then this \"lock\" becomes permanent?",
			   "answer":"Yes, that makes sense. In real life, there might be slight differences, but for the sake of this design, let's go with that."
			},
			{
			   "question":"Okay. And do we want to design any auxiliary features like being able to contact hosts, authentication and payment services, etc., or are we really just focusing on browsing and reserving?",
			   "answer":"Let's really just focus on browsing and booking. We can ignore the rest."
			},
			{
			   "question":"I see. So, since it sounds like we're designing a pretty targeted part of the entire Airbnb service, I want to make sure that I know exactly every functionality that we want to support. My understanding is that users can go on the main Airbnb website or app, they can look up properties based on certain criteria, like location, available date range, pricing, property details, etc., and then they can decide to book a location. As for hosts, they can basically just create a listing and delete it like I said earlier. Is that correct?",
			   "answer":"Yes. But actually, for this design, let's purely filter based on location and available date range as far as listing characteristics are concerned; let's not worry about other criteria like pricing and property details."
			},
			{
			   "question":"What is the scale that we're designing this for? Specifically, roughly how many listings and renters do we expect to cater to?",
			   "answer":"Let's only consider Airbnb's U.S. operations. So let's say 50 million users and 1 million listings."
			},
			{
			   "question":"Regarding systems characteristics like availability and latency, I'm assuming that, even if a renter looks up properties in a densely-populated area like NYC, where there might be a lot of listings, we care about serving these listings fast, accurately, and reliably. Is that correct?",
			   "answer":"Yes, that's correct. Ideally, we don't want any downtime for renters browsing listings."
			}
		 ]
	  }
   ]
}
